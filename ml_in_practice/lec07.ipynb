{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPCl2RuOU+FOEeiEMfjA4+K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_NBne3_ikDQ"
      },
      "source": [
        "## lec07"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s33ckyWFEYAe"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from torch import nn, optim, from_numpy, cuda\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbB3-xhpYgu2",
        "outputId": "5e3e02a2-fcbf-42c2-f015-a43705d16064"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "xy = np.loadtxt('/content/gdrive/My Drive/Colab Notebooks/ml_in_practice_data/data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "x_data = from_numpy(xy[:,  0:-1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "print('X shape: {} | Y shape: {}'.format(x_data.shape, y_data.shape))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "X shape: torch.Size([759, 8]) | Y shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVxbdNNHYgsd"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYCBVkHTYgqO",
        "outputId": "2e032340-f486-4061-f699-278780d65a8a"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 | Loss: 0.6817\n",
            "Epoch: 2/100 | Loss: 0.6776\n",
            "Epoch: 3/100 | Loss: 0.6740\n",
            "Epoch: 4/100 | Loss: 0.6708\n",
            "Epoch: 5/100 | Loss: 0.6680\n",
            "Epoch: 6/100 | Loss: 0.6654\n",
            "Epoch: 7/100 | Loss: 0.6632\n",
            "Epoch: 8/100 | Loss: 0.6612\n",
            "Epoch: 9/100 | Loss: 0.6595\n",
            "Epoch: 10/100 | Loss: 0.6579\n",
            "Epoch: 11/100 | Loss: 0.6565\n",
            "Epoch: 12/100 | Loss: 0.6552\n",
            "Epoch: 13/100 | Loss: 0.6541\n",
            "Epoch: 14/100 | Loss: 0.6532\n",
            "Epoch: 15/100 | Loss: 0.6523\n",
            "Epoch: 16/100 | Loss: 0.6515\n",
            "Epoch: 17/100 | Loss: 0.6508\n",
            "Epoch: 18/100 | Loss: 0.6502\n",
            "Epoch: 19/100 | Loss: 0.6497\n",
            "Epoch: 20/100 | Loss: 0.6492\n",
            "Epoch: 21/100 | Loss: 0.6487\n",
            "Epoch: 22/100 | Loss: 0.6483\n",
            "Epoch: 23/100 | Loss: 0.6480\n",
            "Epoch: 24/100 | Loss: 0.6477\n",
            "Epoch: 25/100 | Loss: 0.6474\n",
            "Epoch: 26/100 | Loss: 0.6472\n",
            "Epoch: 27/100 | Loss: 0.6469\n",
            "Epoch: 28/100 | Loss: 0.6467\n",
            "Epoch: 29/100 | Loss: 0.6466\n",
            "Epoch: 30/100 | Loss: 0.6464\n",
            "Epoch: 31/100 | Loss: 0.6463\n",
            "Epoch: 32/100 | Loss: 0.6461\n",
            "Epoch: 33/100 | Loss: 0.6460\n",
            "Epoch: 34/100 | Loss: 0.6459\n",
            "Epoch: 35/100 | Loss: 0.6458\n",
            "Epoch: 36/100 | Loss: 0.6458\n",
            "Epoch: 37/100 | Loss: 0.6457\n",
            "Epoch: 38/100 | Loss: 0.6456\n",
            "Epoch: 39/100 | Loss: 0.6456\n",
            "Epoch: 40/100 | Loss: 0.6455\n",
            "Epoch: 41/100 | Loss: 0.6455\n",
            "Epoch: 42/100 | Loss: 0.6454\n",
            "Epoch: 43/100 | Loss: 0.6454\n",
            "Epoch: 44/100 | Loss: 0.6454\n",
            "Epoch: 45/100 | Loss: 0.6453\n",
            "Epoch: 46/100 | Loss: 0.6453\n",
            "Epoch: 47/100 | Loss: 0.6453\n",
            "Epoch: 48/100 | Loss: 0.6453\n",
            "Epoch: 49/100 | Loss: 0.6452\n",
            "Epoch: 50/100 | Loss: 0.6452\n",
            "Epoch: 51/100 | Loss: 0.6452\n",
            "Epoch: 52/100 | Loss: 0.6452\n",
            "Epoch: 53/100 | Loss: 0.6452\n",
            "Epoch: 54/100 | Loss: 0.6452\n",
            "Epoch: 55/100 | Loss: 0.6452\n",
            "Epoch: 56/100 | Loss: 0.6452\n",
            "Epoch: 57/100 | Loss: 0.6451\n",
            "Epoch: 58/100 | Loss: 0.6451\n",
            "Epoch: 59/100 | Loss: 0.6451\n",
            "Epoch: 60/100 | Loss: 0.6451\n",
            "Epoch: 61/100 | Loss: 0.6451\n",
            "Epoch: 62/100 | Loss: 0.6451\n",
            "Epoch: 63/100 | Loss: 0.6451\n",
            "Epoch: 64/100 | Loss: 0.6451\n",
            "Epoch: 65/100 | Loss: 0.6451\n",
            "Epoch: 66/100 | Loss: 0.6451\n",
            "Epoch: 67/100 | Loss: 0.6451\n",
            "Epoch: 68/100 | Loss: 0.6451\n",
            "Epoch: 69/100 | Loss: 0.6451\n",
            "Epoch: 70/100 | Loss: 0.6451\n",
            "Epoch: 71/100 | Loss: 0.6451\n",
            "Epoch: 72/100 | Loss: 0.6451\n",
            "Epoch: 73/100 | Loss: 0.6451\n",
            "Epoch: 74/100 | Loss: 0.6451\n",
            "Epoch: 75/100 | Loss: 0.6451\n",
            "Epoch: 76/100 | Loss: 0.6451\n",
            "Epoch: 77/100 | Loss: 0.6451\n",
            "Epoch: 78/100 | Loss: 0.6451\n",
            "Epoch: 79/100 | Loss: 0.6451\n",
            "Epoch: 80/100 | Loss: 0.6451\n",
            "Epoch: 81/100 | Loss: 0.6451\n",
            "Epoch: 82/100 | Loss: 0.6451\n",
            "Epoch: 83/100 | Loss: 0.6451\n",
            "Epoch: 84/100 | Loss: 0.6451\n",
            "Epoch: 85/100 | Loss: 0.6451\n",
            "Epoch: 86/100 | Loss: 0.6451\n",
            "Epoch: 87/100 | Loss: 0.6451\n",
            "Epoch: 88/100 | Loss: 0.6451\n",
            "Epoch: 89/100 | Loss: 0.6451\n",
            "Epoch: 90/100 | Loss: 0.6451\n",
            "Epoch: 91/100 | Loss: 0.6451\n",
            "Epoch: 92/100 | Loss: 0.6451\n",
            "Epoch: 93/100 | Loss: 0.6451\n",
            "Epoch: 94/100 | Loss: 0.6451\n",
            "Epoch: 95/100 | Loss: 0.6451\n",
            "Epoch: 96/100 | Loss: 0.6451\n",
            "Epoch: 97/100 | Loss: 0.6451\n",
            "Epoch: 98/100 | Loss: 0.6451\n",
            "Epoch: 99/100 | Loss: 0.6451\n",
            "Epoch: 100/100 | Loss: 0.6451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6AVOGM_hK-l"
      },
      "source": [
        "- gradient vanishing 문제 때문에 loss가 잘 감소하지 않았다\n",
        "- 이를 해결하기 위해 ReLU 함수 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br_fzLIkYgoG"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.relu(self.l1(x))\n",
        "        out2 = self.relu(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipeMf44PYgld",
        "outputId": "d069ee65-3201-47b9-b9cc-173f72f68df4"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 | Loss: 0.6728\n",
            "Epoch: 2/100 | Loss: 0.6705\n",
            "Epoch: 3/100 | Loss: 0.6683\n",
            "Epoch: 4/100 | Loss: 0.6664\n",
            "Epoch: 5/100 | Loss: 0.6646\n",
            "Epoch: 6/100 | Loss: 0.6628\n",
            "Epoch: 7/100 | Loss: 0.6613\n",
            "Epoch: 8/100 | Loss: 0.6598\n",
            "Epoch: 9/100 | Loss: 0.6584\n",
            "Epoch: 10/100 | Loss: 0.6571\n",
            "Epoch: 11/100 | Loss: 0.6559\n",
            "Epoch: 12/100 | Loss: 0.6547\n",
            "Epoch: 13/100 | Loss: 0.6537\n",
            "Epoch: 14/100 | Loss: 0.6527\n",
            "Epoch: 15/100 | Loss: 0.6517\n",
            "Epoch: 16/100 | Loss: 0.6509\n",
            "Epoch: 17/100 | Loss: 0.6500\n",
            "Epoch: 18/100 | Loss: 0.6493\n",
            "Epoch: 19/100 | Loss: 0.6485\n",
            "Epoch: 20/100 | Loss: 0.6478\n",
            "Epoch: 21/100 | Loss: 0.6472\n",
            "Epoch: 22/100 | Loss: 0.6465\n",
            "Epoch: 23/100 | Loss: 0.6460\n",
            "Epoch: 24/100 | Loss: 0.6454\n",
            "Epoch: 25/100 | Loss: 0.6449\n",
            "Epoch: 26/100 | Loss: 0.6444\n",
            "Epoch: 27/100 | Loss: 0.6439\n",
            "Epoch: 28/100 | Loss: 0.6435\n",
            "Epoch: 29/100 | Loss: 0.6431\n",
            "Epoch: 30/100 | Loss: 0.6427\n",
            "Epoch: 31/100 | Loss: 0.6423\n",
            "Epoch: 32/100 | Loss: 0.6420\n",
            "Epoch: 33/100 | Loss: 0.6417\n",
            "Epoch: 34/100 | Loss: 0.6413\n",
            "Epoch: 35/100 | Loss: 0.6410\n",
            "Epoch: 36/100 | Loss: 0.6407\n",
            "Epoch: 37/100 | Loss: 0.6404\n",
            "Epoch: 38/100 | Loss: 0.6402\n",
            "Epoch: 39/100 | Loss: 0.6399\n",
            "Epoch: 40/100 | Loss: 0.6396\n",
            "Epoch: 41/100 | Loss: 0.6394\n",
            "Epoch: 42/100 | Loss: 0.6391\n",
            "Epoch: 43/100 | Loss: 0.6389\n",
            "Epoch: 44/100 | Loss: 0.6386\n",
            "Epoch: 45/100 | Loss: 0.6384\n",
            "Epoch: 46/100 | Loss: 0.6382\n",
            "Epoch: 47/100 | Loss: 0.6379\n",
            "Epoch: 48/100 | Loss: 0.6377\n",
            "Epoch: 49/100 | Loss: 0.6375\n",
            "Epoch: 50/100 | Loss: 0.6373\n",
            "Epoch: 51/100 | Loss: 0.6370\n",
            "Epoch: 52/100 | Loss: 0.6368\n",
            "Epoch: 53/100 | Loss: 0.6366\n",
            "Epoch: 54/100 | Loss: 0.6364\n",
            "Epoch: 55/100 | Loss: 0.6362\n",
            "Epoch: 56/100 | Loss: 0.6360\n",
            "Epoch: 57/100 | Loss: 0.6357\n",
            "Epoch: 58/100 | Loss: 0.6355\n",
            "Epoch: 59/100 | Loss: 0.6353\n",
            "Epoch: 60/100 | Loss: 0.6351\n",
            "Epoch: 61/100 | Loss: 0.6349\n",
            "Epoch: 62/100 | Loss: 0.6347\n",
            "Epoch: 63/100 | Loss: 0.6345\n",
            "Epoch: 64/100 | Loss: 0.6343\n",
            "Epoch: 65/100 | Loss: 0.6341\n",
            "Epoch: 66/100 | Loss: 0.6339\n",
            "Epoch: 67/100 | Loss: 0.6337\n",
            "Epoch: 68/100 | Loss: 0.6335\n",
            "Epoch: 69/100 | Loss: 0.6332\n",
            "Epoch: 70/100 | Loss: 0.6330\n",
            "Epoch: 71/100 | Loss: 0.6328\n",
            "Epoch: 72/100 | Loss: 0.6326\n",
            "Epoch: 73/100 | Loss: 0.6324\n",
            "Epoch: 74/100 | Loss: 0.6322\n",
            "Epoch: 75/100 | Loss: 0.6320\n",
            "Epoch: 76/100 | Loss: 0.6317\n",
            "Epoch: 77/100 | Loss: 0.6315\n",
            "Epoch: 78/100 | Loss: 0.6313\n",
            "Epoch: 79/100 | Loss: 0.6311\n",
            "Epoch: 80/100 | Loss: 0.6309\n",
            "Epoch: 81/100 | Loss: 0.6306\n",
            "Epoch: 82/100 | Loss: 0.6304\n",
            "Epoch: 83/100 | Loss: 0.6302\n",
            "Epoch: 84/100 | Loss: 0.6299\n",
            "Epoch: 85/100 | Loss: 0.6297\n",
            "Epoch: 86/100 | Loss: 0.6295\n",
            "Epoch: 87/100 | Loss: 0.6292\n",
            "Epoch: 88/100 | Loss: 0.6290\n",
            "Epoch: 89/100 | Loss: 0.6287\n",
            "Epoch: 90/100 | Loss: 0.6285\n",
            "Epoch: 91/100 | Loss: 0.6282\n",
            "Epoch: 92/100 | Loss: 0.6280\n",
            "Epoch: 93/100 | Loss: 0.6277\n",
            "Epoch: 94/100 | Loss: 0.6274\n",
            "Epoch: 95/100 | Loss: 0.6272\n",
            "Epoch: 96/100 | Loss: 0.6269\n",
            "Epoch: 97/100 | Loss: 0.6266\n",
            "Epoch: 98/100 | Loss: 0.6263\n",
            "Epoch: 99/100 | Loss: 0.6261\n",
            "Epoch: 100/100 | Loss: 0.6258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4gcwsdUoVeI"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-RCAb7YiSpS"
      },
      "source": [
        "class DiabetesDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('/content/gdrive/My Drive/Colab Notebooks/ml_in_practice_data/data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyODmtaYrktt"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpy90qHWiSmw",
        "outputId": "8c93f5d8-5379-4d0b-e604-f464ba686f5e"
      },
      "source": [
        "for epoch in range(100):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, labels)\n",
        "        print(f'Epoch: {epoch + i} | Batch: {i + 1} | Loss: {loss.item(): .4f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Batch: 1 | Loss:  0.5812\n",
            "Epoch: 1 | Batch: 2 | Loss:  0.5200\n",
            "Epoch: 2 | Batch: 3 | Loss:  0.6712\n",
            "Epoch: 3 | Batch: 4 | Loss:  0.6610\n",
            "Epoch: 4 | Batch: 5 | Loss:  0.5742\n",
            "Epoch: 5 | Batch: 6 | Loss:  0.6919\n",
            "Epoch: 6 | Batch: 7 | Loss:  0.6304\n",
            "Epoch: 7 | Batch: 8 | Loss:  0.5412\n",
            "Epoch: 8 | Batch: 9 | Loss:  0.4675\n",
            "Epoch: 9 | Batch: 10 | Loss:  0.5964\n",
            "Epoch: 10 | Batch: 11 | Loss:  0.5491\n",
            "Epoch: 11 | Batch: 12 | Loss:  0.6512\n",
            "Epoch: 12 | Batch: 13 | Loss:  0.6980\n",
            "Epoch: 13 | Batch: 14 | Loss:  0.6462\n",
            "Epoch: 14 | Batch: 15 | Loss:  0.6202\n",
            "Epoch: 15 | Batch: 16 | Loss:  0.5362\n",
            "Epoch: 16 | Batch: 17 | Loss:  0.6494\n",
            "Epoch: 17 | Batch: 18 | Loss:  0.5590\n",
            "Epoch: 18 | Batch: 19 | Loss:  0.5946\n",
            "Epoch: 19 | Batch: 20 | Loss:  0.6702\n",
            "Epoch: 20 | Batch: 21 | Loss:  0.6375\n",
            "Epoch: 21 | Batch: 22 | Loss:  0.5569\n",
            "Epoch: 22 | Batch: 23 | Loss:  0.5778\n",
            "Epoch: 23 | Batch: 24 | Loss:  0.5276\n",
            "Epoch: 1 | Batch: 1 | Loss:  0.5924\n",
            "Epoch: 2 | Batch: 2 | Loss:  0.5964\n",
            "Epoch: 3 | Batch: 3 | Loss:  0.7307\n",
            "Epoch: 4 | Batch: 4 | Loss:  0.5864\n",
            "Epoch: 5 | Batch: 5 | Loss:  0.4958\n",
            "Epoch: 6 | Batch: 6 | Loss:  0.6613\n",
            "Epoch: 7 | Batch: 7 | Loss:  0.5761\n",
            "Epoch: 8 | Batch: 8 | Loss:  0.5685\n",
            "Epoch: 9 | Batch: 9 | Loss:  0.5177\n",
            "Epoch: 10 | Batch: 10 | Loss:  0.5212\n",
            "Epoch: 11 | Batch: 11 | Loss:  0.5164\n",
            "Epoch: 12 | Batch: 12 | Loss:  0.4344\n",
            "Epoch: 13 | Batch: 13 | Loss:  0.6003\n",
            "Epoch: 14 | Batch: 14 | Loss:  0.6303\n",
            "Epoch: 15 | Batch: 15 | Loss:  0.5925\n",
            "Epoch: 16 | Batch: 16 | Loss:  0.6502\n",
            "Epoch: 17 | Batch: 17 | Loss:  0.6147\n",
            "Epoch: 18 | Batch: 18 | Loss:  0.6231\n",
            "Epoch: 19 | Batch: 19 | Loss:  0.5663\n",
            "Epoch: 20 | Batch: 20 | Loss:  0.6581\n",
            "Epoch: 21 | Batch: 21 | Loss:  0.5283\n",
            "Epoch: 22 | Batch: 22 | Loss:  0.5686\n",
            "Epoch: 23 | Batch: 23 | Loss:  0.6179\n",
            "Epoch: 24 | Batch: 24 | Loss:  0.5855\n",
            "Epoch: 2 | Batch: 1 | Loss:  0.5998\n",
            "Epoch: 3 | Batch: 2 | Loss:  0.5639\n",
            "Epoch: 4 | Batch: 3 | Loss:  0.5959\n",
            "Epoch: 5 | Batch: 4 | Loss:  0.5851\n",
            "Epoch: 6 | Batch: 5 | Loss:  0.5996\n",
            "Epoch: 7 | Batch: 6 | Loss:  0.5585\n",
            "Epoch: 8 | Batch: 7 | Loss:  0.4725\n",
            "Epoch: 9 | Batch: 8 | Loss:  0.6698\n",
            "Epoch: 10 | Batch: 9 | Loss:  0.5104\n",
            "Epoch: 11 | Batch: 10 | Loss:  0.5346\n",
            "Epoch: 12 | Batch: 11 | Loss:  0.5901\n",
            "Epoch: 13 | Batch: 12 | Loss:  0.5312\n",
            "Epoch: 14 | Batch: 13 | Loss:  0.6879\n",
            "Epoch: 15 | Batch: 14 | Loss:  0.5381\n",
            "Epoch: 16 | Batch: 15 | Loss:  0.6439\n",
            "Epoch: 17 | Batch: 16 | Loss:  0.5161\n",
            "Epoch: 18 | Batch: 17 | Loss:  0.5873\n",
            "Epoch: 19 | Batch: 18 | Loss:  0.5809\n",
            "Epoch: 20 | Batch: 19 | Loss:  0.5275\n",
            "Epoch: 21 | Batch: 20 | Loss:  0.4407\n",
            "Epoch: 22 | Batch: 21 | Loss:  0.5693\n",
            "Epoch: 23 | Batch: 22 | Loss:  0.5784\n",
            "Epoch: 24 | Batch: 23 | Loss:  0.5451\n",
            "Epoch: 25 | Batch: 24 | Loss:  0.5316\n",
            "Epoch: 3 | Batch: 1 | Loss:  0.4905\n",
            "Epoch: 4 | Batch: 2 | Loss:  0.5758\n",
            "Epoch: 5 | Batch: 3 | Loss:  0.5763\n",
            "Epoch: 6 | Batch: 4 | Loss:  0.5621\n",
            "Epoch: 7 | Batch: 5 | Loss:  0.5068\n",
            "Epoch: 8 | Batch: 6 | Loss:  0.5691\n",
            "Epoch: 9 | Batch: 7 | Loss:  0.6056\n",
            "Epoch: 10 | Batch: 8 | Loss:  0.5501\n",
            "Epoch: 11 | Batch: 9 | Loss:  0.5950\n",
            "Epoch: 12 | Batch: 10 | Loss:  0.5726\n",
            "Epoch: 13 | Batch: 11 | Loss:  0.4838\n",
            "Epoch: 14 | Batch: 12 | Loss:  0.5964\n",
            "Epoch: 15 | Batch: 13 | Loss:  0.5153\n",
            "Epoch: 16 | Batch: 14 | Loss:  0.4482\n",
            "Epoch: 17 | Batch: 15 | Loss:  0.4978\n",
            "Epoch: 18 | Batch: 16 | Loss:  0.5454\n",
            "Epoch: 19 | Batch: 17 | Loss:  0.6215\n",
            "Epoch: 20 | Batch: 18 | Loss:  0.4106\n",
            "Epoch: 21 | Batch: 19 | Loss:  0.5643\n",
            "Epoch: 22 | Batch: 20 | Loss:  0.6233\n",
            "Epoch: 23 | Batch: 21 | Loss:  0.5695\n",
            "Epoch: 24 | Batch: 22 | Loss:  0.5479\n",
            "Epoch: 25 | Batch: 23 | Loss:  0.5128\n",
            "Epoch: 26 | Batch: 24 | Loss:  0.5289\n",
            "Epoch: 4 | Batch: 1 | Loss:  0.5420\n",
            "Epoch: 5 | Batch: 2 | Loss:  0.5056\n",
            "Epoch: 6 | Batch: 3 | Loss:  0.5503\n",
            "Epoch: 7 | Batch: 4 | Loss:  0.4905\n",
            "Epoch: 8 | Batch: 5 | Loss:  0.4857\n",
            "Epoch: 9 | Batch: 6 | Loss:  0.4744\n",
            "Epoch: 10 | Batch: 7 | Loss:  0.4071\n",
            "Epoch: 11 | Batch: 8 | Loss:  0.5218\n",
            "Epoch: 12 | Batch: 9 | Loss:  0.5574\n",
            "Epoch: 13 | Batch: 10 | Loss:  0.5015\n",
            "Epoch: 14 | Batch: 11 | Loss:  0.5745\n",
            "Epoch: 15 | Batch: 12 | Loss:  0.4131\n",
            "Epoch: 16 | Batch: 13 | Loss:  0.5515\n",
            "Epoch: 17 | Batch: 14 | Loss:  0.6424\n",
            "Epoch: 18 | Batch: 15 | Loss:  0.5369\n",
            "Epoch: 19 | Batch: 16 | Loss:  0.5481\n",
            "Epoch: 20 | Batch: 17 | Loss:  0.5516\n",
            "Epoch: 21 | Batch: 18 | Loss:  0.4742\n",
            "Epoch: 22 | Batch: 19 | Loss:  0.6626\n",
            "Epoch: 23 | Batch: 20 | Loss:  0.4860\n",
            "Epoch: 24 | Batch: 21 | Loss:  0.4290\n",
            "Epoch: 25 | Batch: 22 | Loss:  0.5923\n",
            "Epoch: 26 | Batch: 23 | Loss:  0.5840\n",
            "Epoch: 27 | Batch: 24 | Loss:  0.4959\n",
            "Epoch: 5 | Batch: 1 | Loss:  0.4730\n",
            "Epoch: 6 | Batch: 2 | Loss:  0.5097\n",
            "Epoch: 7 | Batch: 3 | Loss:  0.4316\n",
            "Epoch: 8 | Batch: 4 | Loss:  0.4820\n",
            "Epoch: 9 | Batch: 5 | Loss:  0.5032\n",
            "Epoch: 10 | Batch: 6 | Loss:  0.5565\n",
            "Epoch: 11 | Batch: 7 | Loss:  0.4949\n",
            "Epoch: 12 | Batch: 8 | Loss:  0.5367\n",
            "Epoch: 13 | Batch: 9 | Loss:  0.5517\n",
            "Epoch: 14 | Batch: 10 | Loss:  0.5345\n",
            "Epoch: 15 | Batch: 11 | Loss:  0.4642\n",
            "Epoch: 16 | Batch: 12 | Loss:  0.5088\n",
            "Epoch: 17 | Batch: 13 | Loss:  0.4532\n",
            "Epoch: 18 | Batch: 14 | Loss:  0.6162\n",
            "Epoch: 19 | Batch: 15 | Loss:  0.5056\n",
            "Epoch: 20 | Batch: 16 | Loss:  0.5283\n",
            "Epoch: 21 | Batch: 17 | Loss:  0.5667\n",
            "Epoch: 22 | Batch: 18 | Loss:  0.3954\n",
            "Epoch: 23 | Batch: 19 | Loss:  0.5772\n",
            "Epoch: 24 | Batch: 20 | Loss:  0.4804\n",
            "Epoch: 25 | Batch: 21 | Loss:  0.5128\n",
            "Epoch: 26 | Batch: 22 | Loss:  0.4587\n",
            "Epoch: 27 | Batch: 23 | Loss:  0.5009\n",
            "Epoch: 28 | Batch: 24 | Loss:  0.5582\n",
            "Epoch: 6 | Batch: 1 | Loss:  0.4549\n",
            "Epoch: 7 | Batch: 2 | Loss:  0.4877\n",
            "Epoch: 8 | Batch: 3 | Loss:  0.4670\n",
            "Epoch: 9 | Batch: 4 | Loss:  0.5335\n",
            "Epoch: 10 | Batch: 5 | Loss:  0.5575\n",
            "Epoch: 11 | Batch: 6 | Loss:  0.4948\n",
            "Epoch: 12 | Batch: 7 | Loss:  0.4847\n",
            "Epoch: 13 | Batch: 8 | Loss:  0.5222\n",
            "Epoch: 14 | Batch: 9 | Loss:  0.5073\n",
            "Epoch: 15 | Batch: 10 | Loss:  0.4302\n",
            "Epoch: 16 | Batch: 11 | Loss:  0.5778\n",
            "Epoch: 17 | Batch: 12 | Loss:  0.5419\n",
            "Epoch: 18 | Batch: 13 | Loss:  0.5801\n",
            "Epoch: 19 | Batch: 14 | Loss:  0.3709\n",
            "Epoch: 20 | Batch: 15 | Loss:  0.4430\n",
            "Epoch: 21 | Batch: 16 | Loss:  0.4598\n",
            "Epoch: 22 | Batch: 17 | Loss:  0.5412\n",
            "Epoch: 23 | Batch: 18 | Loss:  0.4986\n",
            "Epoch: 24 | Batch: 19 | Loss:  0.4194\n",
            "Epoch: 25 | Batch: 20 | Loss:  0.5250\n",
            "Epoch: 26 | Batch: 21 | Loss:  0.5401\n",
            "Epoch: 27 | Batch: 22 | Loss:  0.5550\n",
            "Epoch: 28 | Batch: 23 | Loss:  0.4618\n",
            "Epoch: 29 | Batch: 24 | Loss:  0.4225\n",
            "Epoch: 7 | Batch: 1 | Loss:  0.4803\n",
            "Epoch: 8 | Batch: 2 | Loss:  0.4036\n",
            "Epoch: 9 | Batch: 3 | Loss:  0.5438\n",
            "Epoch: 10 | Batch: 4 | Loss:  0.5125\n",
            "Epoch: 11 | Batch: 5 | Loss:  0.5845\n",
            "Epoch: 12 | Batch: 6 | Loss:  0.5741\n",
            "Epoch: 13 | Batch: 7 | Loss:  0.5086\n",
            "Epoch: 14 | Batch: 8 | Loss:  0.5300\n",
            "Epoch: 15 | Batch: 9 | Loss:  0.5214\n",
            "Epoch: 16 | Batch: 10 | Loss:  0.4961\n",
            "Epoch: 17 | Batch: 11 | Loss:  0.4546\n",
            "Epoch: 18 | Batch: 12 | Loss:  0.4275\n",
            "Epoch: 19 | Batch: 13 | Loss:  0.5135\n",
            "Epoch: 20 | Batch: 14 | Loss:  0.3332\n",
            "Epoch: 21 | Batch: 15 | Loss:  0.4653\n",
            "Epoch: 22 | Batch: 16 | Loss:  0.3592\n",
            "Epoch: 23 | Batch: 17 | Loss:  0.4943\n",
            "Epoch: 24 | Batch: 18 | Loss:  0.4514\n",
            "Epoch: 25 | Batch: 19 | Loss:  0.5979\n",
            "Epoch: 26 | Batch: 20 | Loss:  0.4485\n",
            "Epoch: 27 | Batch: 21 | Loss:  0.5606\n",
            "Epoch: 28 | Batch: 22 | Loss:  0.5498\n",
            "Epoch: 29 | Batch: 23 | Loss:  0.4002\n",
            "Epoch: 30 | Batch: 24 | Loss:  0.4898\n",
            "Epoch: 8 | Batch: 1 | Loss:  0.3615\n",
            "Epoch: 9 | Batch: 2 | Loss:  0.4255\n",
            "Epoch: 10 | Batch: 3 | Loss:  0.5617\n",
            "Epoch: 11 | Batch: 4 | Loss:  0.4281\n",
            "Epoch: 12 | Batch: 5 | Loss:  0.4991\n",
            "Epoch: 13 | Batch: 6 | Loss:  0.4024\n",
            "Epoch: 14 | Batch: 7 | Loss:  0.3822\n",
            "Epoch: 15 | Batch: 8 | Loss:  0.4128\n",
            "Epoch: 16 | Batch: 9 | Loss:  0.5312\n",
            "Epoch: 17 | Batch: 10 | Loss:  0.5192\n",
            "Epoch: 18 | Batch: 11 | Loss:  0.4940\n",
            "Epoch: 19 | Batch: 12 | Loss:  0.6373\n",
            "Epoch: 20 | Batch: 13 | Loss:  0.4640\n",
            "Epoch: 21 | Batch: 14 | Loss:  0.6646\n",
            "Epoch: 22 | Batch: 15 | Loss:  0.6269\n",
            "Epoch: 23 | Batch: 16 | Loss:  0.5076\n",
            "Epoch: 24 | Batch: 17 | Loss:  0.4186\n",
            "Epoch: 25 | Batch: 18 | Loss:  0.4282\n",
            "Epoch: 26 | Batch: 19 | Loss:  0.4413\n",
            "Epoch: 27 | Batch: 20 | Loss:  0.3317\n",
            "Epoch: 28 | Batch: 21 | Loss:  0.6002\n",
            "Epoch: 29 | Batch: 22 | Loss:  0.4348\n",
            "Epoch: 30 | Batch: 23 | Loss:  0.4813\n",
            "Epoch: 31 | Batch: 24 | Loss:  0.4721\n",
            "Epoch: 9 | Batch: 1 | Loss:  0.5828\n",
            "Epoch: 10 | Batch: 2 | Loss:  0.5687\n",
            "Epoch: 11 | Batch: 3 | Loss:  0.4606\n",
            "Epoch: 12 | Batch: 4 | Loss:  0.4647\n",
            "Epoch: 13 | Batch: 5 | Loss:  0.4833\n",
            "Epoch: 14 | Batch: 6 | Loss:  0.5318\n",
            "Epoch: 15 | Batch: 7 | Loss:  0.5519\n",
            "Epoch: 16 | Batch: 8 | Loss:  0.3976\n",
            "Epoch: 17 | Batch: 9 | Loss:  0.3748\n",
            "Epoch: 18 | Batch: 10 | Loss:  0.4451\n",
            "Epoch: 19 | Batch: 11 | Loss:  0.4333\n",
            "Epoch: 20 | Batch: 12 | Loss:  0.3912\n",
            "Epoch: 21 | Batch: 13 | Loss:  0.4924\n",
            "Epoch: 22 | Batch: 14 | Loss:  0.3743\n",
            "Epoch: 23 | Batch: 15 | Loss:  0.5123\n",
            "Epoch: 24 | Batch: 16 | Loss:  0.5006\n",
            "Epoch: 25 | Batch: 17 | Loss:  0.3960\n",
            "Epoch: 26 | Batch: 18 | Loss:  0.5006\n",
            "Epoch: 27 | Batch: 19 | Loss:  0.5613\n",
            "Epoch: 28 | Batch: 20 | Loss:  0.4590\n",
            "Epoch: 29 | Batch: 21 | Loss:  0.3995\n",
            "Epoch: 30 | Batch: 22 | Loss:  0.5234\n",
            "Epoch: 31 | Batch: 23 | Loss:  0.4823\n",
            "Epoch: 32 | Batch: 24 | Loss:  0.6168\n",
            "Epoch: 10 | Batch: 1 | Loss:  0.5587\n",
            "Epoch: 11 | Batch: 2 | Loss:  0.4589\n",
            "Epoch: 12 | Batch: 3 | Loss:  0.4416\n",
            "Epoch: 13 | Batch: 4 | Loss:  0.4914\n",
            "Epoch: 14 | Batch: 5 | Loss:  0.3312\n",
            "Epoch: 15 | Batch: 6 | Loss:  0.6172\n",
            "Epoch: 16 | Batch: 7 | Loss:  0.5245\n",
            "Epoch: 17 | Batch: 8 | Loss:  0.5973\n",
            "Epoch: 18 | Batch: 9 | Loss:  0.5654\n",
            "Epoch: 19 | Batch: 10 | Loss:  0.5476\n",
            "Epoch: 20 | Batch: 11 | Loss:  0.4350\n",
            "Epoch: 21 | Batch: 12 | Loss:  0.3878\n",
            "Epoch: 22 | Batch: 13 | Loss:  0.3670\n",
            "Epoch: 23 | Batch: 14 | Loss:  0.3945\n",
            "Epoch: 24 | Batch: 15 | Loss:  0.6227\n",
            "Epoch: 25 | Batch: 16 | Loss:  0.5060\n",
            "Epoch: 26 | Batch: 17 | Loss:  0.5762\n",
            "Epoch: 27 | Batch: 18 | Loss:  0.4225\n",
            "Epoch: 28 | Batch: 19 | Loss:  0.4422\n",
            "Epoch: 29 | Batch: 20 | Loss:  0.4125\n",
            "Epoch: 30 | Batch: 21 | Loss:  0.4858\n",
            "Epoch: 31 | Batch: 22 | Loss:  0.5116\n",
            "Epoch: 32 | Batch: 23 | Loss:  0.3313\n",
            "Epoch: 33 | Batch: 24 | Loss:  0.3601\n",
            "Epoch: 11 | Batch: 1 | Loss:  0.3988\n",
            "Epoch: 12 | Batch: 2 | Loss:  0.4777\n",
            "Epoch: 13 | Batch: 3 | Loss:  0.3803\n",
            "Epoch: 14 | Batch: 4 | Loss:  0.5159\n",
            "Epoch: 15 | Batch: 5 | Loss:  0.3234\n",
            "Epoch: 16 | Batch: 6 | Loss:  0.6367\n",
            "Epoch: 17 | Batch: 7 | Loss:  0.3803\n",
            "Epoch: 18 | Batch: 8 | Loss:  0.4066\n",
            "Epoch: 19 | Batch: 9 | Loss:  0.6216\n",
            "Epoch: 20 | Batch: 10 | Loss:  0.4438\n",
            "Epoch: 21 | Batch: 11 | Loss:  0.6173\n",
            "Epoch: 22 | Batch: 12 | Loss:  0.3753\n",
            "Epoch: 23 | Batch: 13 | Loss:  0.3479\n",
            "Epoch: 24 | Batch: 14 | Loss:  0.4014\n",
            "Epoch: 25 | Batch: 15 | Loss:  0.5141\n",
            "Epoch: 26 | Batch: 16 | Loss:  0.4945\n",
            "Epoch: 27 | Batch: 17 | Loss:  0.4104\n",
            "Epoch: 28 | Batch: 18 | Loss:  0.6086\n",
            "Epoch: 29 | Batch: 19 | Loss:  0.5298\n",
            "Epoch: 30 | Batch: 20 | Loss:  0.4464\n",
            "Epoch: 31 | Batch: 21 | Loss:  0.3898\n",
            "Epoch: 32 | Batch: 22 | Loss:  0.6313\n",
            "Epoch: 33 | Batch: 23 | Loss:  0.4421\n",
            "Epoch: 34 | Batch: 24 | Loss:  0.5216\n",
            "Epoch: 12 | Batch: 1 | Loss:  0.4984\n",
            "Epoch: 13 | Batch: 2 | Loss:  0.4134\n",
            "Epoch: 14 | Batch: 3 | Loss:  0.4500\n",
            "Epoch: 15 | Batch: 4 | Loss:  0.5962\n",
            "Epoch: 16 | Batch: 5 | Loss:  0.6853\n",
            "Epoch: 17 | Batch: 6 | Loss:  0.2694\n",
            "Epoch: 18 | Batch: 7 | Loss:  0.4638\n",
            "Epoch: 19 | Batch: 8 | Loss:  0.2994\n",
            "Epoch: 20 | Batch: 9 | Loss:  0.4904\n",
            "Epoch: 21 | Batch: 10 | Loss:  0.4249\n",
            "Epoch: 22 | Batch: 11 | Loss:  0.5712\n",
            "Epoch: 23 | Batch: 12 | Loss:  0.4586\n",
            "Epoch: 24 | Batch: 13 | Loss:  0.5846\n",
            "Epoch: 25 | Batch: 14 | Loss:  0.3917\n",
            "Epoch: 26 | Batch: 15 | Loss:  0.4888\n",
            "Epoch: 27 | Batch: 16 | Loss:  0.4315\n",
            "Epoch: 28 | Batch: 17 | Loss:  0.4229\n",
            "Epoch: 29 | Batch: 18 | Loss:  0.4948\n",
            "Epoch: 30 | Batch: 19 | Loss:  0.4299\n",
            "Epoch: 31 | Batch: 20 | Loss:  0.4085\n",
            "Epoch: 32 | Batch: 21 | Loss:  0.5058\n",
            "Epoch: 33 | Batch: 22 | Loss:  0.5329\n",
            "Epoch: 34 | Batch: 23 | Loss:  0.5454\n",
            "Epoch: 35 | Batch: 24 | Loss:  0.5097\n",
            "Epoch: 13 | Batch: 1 | Loss:  0.4657\n",
            "Epoch: 14 | Batch: 2 | Loss:  0.4995\n",
            "Epoch: 15 | Batch: 3 | Loss:  0.6154\n",
            "Epoch: 16 | Batch: 4 | Loss:  0.4990\n",
            "Epoch: 17 | Batch: 5 | Loss:  0.4075\n",
            "Epoch: 18 | Batch: 6 | Loss:  0.5973\n",
            "Epoch: 19 | Batch: 7 | Loss:  0.3596\n",
            "Epoch: 20 | Batch: 8 | Loss:  0.3127\n",
            "Epoch: 21 | Batch: 9 | Loss:  0.3193\n",
            "Epoch: 22 | Batch: 10 | Loss:  0.4882\n",
            "Epoch: 23 | Batch: 11 | Loss:  0.5529\n",
            "Epoch: 24 | Batch: 12 | Loss:  0.3483\n",
            "Epoch: 25 | Batch: 13 | Loss:  0.5133\n",
            "Epoch: 26 | Batch: 14 | Loss:  0.5807\n",
            "Epoch: 27 | Batch: 15 | Loss:  0.4531\n",
            "Epoch: 28 | Batch: 16 | Loss:  0.4194\n",
            "Epoch: 29 | Batch: 17 | Loss:  0.4393\n",
            "Epoch: 30 | Batch: 18 | Loss:  0.5297\n",
            "Epoch: 31 | Batch: 19 | Loss:  0.4630\n",
            "Epoch: 32 | Batch: 20 | Loss:  0.4490\n",
            "Epoch: 33 | Batch: 21 | Loss:  0.4242\n",
            "Epoch: 34 | Batch: 22 | Loss:  0.5993\n",
            "Epoch: 35 | Batch: 23 | Loss:  0.4981\n",
            "Epoch: 36 | Batch: 24 | Loss:  0.3955\n",
            "Epoch: 14 | Batch: 1 | Loss:  0.4505\n",
            "Epoch: 15 | Batch: 2 | Loss:  0.4480\n",
            "Epoch: 16 | Batch: 3 | Loss:  0.4131\n",
            "Epoch: 17 | Batch: 4 | Loss:  0.6459\n",
            "Epoch: 18 | Batch: 5 | Loss:  0.4018\n",
            "Epoch: 19 | Batch: 6 | Loss:  0.4236\n",
            "Epoch: 20 | Batch: 7 | Loss:  0.5429\n",
            "Epoch: 21 | Batch: 8 | Loss:  0.4419\n",
            "Epoch: 22 | Batch: 9 | Loss:  0.4374\n",
            "Epoch: 23 | Batch: 10 | Loss:  0.6146\n",
            "Epoch: 24 | Batch: 11 | Loss:  0.3752\n",
            "Epoch: 25 | Batch: 12 | Loss:  0.3518\n",
            "Epoch: 26 | Batch: 13 | Loss:  0.4454\n",
            "Epoch: 27 | Batch: 14 | Loss:  0.5599\n",
            "Epoch: 28 | Batch: 15 | Loss:  0.3880\n",
            "Epoch: 29 | Batch: 16 | Loss:  0.4457\n",
            "Epoch: 30 | Batch: 17 | Loss:  0.4352\n",
            "Epoch: 31 | Batch: 18 | Loss:  0.6757\n",
            "Epoch: 32 | Batch: 19 | Loss:  0.3805\n",
            "Epoch: 33 | Batch: 20 | Loss:  0.6325\n",
            "Epoch: 34 | Batch: 21 | Loss:  0.3739\n",
            "Epoch: 35 | Batch: 22 | Loss:  0.4369\n",
            "Epoch: 36 | Batch: 23 | Loss:  0.3843\n",
            "Epoch: 37 | Batch: 24 | Loss:  0.5380\n",
            "Epoch: 15 | Batch: 1 | Loss:  0.4968\n",
            "Epoch: 16 | Batch: 2 | Loss:  0.5924\n",
            "Epoch: 17 | Batch: 3 | Loss:  0.2968\n",
            "Epoch: 18 | Batch: 4 | Loss:  0.4607\n",
            "Epoch: 19 | Batch: 5 | Loss:  0.3445\n",
            "Epoch: 20 | Batch: 6 | Loss:  0.4030\n",
            "Epoch: 21 | Batch: 7 | Loss:  0.4071\n",
            "Epoch: 22 | Batch: 8 | Loss:  0.5196\n",
            "Epoch: 23 | Batch: 9 | Loss:  0.4960\n",
            "Epoch: 24 | Batch: 10 | Loss:  0.5052\n",
            "Epoch: 25 | Batch: 11 | Loss:  0.8069\n",
            "Epoch: 26 | Batch: 12 | Loss:  0.5561\n",
            "Epoch: 27 | Batch: 13 | Loss:  0.3275\n",
            "Epoch: 28 | Batch: 14 | Loss:  0.5317\n",
            "Epoch: 29 | Batch: 15 | Loss:  0.3831\n",
            "Epoch: 30 | Batch: 16 | Loss:  0.4081\n",
            "Epoch: 31 | Batch: 17 | Loss:  0.3795\n",
            "Epoch: 32 | Batch: 18 | Loss:  0.3899\n",
            "Epoch: 33 | Batch: 19 | Loss:  0.4715\n",
            "Epoch: 34 | Batch: 20 | Loss:  0.3557\n",
            "Epoch: 35 | Batch: 21 | Loss:  0.6207\n",
            "Epoch: 36 | Batch: 22 | Loss:  0.4677\n",
            "Epoch: 37 | Batch: 23 | Loss:  0.5273\n",
            "Epoch: 38 | Batch: 24 | Loss:  0.5450\n",
            "Epoch: 16 | Batch: 1 | Loss:  0.4063\n",
            "Epoch: 17 | Batch: 2 | Loss:  0.5522\n",
            "Epoch: 18 | Batch: 3 | Loss:  0.2509\n",
            "Epoch: 19 | Batch: 4 | Loss:  0.5437\n",
            "Epoch: 20 | Batch: 5 | Loss:  0.5385\n",
            "Epoch: 21 | Batch: 6 | Loss:  0.5407\n",
            "Epoch: 22 | Batch: 7 | Loss:  0.3690\n",
            "Epoch: 23 | Batch: 8 | Loss:  0.4532\n",
            "Epoch: 24 | Batch: 9 | Loss:  0.3511\n",
            "Epoch: 25 | Batch: 10 | Loss:  0.6254\n",
            "Epoch: 26 | Batch: 11 | Loss:  0.5140\n",
            "Epoch: 27 | Batch: 12 | Loss:  0.5707\n",
            "Epoch: 28 | Batch: 13 | Loss:  0.6048\n",
            "Epoch: 29 | Batch: 14 | Loss:  0.5136\n",
            "Epoch: 30 | Batch: 15 | Loss:  0.5679\n",
            "Epoch: 31 | Batch: 16 | Loss:  0.3953\n",
            "Epoch: 32 | Batch: 17 | Loss:  0.3786\n",
            "Epoch: 33 | Batch: 18 | Loss:  0.3890\n",
            "Epoch: 34 | Batch: 19 | Loss:  0.4531\n",
            "Epoch: 35 | Batch: 20 | Loss:  0.3543\n",
            "Epoch: 36 | Batch: 21 | Loss:  0.2946\n",
            "Epoch: 37 | Batch: 22 | Loss:  0.5043\n",
            "Epoch: 38 | Batch: 23 | Loss:  0.6197\n",
            "Epoch: 39 | Batch: 24 | Loss:  0.5774\n",
            "Epoch: 17 | Batch: 1 | Loss:  0.4770\n",
            "Epoch: 18 | Batch: 2 | Loss:  0.6365\n",
            "Epoch: 19 | Batch: 3 | Loss:  0.3695\n",
            "Epoch: 20 | Batch: 4 | Loss:  0.4298\n",
            "Epoch: 21 | Batch: 5 | Loss:  0.4446\n",
            "Epoch: 22 | Batch: 6 | Loss:  0.4203\n",
            "Epoch: 23 | Batch: 7 | Loss:  0.4490\n",
            "Epoch: 24 | Batch: 8 | Loss:  0.3983\n",
            "Epoch: 25 | Batch: 9 | Loss:  0.5554\n",
            "Epoch: 26 | Batch: 10 | Loss:  0.3866\n",
            "Epoch: 27 | Batch: 11 | Loss:  0.3430\n",
            "Epoch: 28 | Batch: 12 | Loss:  0.2770\n",
            "Epoch: 29 | Batch: 13 | Loss:  0.3265\n",
            "Epoch: 30 | Batch: 14 | Loss:  0.3853\n",
            "Epoch: 31 | Batch: 15 | Loss:  0.5438\n",
            "Epoch: 32 | Batch: 16 | Loss:  0.5898\n",
            "Epoch: 33 | Batch: 17 | Loss:  0.5957\n",
            "Epoch: 34 | Batch: 18 | Loss:  0.5282\n",
            "Epoch: 35 | Batch: 19 | Loss:  0.3543\n",
            "Epoch: 36 | Batch: 20 | Loss:  0.4247\n",
            "Epoch: 37 | Batch: 21 | Loss:  0.5928\n",
            "Epoch: 38 | Batch: 22 | Loss:  0.6602\n",
            "Epoch: 39 | Batch: 23 | Loss:  0.4485\n",
            "Epoch: 40 | Batch: 24 | Loss:  0.5723\n",
            "Epoch: 18 | Batch: 1 | Loss:  0.4917\n",
            "Epoch: 19 | Batch: 2 | Loss:  0.5750\n",
            "Epoch: 20 | Batch: 3 | Loss:  0.5713\n",
            "Epoch: 21 | Batch: 4 | Loss:  0.4241\n",
            "Epoch: 22 | Batch: 5 | Loss:  0.3965\n",
            "Epoch: 23 | Batch: 6 | Loss:  0.5966\n",
            "Epoch: 24 | Batch: 7 | Loss:  0.5952\n",
            "Epoch: 25 | Batch: 8 | Loss:  0.5534\n",
            "Epoch: 26 | Batch: 9 | Loss:  0.3007\n",
            "Epoch: 27 | Batch: 10 | Loss:  0.4640\n",
            "Epoch: 28 | Batch: 11 | Loss:  0.4241\n",
            "Epoch: 29 | Batch: 12 | Loss:  0.5612\n",
            "Epoch: 30 | Batch: 13 | Loss:  0.4783\n",
            "Epoch: 31 | Batch: 14 | Loss:  0.4336\n",
            "Epoch: 32 | Batch: 15 | Loss:  0.4668\n",
            "Epoch: 33 | Batch: 16 | Loss:  0.3423\n",
            "Epoch: 34 | Batch: 17 | Loss:  0.5140\n",
            "Epoch: 35 | Batch: 18 | Loss:  0.5580\n",
            "Epoch: 36 | Batch: 19 | Loss:  0.4580\n",
            "Epoch: 37 | Batch: 20 | Loss:  0.5008\n",
            "Epoch: 38 | Batch: 21 | Loss:  0.2905\n",
            "Epoch: 39 | Batch: 22 | Loss:  0.3629\n",
            "Epoch: 40 | Batch: 23 | Loss:  0.4077\n",
            "Epoch: 41 | Batch: 24 | Loss:  0.4119\n",
            "Epoch: 19 | Batch: 1 | Loss:  0.3361\n",
            "Epoch: 20 | Batch: 2 | Loss:  0.4683\n",
            "Epoch: 21 | Batch: 3 | Loss:  0.3586\n",
            "Epoch: 22 | Batch: 4 | Loss:  0.6510\n",
            "Epoch: 23 | Batch: 5 | Loss:  0.3559\n",
            "Epoch: 24 | Batch: 6 | Loss:  0.4386\n",
            "Epoch: 25 | Batch: 7 | Loss:  0.5359\n",
            "Epoch: 26 | Batch: 8 | Loss:  0.5070\n",
            "Epoch: 27 | Batch: 9 | Loss:  0.5447\n",
            "Epoch: 28 | Batch: 10 | Loss:  0.5218\n",
            "Epoch: 29 | Batch: 11 | Loss:  0.4149\n",
            "Epoch: 30 | Batch: 12 | Loss:  0.3961\n",
            "Epoch: 31 | Batch: 13 | Loss:  0.4429\n",
            "Epoch: 32 | Batch: 14 | Loss:  0.5072\n",
            "Epoch: 33 | Batch: 15 | Loss:  0.4283\n",
            "Epoch: 34 | Batch: 16 | Loss:  0.3799\n",
            "Epoch: 35 | Batch: 17 | Loss:  0.4889\n",
            "Epoch: 36 | Batch: 18 | Loss:  0.5368\n",
            "Epoch: 37 | Batch: 19 | Loss:  0.5251\n",
            "Epoch: 38 | Batch: 20 | Loss:  0.5143\n",
            "Epoch: 39 | Batch: 21 | Loss:  0.6022\n",
            "Epoch: 40 | Batch: 22 | Loss:  0.4229\n",
            "Epoch: 41 | Batch: 23 | Loss:  0.3883\n",
            "Epoch: 42 | Batch: 24 | Loss:  0.3957\n",
            "Epoch: 20 | Batch: 1 | Loss:  0.5019\n",
            "Epoch: 21 | Batch: 2 | Loss:  0.5475\n",
            "Epoch: 22 | Batch: 3 | Loss:  0.5488\n",
            "Epoch: 23 | Batch: 4 | Loss:  0.4409\n",
            "Epoch: 24 | Batch: 5 | Loss:  0.4876\n",
            "Epoch: 25 | Batch: 6 | Loss:  0.4333\n",
            "Epoch: 26 | Batch: 7 | Loss:  0.5223\n",
            "Epoch: 27 | Batch: 8 | Loss:  0.4742\n",
            "Epoch: 28 | Batch: 9 | Loss:  0.3273\n",
            "Epoch: 29 | Batch: 10 | Loss:  0.3716\n",
            "Epoch: 30 | Batch: 11 | Loss:  0.4315\n",
            "Epoch: 31 | Batch: 12 | Loss:  0.6273\n",
            "Epoch: 32 | Batch: 13 | Loss:  0.4911\n",
            "Epoch: 33 | Batch: 14 | Loss:  0.4069\n",
            "Epoch: 34 | Batch: 15 | Loss:  0.3967\n",
            "Epoch: 35 | Batch: 16 | Loss:  0.5513\n",
            "Epoch: 36 | Batch: 17 | Loss:  0.4980\n",
            "Epoch: 37 | Batch: 18 | Loss:  0.4327\n",
            "Epoch: 38 | Batch: 19 | Loss:  0.4240\n",
            "Epoch: 39 | Batch: 20 | Loss:  0.5299\n",
            "Epoch: 40 | Batch: 21 | Loss:  0.4037\n",
            "Epoch: 41 | Batch: 22 | Loss:  0.5205\n",
            "Epoch: 42 | Batch: 23 | Loss:  0.4602\n",
            "Epoch: 43 | Batch: 24 | Loss:  0.3566\n",
            "Epoch: 21 | Batch: 1 | Loss:  0.5377\n",
            "Epoch: 22 | Batch: 2 | Loss:  0.4862\n",
            "Epoch: 23 | Batch: 3 | Loss:  0.3883\n",
            "Epoch: 24 | Batch: 4 | Loss:  0.4987\n",
            "Epoch: 25 | Batch: 5 | Loss:  0.4428\n",
            "Epoch: 26 | Batch: 6 | Loss:  0.3438\n",
            "Epoch: 27 | Batch: 7 | Loss:  0.3556\n",
            "Epoch: 28 | Batch: 8 | Loss:  0.4909\n",
            "Epoch: 29 | Batch: 9 | Loss:  0.4638\n",
            "Epoch: 30 | Batch: 10 | Loss:  0.4819\n",
            "Epoch: 31 | Batch: 11 | Loss:  0.5499\n",
            "Epoch: 32 | Batch: 12 | Loss:  0.4848\n",
            "Epoch: 33 | Batch: 13 | Loss:  0.4166\n",
            "Epoch: 34 | Batch: 14 | Loss:  0.4624\n",
            "Epoch: 35 | Batch: 15 | Loss:  0.4241\n",
            "Epoch: 36 | Batch: 16 | Loss:  0.6048\n",
            "Epoch: 37 | Batch: 17 | Loss:  0.4186\n",
            "Epoch: 38 | Batch: 18 | Loss:  0.5301\n",
            "Epoch: 39 | Batch: 19 | Loss:  0.2943\n",
            "Epoch: 40 | Batch: 20 | Loss:  0.5121\n",
            "Epoch: 41 | Batch: 21 | Loss:  0.4710\n",
            "Epoch: 42 | Batch: 22 | Loss:  0.6280\n",
            "Epoch: 43 | Batch: 23 | Loss:  0.3932\n",
            "Epoch: 44 | Batch: 24 | Loss:  0.5062\n",
            "Epoch: 22 | Batch: 1 | Loss:  0.5773\n",
            "Epoch: 23 | Batch: 2 | Loss:  0.4224\n",
            "Epoch: 24 | Batch: 3 | Loss:  0.3737\n",
            "Epoch: 25 | Batch: 4 | Loss:  0.3066\n",
            "Epoch: 26 | Batch: 5 | Loss:  0.5518\n",
            "Epoch: 27 | Batch: 6 | Loss:  0.4215\n",
            "Epoch: 28 | Batch: 7 | Loss:  0.4269\n",
            "Epoch: 29 | Batch: 8 | Loss:  0.7000\n",
            "Epoch: 30 | Batch: 9 | Loss:  0.3932\n",
            "Epoch: 31 | Batch: 10 | Loss:  0.4638\n",
            "Epoch: 32 | Batch: 11 | Loss:  0.3606\n",
            "Epoch: 33 | Batch: 12 | Loss:  0.5072\n",
            "Epoch: 34 | Batch: 13 | Loss:  0.3394\n",
            "Epoch: 35 | Batch: 14 | Loss:  0.8142\n",
            "Epoch: 36 | Batch: 15 | Loss:  0.3527\n",
            "Epoch: 37 | Batch: 16 | Loss:  0.5363\n",
            "Epoch: 38 | Batch: 17 | Loss:  0.4898\n",
            "Epoch: 39 | Batch: 18 | Loss:  0.3841\n",
            "Epoch: 40 | Batch: 19 | Loss:  0.5986\n",
            "Epoch: 41 | Batch: 20 | Loss:  0.4501\n",
            "Epoch: 42 | Batch: 21 | Loss:  0.3151\n",
            "Epoch: 43 | Batch: 22 | Loss:  0.4521\n",
            "Epoch: 44 | Batch: 23 | Loss:  0.5008\n",
            "Epoch: 45 | Batch: 24 | Loss:  0.3759\n",
            "Epoch: 23 | Batch: 1 | Loss:  0.4441\n",
            "Epoch: 24 | Batch: 2 | Loss:  0.5518\n",
            "Epoch: 25 | Batch: 3 | Loss:  0.4565\n",
            "Epoch: 26 | Batch: 4 | Loss:  0.3990\n",
            "Epoch: 27 | Batch: 5 | Loss:  0.5290\n",
            "Epoch: 28 | Batch: 6 | Loss:  0.4723\n",
            "Epoch: 29 | Batch: 7 | Loss:  0.5785\n",
            "Epoch: 30 | Batch: 8 | Loss:  0.6659\n",
            "Epoch: 31 | Batch: 9 | Loss:  0.4525\n",
            "Epoch: 32 | Batch: 10 | Loss:  0.4225\n",
            "Epoch: 33 | Batch: 11 | Loss:  0.4245\n",
            "Epoch: 34 | Batch: 12 | Loss:  0.4398\n",
            "Epoch: 35 | Batch: 13 | Loss:  0.6041\n",
            "Epoch: 36 | Batch: 14 | Loss:  0.3481\n",
            "Epoch: 37 | Batch: 15 | Loss:  0.6709\n",
            "Epoch: 38 | Batch: 16 | Loss:  0.3969\n",
            "Epoch: 39 | Batch: 17 | Loss:  0.3618\n",
            "Epoch: 40 | Batch: 18 | Loss:  0.3725\n",
            "Epoch: 41 | Batch: 19 | Loss:  0.5155\n",
            "Epoch: 42 | Batch: 20 | Loss:  0.4385\n",
            "Epoch: 43 | Batch: 21 | Loss:  0.4198\n",
            "Epoch: 44 | Batch: 22 | Loss:  0.3783\n",
            "Epoch: 45 | Batch: 23 | Loss:  0.3153\n",
            "Epoch: 46 | Batch: 24 | Loss:  0.6074\n",
            "Epoch: 24 | Batch: 1 | Loss:  0.4544\n",
            "Epoch: 25 | Batch: 2 | Loss:  0.3540\n",
            "Epoch: 26 | Batch: 3 | Loss:  0.6677\n",
            "Epoch: 27 | Batch: 4 | Loss:  0.3736\n",
            "Epoch: 28 | Batch: 5 | Loss:  0.4546\n",
            "Epoch: 29 | Batch: 6 | Loss:  0.4515\n",
            "Epoch: 30 | Batch: 7 | Loss:  0.3398\n",
            "Epoch: 31 | Batch: 8 | Loss:  0.5828\n",
            "Epoch: 32 | Batch: 9 | Loss:  0.4496\n",
            "Epoch: 33 | Batch: 10 | Loss:  0.4597\n",
            "Epoch: 34 | Batch: 11 | Loss:  0.4821\n",
            "Epoch: 35 | Batch: 12 | Loss:  0.3272\n",
            "Epoch: 36 | Batch: 13 | Loss:  0.5057\n",
            "Epoch: 37 | Batch: 14 | Loss:  0.4232\n",
            "Epoch: 38 | Batch: 15 | Loss:  0.3273\n",
            "Epoch: 39 | Batch: 16 | Loss:  0.4943\n",
            "Epoch: 40 | Batch: 17 | Loss:  0.4809\n",
            "Epoch: 41 | Batch: 18 | Loss:  0.4860\n",
            "Epoch: 42 | Batch: 19 | Loss:  0.5064\n",
            "Epoch: 43 | Batch: 20 | Loss:  0.4935\n",
            "Epoch: 44 | Batch: 21 | Loss:  0.5479\n",
            "Epoch: 45 | Batch: 22 | Loss:  0.5046\n",
            "Epoch: 46 | Batch: 23 | Loss:  0.6023\n",
            "Epoch: 47 | Batch: 24 | Loss:  0.4363\n",
            "Epoch: 25 | Batch: 1 | Loss:  0.4523\n",
            "Epoch: 26 | Batch: 2 | Loss:  0.3347\n",
            "Epoch: 27 | Batch: 3 | Loss:  0.4858\n",
            "Epoch: 28 | Batch: 4 | Loss:  0.5798\n",
            "Epoch: 29 | Batch: 5 | Loss:  0.3719\n",
            "Epoch: 30 | Batch: 6 | Loss:  0.5071\n",
            "Epoch: 31 | Batch: 7 | Loss:  0.4130\n",
            "Epoch: 32 | Batch: 8 | Loss:  0.3698\n",
            "Epoch: 33 | Batch: 9 | Loss:  0.5521\n",
            "Epoch: 34 | Batch: 10 | Loss:  0.5477\n",
            "Epoch: 35 | Batch: 11 | Loss:  0.4006\n",
            "Epoch: 36 | Batch: 12 | Loss:  0.4462\n",
            "Epoch: 37 | Batch: 13 | Loss:  0.2835\n",
            "Epoch: 38 | Batch: 14 | Loss:  0.3897\n",
            "Epoch: 39 | Batch: 15 | Loss:  0.4929\n",
            "Epoch: 40 | Batch: 16 | Loss:  0.6382\n",
            "Epoch: 41 | Batch: 17 | Loss:  0.4299\n",
            "Epoch: 42 | Batch: 18 | Loss:  0.6065\n",
            "Epoch: 43 | Batch: 19 | Loss:  0.7255\n",
            "Epoch: 44 | Batch: 20 | Loss:  0.3523\n",
            "Epoch: 45 | Batch: 21 | Loss:  0.4524\n",
            "Epoch: 46 | Batch: 22 | Loss:  0.4927\n",
            "Epoch: 47 | Batch: 23 | Loss:  0.4391\n",
            "Epoch: 48 | Batch: 24 | Loss:  0.4218\n",
            "Epoch: 26 | Batch: 1 | Loss:  0.4899\n",
            "Epoch: 27 | Batch: 2 | Loss:  0.4243\n",
            "Epoch: 28 | Batch: 3 | Loss:  0.3112\n",
            "Epoch: 29 | Batch: 4 | Loss:  0.6157\n",
            "Epoch: 30 | Batch: 5 | Loss:  0.4833\n",
            "Epoch: 31 | Batch: 6 | Loss:  0.3338\n",
            "Epoch: 32 | Batch: 7 | Loss:  0.2866\n",
            "Epoch: 33 | Batch: 8 | Loss:  0.7714\n",
            "Epoch: 34 | Batch: 9 | Loss:  0.4816\n",
            "Epoch: 35 | Batch: 10 | Loss:  0.5147\n",
            "Epoch: 36 | Batch: 11 | Loss:  0.4228\n",
            "Epoch: 37 | Batch: 12 | Loss:  0.6540\n",
            "Epoch: 38 | Batch: 13 | Loss:  0.3302\n",
            "Epoch: 39 | Batch: 14 | Loss:  0.4981\n",
            "Epoch: 40 | Batch: 15 | Loss:  0.4305\n",
            "Epoch: 41 | Batch: 16 | Loss:  0.4308\n",
            "Epoch: 42 | Batch: 17 | Loss:  0.5267\n",
            "Epoch: 43 | Batch: 18 | Loss:  0.5044\n",
            "Epoch: 44 | Batch: 19 | Loss:  0.2527\n",
            "Epoch: 45 | Batch: 20 | Loss:  0.5477\n",
            "Epoch: 46 | Batch: 21 | Loss:  0.3574\n",
            "Epoch: 47 | Batch: 22 | Loss:  0.5051\n",
            "Epoch: 48 | Batch: 23 | Loss:  0.6511\n",
            "Epoch: 49 | Batch: 24 | Loss:  0.3989\n",
            "Epoch: 27 | Batch: 1 | Loss:  0.4631\n",
            "Epoch: 28 | Batch: 2 | Loss:  0.5905\n",
            "Epoch: 29 | Batch: 3 | Loss:  0.2996\n",
            "Epoch: 30 | Batch: 4 | Loss:  0.3242\n",
            "Epoch: 31 | Batch: 5 | Loss:  0.4533\n",
            "Epoch: 32 | Batch: 6 | Loss:  0.3692\n",
            "Epoch: 33 | Batch: 7 | Loss:  0.4615\n",
            "Epoch: 34 | Batch: 8 | Loss:  0.4971\n",
            "Epoch: 35 | Batch: 9 | Loss:  0.3746\n",
            "Epoch: 36 | Batch: 10 | Loss:  0.6635\n",
            "Epoch: 37 | Batch: 11 | Loss:  0.5344\n",
            "Epoch: 38 | Batch: 12 | Loss:  0.3827\n",
            "Epoch: 39 | Batch: 13 | Loss:  0.5054\n",
            "Epoch: 40 | Batch: 14 | Loss:  0.4401\n",
            "Epoch: 41 | Batch: 15 | Loss:  0.5577\n",
            "Epoch: 42 | Batch: 16 | Loss:  0.5841\n",
            "Epoch: 43 | Batch: 17 | Loss:  0.4795\n",
            "Epoch: 44 | Batch: 18 | Loss:  0.5059\n",
            "Epoch: 45 | Batch: 19 | Loss:  0.3612\n",
            "Epoch: 46 | Batch: 20 | Loss:  0.4806\n",
            "Epoch: 47 | Batch: 21 | Loss:  0.5144\n",
            "Epoch: 48 | Batch: 22 | Loss:  0.4235\n",
            "Epoch: 49 | Batch: 23 | Loss:  0.4585\n",
            "Epoch: 50 | Batch: 24 | Loss:  0.4048\n",
            "Epoch: 28 | Batch: 1 | Loss:  0.3722\n",
            "Epoch: 29 | Batch: 2 | Loss:  0.3060\n",
            "Epoch: 30 | Batch: 3 | Loss:  0.6349\n",
            "Epoch: 31 | Batch: 4 | Loss:  0.5759\n",
            "Epoch: 32 | Batch: 5 | Loss:  0.4790\n",
            "Epoch: 33 | Batch: 6 | Loss:  0.2638\n",
            "Epoch: 34 | Batch: 7 | Loss:  0.3308\n",
            "Epoch: 35 | Batch: 8 | Loss:  0.3903\n",
            "Epoch: 36 | Batch: 9 | Loss:  0.4675\n",
            "Epoch: 37 | Batch: 10 | Loss:  0.5523\n",
            "Epoch: 38 | Batch: 11 | Loss:  0.5054\n",
            "Epoch: 39 | Batch: 12 | Loss:  0.6303\n",
            "Epoch: 40 | Batch: 13 | Loss:  0.3524\n",
            "Epoch: 41 | Batch: 14 | Loss:  0.4207\n",
            "Epoch: 42 | Batch: 15 | Loss:  0.5539\n",
            "Epoch: 43 | Batch: 16 | Loss:  0.4034\n",
            "Epoch: 44 | Batch: 17 | Loss:  0.4886\n",
            "Epoch: 45 | Batch: 18 | Loss:  0.4473\n",
            "Epoch: 46 | Batch: 19 | Loss:  0.4024\n",
            "Epoch: 47 | Batch: 20 | Loss:  0.6244\n",
            "Epoch: 48 | Batch: 21 | Loss:  0.4202\n",
            "Epoch: 49 | Batch: 22 | Loss:  0.4539\n",
            "Epoch: 50 | Batch: 23 | Loss:  0.3891\n",
            "Epoch: 51 | Batch: 24 | Loss:  0.5599\n",
            "Epoch: 29 | Batch: 1 | Loss:  0.5435\n",
            "Epoch: 30 | Batch: 2 | Loss:  0.5197\n",
            "Epoch: 31 | Batch: 3 | Loss:  0.3722\n",
            "Epoch: 32 | Batch: 4 | Loss:  0.4306\n",
            "Epoch: 33 | Batch: 5 | Loss:  0.5159\n",
            "Epoch: 34 | Batch: 6 | Loss:  0.6040\n",
            "Epoch: 35 | Batch: 7 | Loss:  0.3920\n",
            "Epoch: 36 | Batch: 8 | Loss:  0.4330\n",
            "Epoch: 37 | Batch: 9 | Loss:  0.4106\n",
            "Epoch: 38 | Batch: 10 | Loss:  0.4478\n",
            "Epoch: 39 | Batch: 11 | Loss:  0.4703\n",
            "Epoch: 40 | Batch: 12 | Loss:  0.5177\n",
            "Epoch: 41 | Batch: 13 | Loss:  0.3614\n",
            "Epoch: 42 | Batch: 14 | Loss:  0.4249\n",
            "Epoch: 43 | Batch: 15 | Loss:  0.4452\n",
            "Epoch: 44 | Batch: 16 | Loss:  0.4162\n",
            "Epoch: 45 | Batch: 17 | Loss:  0.5332\n",
            "Epoch: 46 | Batch: 18 | Loss:  0.4374\n",
            "Epoch: 47 | Batch: 19 | Loss:  0.5619\n",
            "Epoch: 48 | Batch: 20 | Loss:  0.5601\n",
            "Epoch: 49 | Batch: 21 | Loss:  0.4242\n",
            "Epoch: 50 | Batch: 22 | Loss:  0.3971\n",
            "Epoch: 51 | Batch: 23 | Loss:  0.6766\n",
            "Epoch: 52 | Batch: 24 | Loss:  0.3716\n",
            "Epoch: 30 | Batch: 1 | Loss:  0.4638\n",
            "Epoch: 31 | Batch: 2 | Loss:  0.4637\n",
            "Epoch: 32 | Batch: 3 | Loss:  0.5777\n",
            "Epoch: 33 | Batch: 4 | Loss:  0.4821\n",
            "Epoch: 34 | Batch: 5 | Loss:  0.5163\n",
            "Epoch: 35 | Batch: 6 | Loss:  0.3956\n",
            "Epoch: 36 | Batch: 7 | Loss:  0.5597\n",
            "Epoch: 37 | Batch: 8 | Loss:  0.4389\n",
            "Epoch: 38 | Batch: 9 | Loss:  0.4887\n",
            "Epoch: 39 | Batch: 10 | Loss:  0.4962\n",
            "Epoch: 40 | Batch: 11 | Loss:  0.4876\n",
            "Epoch: 41 | Batch: 12 | Loss:  0.4554\n",
            "Epoch: 42 | Batch: 13 | Loss:  0.4172\n",
            "Epoch: 43 | Batch: 14 | Loss:  0.5096\n",
            "Epoch: 44 | Batch: 15 | Loss:  0.3818\n",
            "Epoch: 45 | Batch: 16 | Loss:  0.3761\n",
            "Epoch: 46 | Batch: 17 | Loss:  0.3752\n",
            "Epoch: 47 | Batch: 18 | Loss:  0.7877\n",
            "Epoch: 48 | Batch: 19 | Loss:  0.4071\n",
            "Epoch: 49 | Batch: 20 | Loss:  0.4753\n",
            "Epoch: 50 | Batch: 21 | Loss:  0.4759\n",
            "Epoch: 51 | Batch: 22 | Loss:  0.3487\n",
            "Epoch: 52 | Batch: 23 | Loss:  0.3814\n",
            "Epoch: 53 | Batch: 24 | Loss:  0.2936\n",
            "Epoch: 31 | Batch: 1 | Loss:  0.4031\n",
            "Epoch: 32 | Batch: 2 | Loss:  0.5299\n",
            "Epoch: 33 | Batch: 3 | Loss:  0.5144\n",
            "Epoch: 34 | Batch: 4 | Loss:  0.4622\n",
            "Epoch: 35 | Batch: 5 | Loss:  0.4605\n",
            "Epoch: 36 | Batch: 6 | Loss:  0.5116\n",
            "Epoch: 37 | Batch: 7 | Loss:  0.3621\n",
            "Epoch: 38 | Batch: 8 | Loss:  0.5021\n",
            "Epoch: 39 | Batch: 9 | Loss:  0.5052\n",
            "Epoch: 40 | Batch: 10 | Loss:  0.5562\n",
            "Epoch: 41 | Batch: 11 | Loss:  0.3501\n",
            "Epoch: 42 | Batch: 12 | Loss:  0.4705\n",
            "Epoch: 43 | Batch: 13 | Loss:  0.4188\n",
            "Epoch: 44 | Batch: 14 | Loss:  0.5413\n",
            "Epoch: 45 | Batch: 15 | Loss:  0.4405\n",
            "Epoch: 46 | Batch: 16 | Loss:  0.3095\n",
            "Epoch: 47 | Batch: 17 | Loss:  0.3664\n",
            "Epoch: 48 | Batch: 18 | Loss:  0.4309\n",
            "Epoch: 49 | Batch: 19 | Loss:  0.4451\n",
            "Epoch: 50 | Batch: 20 | Loss:  0.4491\n",
            "Epoch: 51 | Batch: 21 | Loss:  0.6270\n",
            "Epoch: 52 | Batch: 22 | Loss:  0.3831\n",
            "Epoch: 53 | Batch: 23 | Loss:  0.4733\n",
            "Epoch: 54 | Batch: 24 | Loss:  0.6496\n",
            "Epoch: 32 | Batch: 1 | Loss:  0.4234\n",
            "Epoch: 33 | Batch: 2 | Loss:  0.3871\n",
            "Epoch: 34 | Batch: 3 | Loss:  0.3931\n",
            "Epoch: 35 | Batch: 4 | Loss:  0.4527\n",
            "Epoch: 36 | Batch: 5 | Loss:  0.6008\n",
            "Epoch: 37 | Batch: 6 | Loss:  0.3618\n",
            "Epoch: 38 | Batch: 7 | Loss:  0.5158\n",
            "Epoch: 39 | Batch: 8 | Loss:  0.5965\n",
            "Epoch: 40 | Batch: 9 | Loss:  0.3730\n",
            "Epoch: 41 | Batch: 10 | Loss:  0.4835\n",
            "Epoch: 42 | Batch: 11 | Loss:  0.5283\n",
            "Epoch: 43 | Batch: 12 | Loss:  0.6205\n",
            "Epoch: 44 | Batch: 13 | Loss:  0.4736\n",
            "Epoch: 45 | Batch: 14 | Loss:  0.4934\n",
            "Epoch: 46 | Batch: 15 | Loss:  0.5528\n",
            "Epoch: 47 | Batch: 16 | Loss:  0.3842\n",
            "Epoch: 48 | Batch: 17 | Loss:  0.4831\n",
            "Epoch: 49 | Batch: 18 | Loss:  0.4662\n",
            "Epoch: 50 | Batch: 19 | Loss:  0.4919\n",
            "Epoch: 51 | Batch: 20 | Loss:  0.4446\n",
            "Epoch: 52 | Batch: 21 | Loss:  0.4028\n",
            "Epoch: 53 | Batch: 22 | Loss:  0.5014\n",
            "Epoch: 54 | Batch: 23 | Loss:  0.3566\n",
            "Epoch: 55 | Batch: 24 | Loss:  0.3137\n",
            "Epoch: 33 | Batch: 1 | Loss:  0.4530\n",
            "Epoch: 34 | Batch: 2 | Loss:  0.6692\n",
            "Epoch: 35 | Batch: 3 | Loss:  0.2635\n",
            "Epoch: 36 | Batch: 4 | Loss:  0.5417\n",
            "Epoch: 37 | Batch: 5 | Loss:  0.4675\n",
            "Epoch: 38 | Batch: 6 | Loss:  0.4238\n",
            "Epoch: 39 | Batch: 7 | Loss:  0.5086\n",
            "Epoch: 40 | Batch: 8 | Loss:  0.5456\n",
            "Epoch: 41 | Batch: 9 | Loss:  0.5535\n",
            "Epoch: 42 | Batch: 10 | Loss:  0.5043\n",
            "Epoch: 43 | Batch: 11 | Loss:  0.3386\n",
            "Epoch: 44 | Batch: 12 | Loss:  0.4739\n",
            "Epoch: 45 | Batch: 13 | Loss:  0.5106\n",
            "Epoch: 46 | Batch: 14 | Loss:  0.4872\n",
            "Epoch: 47 | Batch: 15 | Loss:  0.3019\n",
            "Epoch: 48 | Batch: 16 | Loss:  0.4603\n",
            "Epoch: 49 | Batch: 17 | Loss:  0.5143\n",
            "Epoch: 50 | Batch: 18 | Loss:  0.5304\n",
            "Epoch: 51 | Batch: 19 | Loss:  0.4367\n",
            "Epoch: 52 | Batch: 20 | Loss:  0.2912\n",
            "Epoch: 53 | Batch: 21 | Loss:  0.5628\n",
            "Epoch: 54 | Batch: 22 | Loss:  0.4340\n",
            "Epoch: 55 | Batch: 23 | Loss:  0.3919\n",
            "Epoch: 56 | Batch: 24 | Loss:  0.6308\n",
            "Epoch: 34 | Batch: 1 | Loss:  0.3985\n",
            "Epoch: 35 | Batch: 2 | Loss:  0.3890\n",
            "Epoch: 36 | Batch: 3 | Loss:  0.4705\n",
            "Epoch: 37 | Batch: 4 | Loss:  0.5897\n",
            "Epoch: 38 | Batch: 5 | Loss:  0.4382\n",
            "Epoch: 39 | Batch: 6 | Loss:  0.6130\n",
            "Epoch: 40 | Batch: 7 | Loss:  0.4783\n",
            "Epoch: 41 | Batch: 8 | Loss:  0.5136\n",
            "Epoch: 42 | Batch: 9 | Loss:  0.5058\n",
            "Epoch: 43 | Batch: 10 | Loss:  0.5026\n",
            "Epoch: 44 | Batch: 11 | Loss:  0.4608\n",
            "Epoch: 45 | Batch: 12 | Loss:  0.5452\n",
            "Epoch: 46 | Batch: 13 | Loss:  0.5038\n",
            "Epoch: 47 | Batch: 14 | Loss:  0.4694\n",
            "Epoch: 48 | Batch: 15 | Loss:  0.4879\n",
            "Epoch: 49 | Batch: 16 | Loss:  0.5306\n",
            "Epoch: 50 | Batch: 17 | Loss:  0.3915\n",
            "Epoch: 51 | Batch: 18 | Loss:  0.3119\n",
            "Epoch: 52 | Batch: 19 | Loss:  0.5431\n",
            "Epoch: 53 | Batch: 20 | Loss:  0.5642\n",
            "Epoch: 54 | Batch: 21 | Loss:  0.4108\n",
            "Epoch: 55 | Batch: 22 | Loss:  0.3174\n",
            "Epoch: 56 | Batch: 23 | Loss:  0.3508\n",
            "Epoch: 57 | Batch: 24 | Loss:  0.4217\n",
            "Epoch: 35 | Batch: 1 | Loss:  0.4972\n",
            "Epoch: 36 | Batch: 2 | Loss:  0.3181\n",
            "Epoch: 37 | Batch: 3 | Loss:  0.3898\n",
            "Epoch: 38 | Batch: 4 | Loss:  0.4096\n",
            "Epoch: 39 | Batch: 5 | Loss:  0.6423\n",
            "Epoch: 40 | Batch: 6 | Loss:  0.3152\n",
            "Epoch: 41 | Batch: 7 | Loss:  0.4009\n",
            "Epoch: 42 | Batch: 8 | Loss:  0.6598\n",
            "Epoch: 43 | Batch: 9 | Loss:  0.4903\n",
            "Epoch: 44 | Batch: 10 | Loss:  0.5312\n",
            "Epoch: 45 | Batch: 11 | Loss:  0.5321\n",
            "Epoch: 46 | Batch: 12 | Loss:  0.4707\n",
            "Epoch: 47 | Batch: 13 | Loss:  0.4228\n",
            "Epoch: 48 | Batch: 14 | Loss:  0.4291\n",
            "Epoch: 49 | Batch: 15 | Loss:  0.3735\n",
            "Epoch: 50 | Batch: 16 | Loss:  0.3257\n",
            "Epoch: 51 | Batch: 17 | Loss:  0.4561\n",
            "Epoch: 52 | Batch: 18 | Loss:  0.4873\n",
            "Epoch: 53 | Batch: 19 | Loss:  0.4046\n",
            "Epoch: 54 | Batch: 20 | Loss:  0.5017\n",
            "Epoch: 55 | Batch: 21 | Loss:  0.4673\n",
            "Epoch: 56 | Batch: 22 | Loss:  0.4496\n",
            "Epoch: 57 | Batch: 23 | Loss:  0.5149\n",
            "Epoch: 58 | Batch: 24 | Loss:  0.6733\n",
            "Epoch: 36 | Batch: 1 | Loss:  0.4629\n",
            "Epoch: 37 | Batch: 2 | Loss:  0.3928\n",
            "Epoch: 38 | Batch: 3 | Loss:  0.4696\n",
            "Epoch: 39 | Batch: 4 | Loss:  0.4177\n",
            "Epoch: 40 | Batch: 5 | Loss:  0.4991\n",
            "Epoch: 41 | Batch: 6 | Loss:  0.5241\n",
            "Epoch: 42 | Batch: 7 | Loss:  0.4936\n",
            "Epoch: 43 | Batch: 8 | Loss:  0.3741\n",
            "Epoch: 44 | Batch: 9 | Loss:  0.4277\n",
            "Epoch: 45 | Batch: 10 | Loss:  0.4348\n",
            "Epoch: 46 | Batch: 11 | Loss:  0.4015\n",
            "Epoch: 47 | Batch: 12 | Loss:  0.3799\n",
            "Epoch: 48 | Batch: 13 | Loss:  0.5985\n",
            "Epoch: 49 | Batch: 14 | Loss:  0.7566\n",
            "Epoch: 50 | Batch: 15 | Loss:  0.6426\n",
            "Epoch: 51 | Batch: 16 | Loss:  0.5946\n",
            "Epoch: 52 | Batch: 17 | Loss:  0.2950\n",
            "Epoch: 53 | Batch: 18 | Loss:  0.3719\n",
            "Epoch: 54 | Batch: 19 | Loss:  0.4209\n",
            "Epoch: 55 | Batch: 20 | Loss:  0.5130\n",
            "Epoch: 56 | Batch: 21 | Loss:  0.2939\n",
            "Epoch: 57 | Batch: 22 | Loss:  0.5065\n",
            "Epoch: 58 | Batch: 23 | Loss:  0.3520\n",
            "Epoch: 59 | Batch: 24 | Loss:  0.5996\n",
            "Epoch: 37 | Batch: 1 | Loss:  0.4687\n",
            "Epoch: 38 | Batch: 2 | Loss:  0.4051\n",
            "Epoch: 39 | Batch: 3 | Loss:  0.3604\n",
            "Epoch: 40 | Batch: 4 | Loss:  0.3594\n",
            "Epoch: 41 | Batch: 5 | Loss:  0.3411\n",
            "Epoch: 42 | Batch: 6 | Loss:  0.3546\n",
            "Epoch: 43 | Batch: 7 | Loss:  0.5696\n",
            "Epoch: 44 | Batch: 8 | Loss:  0.3599\n",
            "Epoch: 45 | Batch: 9 | Loss:  0.5260\n",
            "Epoch: 46 | Batch: 10 | Loss:  0.5649\n",
            "Epoch: 47 | Batch: 11 | Loss:  0.3961\n",
            "Epoch: 48 | Batch: 12 | Loss:  0.3337\n",
            "Epoch: 49 | Batch: 13 | Loss:  0.5187\n",
            "Epoch: 50 | Batch: 14 | Loss:  0.4861\n",
            "Epoch: 51 | Batch: 15 | Loss:  0.3846\n",
            "Epoch: 52 | Batch: 16 | Loss:  0.5456\n",
            "Epoch: 53 | Batch: 17 | Loss:  0.4765\n",
            "Epoch: 54 | Batch: 18 | Loss:  0.6150\n",
            "Epoch: 55 | Batch: 19 | Loss:  0.7473\n",
            "Epoch: 56 | Batch: 20 | Loss:  0.5165\n",
            "Epoch: 57 | Batch: 21 | Loss:  0.4356\n",
            "Epoch: 58 | Batch: 22 | Loss:  0.5375\n",
            "Epoch: 59 | Batch: 23 | Loss:  0.3925\n",
            "Epoch: 60 | Batch: 24 | Loss:  0.3936\n",
            "Epoch: 38 | Batch: 1 | Loss:  0.5211\n",
            "Epoch: 39 | Batch: 2 | Loss:  0.4870\n",
            "Epoch: 40 | Batch: 3 | Loss:  0.6030\n",
            "Epoch: 41 | Batch: 4 | Loss:  0.5734\n",
            "Epoch: 42 | Batch: 5 | Loss:  0.4152\n",
            "Epoch: 43 | Batch: 6 | Loss:  0.4151\n",
            "Epoch: 44 | Batch: 7 | Loss:  0.4103\n",
            "Epoch: 45 | Batch: 8 | Loss:  0.4891\n",
            "Epoch: 46 | Batch: 9 | Loss:  0.4048\n",
            "Epoch: 47 | Batch: 10 | Loss:  0.5958\n",
            "Epoch: 48 | Batch: 11 | Loss:  0.4187\n",
            "Epoch: 49 | Batch: 12 | Loss:  0.4438\n",
            "Epoch: 50 | Batch: 13 | Loss:  0.4825\n",
            "Epoch: 51 | Batch: 14 | Loss:  0.4098\n",
            "Epoch: 52 | Batch: 15 | Loss:  0.4794\n",
            "Epoch: 53 | Batch: 16 | Loss:  0.3946\n",
            "Epoch: 54 | Batch: 17 | Loss:  0.3764\n",
            "Epoch: 55 | Batch: 18 | Loss:  0.6747\n",
            "Epoch: 56 | Batch: 19 | Loss:  0.5023\n",
            "Epoch: 57 | Batch: 20 | Loss:  0.3253\n",
            "Epoch: 58 | Batch: 21 | Loss:  0.3440\n",
            "Epoch: 59 | Batch: 22 | Loss:  0.4565\n",
            "Epoch: 60 | Batch: 23 | Loss:  0.3664\n",
            "Epoch: 61 | Batch: 24 | Loss:  0.4794\n",
            "Epoch: 39 | Batch: 1 | Loss:  0.4280\n",
            "Epoch: 40 | Batch: 2 | Loss:  0.5309\n",
            "Epoch: 41 | Batch: 3 | Loss:  0.3689\n",
            "Epoch: 42 | Batch: 4 | Loss:  0.4204\n",
            "Epoch: 43 | Batch: 5 | Loss:  0.5354\n",
            "Epoch: 44 | Batch: 6 | Loss:  0.4272\n",
            "Epoch: 45 | Batch: 7 | Loss:  0.2497\n",
            "Epoch: 46 | Batch: 8 | Loss:  0.5908\n",
            "Epoch: 47 | Batch: 9 | Loss:  0.5404\n",
            "Epoch: 48 | Batch: 10 | Loss:  0.5612\n",
            "Epoch: 49 | Batch: 11 | Loss:  0.4600\n",
            "Epoch: 50 | Batch: 12 | Loss:  0.4373\n",
            "Epoch: 51 | Batch: 13 | Loss:  0.4514\n",
            "Epoch: 52 | Batch: 14 | Loss:  0.5062\n",
            "Epoch: 53 | Batch: 15 | Loss:  0.4846\n",
            "Epoch: 54 | Batch: 16 | Loss:  0.4036\n",
            "Epoch: 55 | Batch: 17 | Loss:  0.4408\n",
            "Epoch: 56 | Batch: 18 | Loss:  0.6173\n",
            "Epoch: 57 | Batch: 19 | Loss:  0.4751\n",
            "Epoch: 58 | Batch: 20 | Loss:  0.4535\n",
            "Epoch: 59 | Batch: 21 | Loss:  0.4794\n",
            "Epoch: 60 | Batch: 22 | Loss:  0.3531\n",
            "Epoch: 61 | Batch: 23 | Loss:  0.3449\n",
            "Epoch: 62 | Batch: 24 | Loss:  0.4450\n",
            "Epoch: 40 | Batch: 1 | Loss:  0.3324\n",
            "Epoch: 41 | Batch: 2 | Loss:  0.5023\n",
            "Epoch: 42 | Batch: 3 | Loss:  0.5811\n",
            "Epoch: 43 | Batch: 4 | Loss:  0.4717\n",
            "Epoch: 44 | Batch: 5 | Loss:  0.3568\n",
            "Epoch: 45 | Batch: 6 | Loss:  0.4572\n",
            "Epoch: 46 | Batch: 7 | Loss:  0.3134\n",
            "Epoch: 47 | Batch: 8 | Loss:  0.6039\n",
            "Epoch: 48 | Batch: 9 | Loss:  0.5060\n",
            "Epoch: 49 | Batch: 10 | Loss:  0.4401\n",
            "Epoch: 50 | Batch: 11 | Loss:  0.4148\n",
            "Epoch: 51 | Batch: 12 | Loss:  0.3654\n",
            "Epoch: 52 | Batch: 13 | Loss:  0.3081\n",
            "Epoch: 53 | Batch: 14 | Loss:  0.4267\n",
            "Epoch: 54 | Batch: 15 | Loss:  0.5456\n",
            "Epoch: 55 | Batch: 16 | Loss:  0.3776\n",
            "Epoch: 56 | Batch: 17 | Loss:  0.4186\n",
            "Epoch: 57 | Batch: 18 | Loss:  0.6091\n",
            "Epoch: 58 | Batch: 19 | Loss:  0.4996\n",
            "Epoch: 59 | Batch: 20 | Loss:  0.5251\n",
            "Epoch: 60 | Batch: 21 | Loss:  0.3531\n",
            "Epoch: 61 | Batch: 22 | Loss:  0.4673\n",
            "Epoch: 62 | Batch: 23 | Loss:  0.6376\n",
            "Epoch: 63 | Batch: 24 | Loss:  0.4642\n",
            "Epoch: 41 | Batch: 1 | Loss:  0.5195\n",
            "Epoch: 42 | Batch: 2 | Loss:  0.4351\n",
            "Epoch: 43 | Batch: 3 | Loss:  0.4655\n",
            "Epoch: 44 | Batch: 4 | Loss:  0.3934\n",
            "Epoch: 45 | Batch: 5 | Loss:  0.4059\n",
            "Epoch: 46 | Batch: 6 | Loss:  0.5268\n",
            "Epoch: 47 | Batch: 7 | Loss:  0.5048\n",
            "Epoch: 48 | Batch: 8 | Loss:  0.2788\n",
            "Epoch: 49 | Batch: 9 | Loss:  0.3635\n",
            "Epoch: 50 | Batch: 10 | Loss:  0.5738\n",
            "Epoch: 51 | Batch: 11 | Loss:  0.4149\n",
            "Epoch: 52 | Batch: 12 | Loss:  0.4321\n",
            "Epoch: 53 | Batch: 13 | Loss:  0.5497\n",
            "Epoch: 54 | Batch: 14 | Loss:  0.4703\n",
            "Epoch: 55 | Batch: 15 | Loss:  0.4484\n",
            "Epoch: 56 | Batch: 16 | Loss:  0.5967\n",
            "Epoch: 57 | Batch: 17 | Loss:  0.4504\n",
            "Epoch: 58 | Batch: 18 | Loss:  0.5507\n",
            "Epoch: 59 | Batch: 19 | Loss:  0.4107\n",
            "Epoch: 60 | Batch: 20 | Loss:  0.4973\n",
            "Epoch: 61 | Batch: 21 | Loss:  0.4284\n",
            "Epoch: 62 | Batch: 22 | Loss:  0.3332\n",
            "Epoch: 63 | Batch: 23 | Loss:  0.3949\n",
            "Epoch: 64 | Batch: 24 | Loss:  0.5430\n",
            "Epoch: 42 | Batch: 1 | Loss:  0.4439\n",
            "Epoch: 43 | Batch: 2 | Loss:  0.4195\n",
            "Epoch: 44 | Batch: 3 | Loss:  0.6011\n",
            "Epoch: 45 | Batch: 4 | Loss:  0.5439\n",
            "Epoch: 46 | Batch: 5 | Loss:  0.3069\n",
            "Epoch: 47 | Batch: 6 | Loss:  0.3679\n",
            "Epoch: 48 | Batch: 7 | Loss:  0.5942\n",
            "Epoch: 49 | Batch: 8 | Loss:  0.4332\n",
            "Epoch: 50 | Batch: 9 | Loss:  0.3828\n",
            "Epoch: 51 | Batch: 10 | Loss:  0.3785\n",
            "Epoch: 52 | Batch: 11 | Loss:  0.7085\n",
            "Epoch: 53 | Batch: 12 | Loss:  0.4575\n",
            "Epoch: 54 | Batch: 13 | Loss:  0.4472\n",
            "Epoch: 55 | Batch: 14 | Loss:  0.4323\n",
            "Epoch: 56 | Batch: 15 | Loss:  0.3220\n",
            "Epoch: 57 | Batch: 16 | Loss:  0.3576\n",
            "Epoch: 58 | Batch: 17 | Loss:  0.5303\n",
            "Epoch: 59 | Batch: 18 | Loss:  0.2789\n",
            "Epoch: 60 | Batch: 19 | Loss:  0.6195\n",
            "Epoch: 61 | Batch: 20 | Loss:  0.4731\n",
            "Epoch: 62 | Batch: 21 | Loss:  0.6093\n",
            "Epoch: 63 | Batch: 22 | Loss:  0.4066\n",
            "Epoch: 64 | Batch: 23 | Loss:  0.4607\n",
            "Epoch: 65 | Batch: 24 | Loss:  0.4351\n",
            "Epoch: 43 | Batch: 1 | Loss:  0.4633\n",
            "Epoch: 44 | Batch: 2 | Loss:  0.3965\n",
            "Epoch: 45 | Batch: 3 | Loss:  0.4447\n",
            "Epoch: 46 | Batch: 4 | Loss:  0.5345\n",
            "Epoch: 47 | Batch: 5 | Loss:  0.4556\n",
            "Epoch: 48 | Batch: 6 | Loss:  0.3636\n",
            "Epoch: 49 | Batch: 7 | Loss:  0.5045\n",
            "Epoch: 50 | Batch: 8 | Loss:  0.4123\n",
            "Epoch: 51 | Batch: 9 | Loss:  0.3079\n",
            "Epoch: 52 | Batch: 10 | Loss:  0.4694\n",
            "Epoch: 53 | Batch: 11 | Loss:  0.4806\n",
            "Epoch: 54 | Batch: 12 | Loss:  0.5650\n",
            "Epoch: 55 | Batch: 13 | Loss:  0.4000\n",
            "Epoch: 56 | Batch: 14 | Loss:  0.5077\n",
            "Epoch: 57 | Batch: 15 | Loss:  0.4619\n",
            "Epoch: 58 | Batch: 16 | Loss:  0.4142\n",
            "Epoch: 59 | Batch: 17 | Loss:  0.3532\n",
            "Epoch: 60 | Batch: 18 | Loss:  0.5877\n",
            "Epoch: 61 | Batch: 19 | Loss:  0.4078\n",
            "Epoch: 62 | Batch: 20 | Loss:  0.3920\n",
            "Epoch: 63 | Batch: 21 | Loss:  0.5882\n",
            "Epoch: 64 | Batch: 22 | Loss:  0.5336\n",
            "Epoch: 65 | Batch: 23 | Loss:  0.3985\n",
            "Epoch: 66 | Batch: 24 | Loss:  0.4755\n",
            "Epoch: 44 | Batch: 1 | Loss:  0.4238\n",
            "Epoch: 45 | Batch: 2 | Loss:  0.3713\n",
            "Epoch: 46 | Batch: 3 | Loss:  0.3322\n",
            "Epoch: 47 | Batch: 4 | Loss:  0.3982\n",
            "Epoch: 48 | Batch: 5 | Loss:  0.6208\n",
            "Epoch: 49 | Batch: 6 | Loss:  0.4954\n",
            "Epoch: 50 | Batch: 7 | Loss:  0.3354\n",
            "Epoch: 51 | Batch: 8 | Loss:  0.4610\n",
            "Epoch: 52 | Batch: 9 | Loss:  0.4640\n",
            "Epoch: 53 | Batch: 10 | Loss:  0.5420\n",
            "Epoch: 54 | Batch: 11 | Loss:  0.5404\n",
            "Epoch: 55 | Batch: 12 | Loss:  0.4651\n",
            "Epoch: 56 | Batch: 13 | Loss:  0.5942\n",
            "Epoch: 57 | Batch: 14 | Loss:  0.5489\n",
            "Epoch: 58 | Batch: 15 | Loss:  0.5329\n",
            "Epoch: 59 | Batch: 16 | Loss:  0.4441\n",
            "Epoch: 60 | Batch: 17 | Loss:  0.3873\n",
            "Epoch: 61 | Batch: 18 | Loss:  0.4589\n",
            "Epoch: 62 | Batch: 19 | Loss:  0.5215\n",
            "Epoch: 63 | Batch: 20 | Loss:  0.4786\n",
            "Epoch: 64 | Batch: 21 | Loss:  0.2983\n",
            "Epoch: 65 | Batch: 22 | Loss:  0.4661\n",
            "Epoch: 66 | Batch: 23 | Loss:  0.3199\n",
            "Epoch: 67 | Batch: 24 | Loss:  0.5293\n",
            "Epoch: 45 | Batch: 1 | Loss:  0.3781\n",
            "Epoch: 46 | Batch: 2 | Loss:  0.3704\n",
            "Epoch: 47 | Batch: 3 | Loss:  0.5644\n",
            "Epoch: 48 | Batch: 4 | Loss:  0.5971\n",
            "Epoch: 49 | Batch: 5 | Loss:  0.5208\n",
            "Epoch: 50 | Batch: 6 | Loss:  0.5466\n",
            "Epoch: 51 | Batch: 7 | Loss:  0.4607\n",
            "Epoch: 52 | Batch: 8 | Loss:  0.4639\n",
            "Epoch: 53 | Batch: 9 | Loss:  0.5277\n",
            "Epoch: 54 | Batch: 10 | Loss:  0.4273\n",
            "Epoch: 55 | Batch: 11 | Loss:  0.4939\n",
            "Epoch: 56 | Batch: 12 | Loss:  0.4290\n",
            "Epoch: 57 | Batch: 13 | Loss:  0.4645\n",
            "Epoch: 58 | Batch: 14 | Loss:  0.3192\n",
            "Epoch: 59 | Batch: 15 | Loss:  0.4019\n",
            "Epoch: 60 | Batch: 16 | Loss:  0.4441\n",
            "Epoch: 61 | Batch: 17 | Loss:  0.5793\n",
            "Epoch: 62 | Batch: 18 | Loss:  0.4891\n",
            "Epoch: 63 | Batch: 19 | Loss:  0.3070\n",
            "Epoch: 64 | Batch: 20 | Loss:  0.4187\n",
            "Epoch: 65 | Batch: 21 | Loss:  0.5531\n",
            "Epoch: 66 | Batch: 22 | Loss:  0.5071\n",
            "Epoch: 67 | Batch: 23 | Loss:  0.3586\n",
            "Epoch: 68 | Batch: 24 | Loss:  0.3431\n",
            "Epoch: 46 | Batch: 1 | Loss:  0.4148\n",
            "Epoch: 47 | Batch: 2 | Loss:  0.5373\n",
            "Epoch: 48 | Batch: 3 | Loss:  0.4610\n",
            "Epoch: 49 | Batch: 4 | Loss:  0.4445\n",
            "Epoch: 50 | Batch: 5 | Loss:  0.4429\n",
            "Epoch: 51 | Batch: 6 | Loss:  0.3571\n",
            "Epoch: 52 | Batch: 7 | Loss:  0.3235\n",
            "Epoch: 53 | Batch: 8 | Loss:  0.5152\n",
            "Epoch: 54 | Batch: 9 | Loss:  0.4883\n",
            "Epoch: 55 | Batch: 10 | Loss:  0.3622\n",
            "Epoch: 56 | Batch: 11 | Loss:  0.5696\n",
            "Epoch: 57 | Batch: 12 | Loss:  0.3789\n",
            "Epoch: 58 | Batch: 13 | Loss:  0.5606\n",
            "Epoch: 59 | Batch: 14 | Loss:  0.3671\n",
            "Epoch: 60 | Batch: 15 | Loss:  0.5155\n",
            "Epoch: 61 | Batch: 16 | Loss:  0.7232\n",
            "Epoch: 62 | Batch: 17 | Loss:  0.4999\n",
            "Epoch: 63 | Batch: 18 | Loss:  0.4665\n",
            "Epoch: 64 | Batch: 19 | Loss:  0.2943\n",
            "Epoch: 65 | Batch: 20 | Loss:  0.5552\n",
            "Epoch: 66 | Batch: 21 | Loss:  0.3169\n",
            "Epoch: 67 | Batch: 22 | Loss:  0.4159\n",
            "Epoch: 68 | Batch: 23 | Loss:  0.3972\n",
            "Epoch: 69 | Batch: 24 | Loss:  0.5680\n",
            "Epoch: 47 | Batch: 1 | Loss:  0.3651\n",
            "Epoch: 48 | Batch: 2 | Loss:  0.5537\n",
            "Epoch: 49 | Batch: 3 | Loss:  0.4103\n",
            "Epoch: 50 | Batch: 4 | Loss:  0.4373\n",
            "Epoch: 51 | Batch: 5 | Loss:  0.4074\n",
            "Epoch: 52 | Batch: 6 | Loss:  0.3882\n",
            "Epoch: 53 | Batch: 7 | Loss:  0.3739\n",
            "Epoch: 54 | Batch: 8 | Loss:  0.5207\n",
            "Epoch: 55 | Batch: 9 | Loss:  0.3397\n",
            "Epoch: 56 | Batch: 10 | Loss:  0.4887\n",
            "Epoch: 57 | Batch: 11 | Loss:  0.4058\n",
            "Epoch: 58 | Batch: 12 | Loss:  0.4388\n",
            "Epoch: 59 | Batch: 13 | Loss:  0.5227\n",
            "Epoch: 60 | Batch: 14 | Loss:  0.3559\n",
            "Epoch: 61 | Batch: 15 | Loss:  0.6023\n",
            "Epoch: 62 | Batch: 16 | Loss:  0.5017\n",
            "Epoch: 63 | Batch: 17 | Loss:  0.4386\n",
            "Epoch: 64 | Batch: 18 | Loss:  0.5201\n",
            "Epoch: 65 | Batch: 19 | Loss:  0.3891\n",
            "Epoch: 66 | Batch: 20 | Loss:  0.4603\n",
            "Epoch: 67 | Batch: 21 | Loss:  0.5110\n",
            "Epoch: 68 | Batch: 22 | Loss:  0.4464\n",
            "Epoch: 69 | Batch: 23 | Loss:  0.5907\n",
            "Epoch: 70 | Batch: 24 | Loss:  0.4447\n",
            "Epoch: 48 | Batch: 1 | Loss:  0.3779\n",
            "Epoch: 49 | Batch: 2 | Loss:  0.4123\n",
            "Epoch: 50 | Batch: 3 | Loss:  0.4612\n",
            "Epoch: 51 | Batch: 4 | Loss:  0.4345\n",
            "Epoch: 52 | Batch: 5 | Loss:  0.4751\n",
            "Epoch: 53 | Batch: 6 | Loss:  0.5653\n",
            "Epoch: 54 | Batch: 7 | Loss:  0.3584\n",
            "Epoch: 55 | Batch: 8 | Loss:  0.4306\n",
            "Epoch: 56 | Batch: 9 | Loss:  0.4024\n",
            "Epoch: 57 | Batch: 10 | Loss:  0.3747\n",
            "Epoch: 58 | Batch: 11 | Loss:  0.3287\n",
            "Epoch: 59 | Batch: 12 | Loss:  0.4871\n",
            "Epoch: 60 | Batch: 13 | Loss:  0.3129\n",
            "Epoch: 61 | Batch: 14 | Loss:  0.4561\n",
            "Epoch: 62 | Batch: 15 | Loss:  0.4925\n",
            "Epoch: 63 | Batch: 16 | Loss:  0.5500\n",
            "Epoch: 64 | Batch: 17 | Loss:  0.3606\n",
            "Epoch: 65 | Batch: 18 | Loss:  0.4286\n",
            "Epoch: 66 | Batch: 19 | Loss:  0.4682\n",
            "Epoch: 67 | Batch: 20 | Loss:  0.4458\n",
            "Epoch: 68 | Batch: 21 | Loss:  0.5825\n",
            "Epoch: 69 | Batch: 22 | Loss:  0.4234\n",
            "Epoch: 70 | Batch: 23 | Loss:  0.5323\n",
            "Epoch: 71 | Batch: 24 | Loss:  0.7992\n",
            "Epoch: 49 | Batch: 1 | Loss:  0.5556\n",
            "Epoch: 50 | Batch: 2 | Loss:  0.3851\n",
            "Epoch: 51 | Batch: 3 | Loss:  0.4955\n",
            "Epoch: 52 | Batch: 4 | Loss:  0.4831\n",
            "Epoch: 53 | Batch: 5 | Loss:  0.5147\n",
            "Epoch: 54 | Batch: 6 | Loss:  0.5112\n",
            "Epoch: 55 | Batch: 7 | Loss:  0.4276\n",
            "Epoch: 56 | Batch: 8 | Loss:  0.4621\n",
            "Epoch: 57 | Batch: 9 | Loss:  0.4575\n",
            "Epoch: 58 | Batch: 10 | Loss:  0.3485\n",
            "Epoch: 59 | Batch: 11 | Loss:  0.5251\n",
            "Epoch: 60 | Batch: 12 | Loss:  0.4822\n",
            "Epoch: 61 | Batch: 13 | Loss:  0.3250\n",
            "Epoch: 62 | Batch: 14 | Loss:  0.6384\n",
            "Epoch: 63 | Batch: 15 | Loss:  0.2939\n",
            "Epoch: 64 | Batch: 16 | Loss:  0.5691\n",
            "Epoch: 65 | Batch: 17 | Loss:  0.4496\n",
            "Epoch: 66 | Batch: 18 | Loss:  0.4993\n",
            "Epoch: 67 | Batch: 19 | Loss:  0.3962\n",
            "Epoch: 68 | Batch: 20 | Loss:  0.4672\n",
            "Epoch: 69 | Batch: 21 | Loss:  0.3255\n",
            "Epoch: 70 | Batch: 22 | Loss:  0.3407\n",
            "Epoch: 71 | Batch: 23 | Loss:  0.5232\n",
            "Epoch: 72 | Batch: 24 | Loss:  0.5806\n",
            "Epoch: 50 | Batch: 1 | Loss:  0.3474\n",
            "Epoch: 51 | Batch: 2 | Loss:  0.4274\n",
            "Epoch: 52 | Batch: 3 | Loss:  0.4693\n",
            "Epoch: 53 | Batch: 4 | Loss:  0.4525\n",
            "Epoch: 54 | Batch: 5 | Loss:  0.3882\n",
            "Epoch: 55 | Batch: 6 | Loss:  0.4223\n",
            "Epoch: 56 | Batch: 7 | Loss:  0.4490\n",
            "Epoch: 57 | Batch: 8 | Loss:  0.3205\n",
            "Epoch: 58 | Batch: 9 | Loss:  0.5983\n",
            "Epoch: 59 | Batch: 10 | Loss:  0.4530\n",
            "Epoch: 60 | Batch: 11 | Loss:  0.4542\n",
            "Epoch: 61 | Batch: 12 | Loss:  0.5173\n",
            "Epoch: 62 | Batch: 13 | Loss:  0.4121\n",
            "Epoch: 63 | Batch: 14 | Loss:  0.5439\n",
            "Epoch: 64 | Batch: 15 | Loss:  0.4001\n",
            "Epoch: 65 | Batch: 16 | Loss:  0.4964\n",
            "Epoch: 66 | Batch: 17 | Loss:  0.4552\n",
            "Epoch: 67 | Batch: 18 | Loss:  0.6131\n",
            "Epoch: 68 | Batch: 19 | Loss:  0.3890\n",
            "Epoch: 69 | Batch: 20 | Loss:  0.4462\n",
            "Epoch: 70 | Batch: 21 | Loss:  0.4633\n",
            "Epoch: 71 | Batch: 22 | Loss:  0.6043\n",
            "Epoch: 72 | Batch: 23 | Loss:  0.4608\n",
            "Epoch: 73 | Batch: 24 | Loss:  0.3585\n",
            "Epoch: 51 | Batch: 1 | Loss:  0.4502\n",
            "Epoch: 52 | Batch: 2 | Loss:  0.4138\n",
            "Epoch: 53 | Batch: 3 | Loss:  0.3296\n",
            "Epoch: 54 | Batch: 4 | Loss:  0.5109\n",
            "Epoch: 55 | Batch: 5 | Loss:  0.2677\n",
            "Epoch: 56 | Batch: 6 | Loss:  0.3770\n",
            "Epoch: 57 | Batch: 7 | Loss:  0.4590\n",
            "Epoch: 58 | Batch: 8 | Loss:  0.4264\n",
            "Epoch: 59 | Batch: 9 | Loss:  0.7494\n",
            "Epoch: 60 | Batch: 10 | Loss:  0.4387\n",
            "Epoch: 61 | Batch: 11 | Loss:  0.6356\n",
            "Epoch: 62 | Batch: 12 | Loss:  0.4738\n",
            "Epoch: 63 | Batch: 13 | Loss:  0.4059\n",
            "Epoch: 64 | Batch: 14 | Loss:  0.5437\n",
            "Epoch: 65 | Batch: 15 | Loss:  0.3766\n",
            "Epoch: 66 | Batch: 16 | Loss:  0.5651\n",
            "Epoch: 67 | Batch: 17 | Loss:  0.4874\n",
            "Epoch: 68 | Batch: 18 | Loss:  0.3629\n",
            "Epoch: 69 | Batch: 19 | Loss:  0.3527\n",
            "Epoch: 70 | Batch: 20 | Loss:  0.4857\n",
            "Epoch: 71 | Batch: 21 | Loss:  0.4580\n",
            "Epoch: 72 | Batch: 22 | Loss:  0.4731\n",
            "Epoch: 73 | Batch: 23 | Loss:  0.4788\n",
            "Epoch: 74 | Batch: 24 | Loss:  0.3988\n",
            "Epoch: 52 | Batch: 1 | Loss:  0.5072\n",
            "Epoch: 53 | Batch: 2 | Loss:  0.3735\n",
            "Epoch: 54 | Batch: 3 | Loss:  0.2877\n",
            "Epoch: 55 | Batch: 4 | Loss:  0.6840\n",
            "Epoch: 56 | Batch: 5 | Loss:  0.4540\n",
            "Epoch: 57 | Batch: 6 | Loss:  0.4874\n",
            "Epoch: 58 | Batch: 7 | Loss:  0.3441\n",
            "Epoch: 59 | Batch: 8 | Loss:  0.4647\n",
            "Epoch: 60 | Batch: 9 | Loss:  0.5939\n",
            "Epoch: 61 | Batch: 10 | Loss:  0.3596\n",
            "Epoch: 62 | Batch: 11 | Loss:  0.3374\n",
            "Epoch: 63 | Batch: 12 | Loss:  0.3368\n",
            "Epoch: 64 | Batch: 13 | Loss:  0.5950\n",
            "Epoch: 65 | Batch: 14 | Loss:  0.6197\n",
            "Epoch: 66 | Batch: 15 | Loss:  0.5913\n",
            "Epoch: 67 | Batch: 16 | Loss:  0.3836\n",
            "Epoch: 68 | Batch: 17 | Loss:  0.4979\n",
            "Epoch: 69 | Batch: 18 | Loss:  0.3667\n",
            "Epoch: 70 | Batch: 19 | Loss:  0.4663\n",
            "Epoch: 71 | Batch: 20 | Loss:  0.5647\n",
            "Epoch: 72 | Batch: 21 | Loss:  0.2943\n",
            "Epoch: 73 | Batch: 22 | Loss:  0.3975\n",
            "Epoch: 74 | Batch: 23 | Loss:  0.4188\n",
            "Epoch: 75 | Batch: 24 | Loss:  0.4329\n",
            "Epoch: 53 | Batch: 1 | Loss:  0.2852\n",
            "Epoch: 54 | Batch: 2 | Loss:  0.5774\n",
            "Epoch: 55 | Batch: 3 | Loss:  0.3451\n",
            "Epoch: 56 | Batch: 4 | Loss:  0.4345\n",
            "Epoch: 57 | Batch: 5 | Loss:  0.3495\n",
            "Epoch: 58 | Batch: 6 | Loss:  0.2717\n",
            "Epoch: 59 | Batch: 7 | Loss:  0.4971\n",
            "Epoch: 60 | Batch: 8 | Loss:  0.3943\n",
            "Epoch: 61 | Batch: 9 | Loss:  0.4108\n",
            "Epoch: 62 | Batch: 10 | Loss:  0.4000\n",
            "Epoch: 63 | Batch: 11 | Loss:  0.3684\n",
            "Epoch: 64 | Batch: 12 | Loss:  0.4229\n",
            "Epoch: 65 | Batch: 13 | Loss:  0.3721\n",
            "Epoch: 66 | Batch: 14 | Loss:  0.5794\n",
            "Epoch: 67 | Batch: 15 | Loss:  0.4741\n",
            "Epoch: 68 | Batch: 16 | Loss:  0.5254\n",
            "Epoch: 69 | Batch: 17 | Loss:  0.7133\n",
            "Epoch: 70 | Batch: 18 | Loss:  0.6612\n",
            "Epoch: 71 | Batch: 19 | Loss:  0.4357\n",
            "Epoch: 72 | Batch: 20 | Loss:  0.4253\n",
            "Epoch: 73 | Batch: 21 | Loss:  0.4952\n",
            "Epoch: 74 | Batch: 22 | Loss:  0.5474\n",
            "Epoch: 75 | Batch: 23 | Loss:  0.5159\n",
            "Epoch: 76 | Batch: 24 | Loss:  0.3870\n",
            "Epoch: 54 | Batch: 1 | Loss:  0.5507\n",
            "Epoch: 55 | Batch: 2 | Loss:  0.4206\n",
            "Epoch: 56 | Batch: 3 | Loss:  0.4082\n",
            "Epoch: 57 | Batch: 4 | Loss:  0.2697\n",
            "Epoch: 58 | Batch: 5 | Loss:  0.4662\n",
            "Epoch: 59 | Batch: 6 | Loss:  0.4578\n",
            "Epoch: 60 | Batch: 7 | Loss:  0.4602\n",
            "Epoch: 61 | Batch: 8 | Loss:  0.5082\n",
            "Epoch: 62 | Batch: 9 | Loss:  0.3599\n",
            "Epoch: 63 | Batch: 10 | Loss:  0.4545\n",
            "Epoch: 64 | Batch: 11 | Loss:  0.4975\n",
            "Epoch: 65 | Batch: 12 | Loss:  0.3534\n",
            "Epoch: 66 | Batch: 13 | Loss:  0.4035\n",
            "Epoch: 67 | Batch: 14 | Loss:  0.4253\n",
            "Epoch: 68 | Batch: 15 | Loss:  0.4161\n",
            "Epoch: 69 | Batch: 16 | Loss:  0.3726\n",
            "Epoch: 70 | Batch: 17 | Loss:  0.4608\n",
            "Epoch: 71 | Batch: 18 | Loss:  0.7635\n",
            "Epoch: 72 | Batch: 19 | Loss:  0.5705\n",
            "Epoch: 73 | Batch: 20 | Loss:  0.4149\n",
            "Epoch: 74 | Batch: 21 | Loss:  0.6326\n",
            "Epoch: 75 | Batch: 22 | Loss:  0.4583\n",
            "Epoch: 76 | Batch: 23 | Loss:  0.4069\n",
            "Epoch: 77 | Batch: 24 | Loss:  0.3348\n",
            "Epoch: 55 | Batch: 1 | Loss:  0.5430\n",
            "Epoch: 56 | Batch: 2 | Loss:  0.4593\n",
            "Epoch: 57 | Batch: 3 | Loss:  0.4914\n",
            "Epoch: 58 | Batch: 4 | Loss:  0.4377\n",
            "Epoch: 59 | Batch: 5 | Loss:  0.4315\n",
            "Epoch: 60 | Batch: 6 | Loss:  0.3889\n",
            "Epoch: 61 | Batch: 7 | Loss:  0.5129\n",
            "Epoch: 62 | Batch: 8 | Loss:  0.4669\n",
            "Epoch: 63 | Batch: 9 | Loss:  0.3800\n",
            "Epoch: 64 | Batch: 10 | Loss:  0.4329\n",
            "Epoch: 65 | Batch: 11 | Loss:  0.5703\n",
            "Epoch: 66 | Batch: 12 | Loss:  0.3763\n",
            "Epoch: 67 | Batch: 13 | Loss:  0.4028\n",
            "Epoch: 68 | Batch: 14 | Loss:  0.4326\n",
            "Epoch: 69 | Batch: 15 | Loss:  0.4495\n",
            "Epoch: 70 | Batch: 16 | Loss:  0.3199\n",
            "Epoch: 71 | Batch: 17 | Loss:  0.6115\n",
            "Epoch: 72 | Batch: 18 | Loss:  0.3422\n",
            "Epoch: 73 | Batch: 19 | Loss:  0.2997\n",
            "Epoch: 74 | Batch: 20 | Loss:  0.6058\n",
            "Epoch: 75 | Batch: 21 | Loss:  0.5135\n",
            "Epoch: 76 | Batch: 22 | Loss:  0.3944\n",
            "Epoch: 77 | Batch: 23 | Loss:  0.6428\n",
            "Epoch: 78 | Batch: 24 | Loss:  0.3920\n",
            "Epoch: 56 | Batch: 1 | Loss:  0.4660\n",
            "Epoch: 57 | Batch: 2 | Loss:  0.5564\n",
            "Epoch: 58 | Batch: 3 | Loss:  0.4028\n",
            "Epoch: 59 | Batch: 4 | Loss:  0.4296\n",
            "Epoch: 60 | Batch: 5 | Loss:  0.4222\n",
            "Epoch: 61 | Batch: 6 | Loss:  0.4332\n",
            "Epoch: 62 | Batch: 7 | Loss:  0.4161\n",
            "Epoch: 63 | Batch: 8 | Loss:  0.5107\n",
            "Epoch: 64 | Batch: 9 | Loss:  0.4869\n",
            "Epoch: 65 | Batch: 10 | Loss:  0.3145\n",
            "Epoch: 66 | Batch: 11 | Loss:  0.5999\n",
            "Epoch: 67 | Batch: 12 | Loss:  0.3203\n",
            "Epoch: 68 | Batch: 13 | Loss:  0.5359\n",
            "Epoch: 69 | Batch: 14 | Loss:  0.4524\n",
            "Epoch: 70 | Batch: 15 | Loss:  0.4795\n",
            "Epoch: 71 | Batch: 16 | Loss:  0.4230\n",
            "Epoch: 72 | Batch: 17 | Loss:  0.4612\n",
            "Epoch: 73 | Batch: 18 | Loss:  0.3852\n",
            "Epoch: 74 | Batch: 19 | Loss:  0.3313\n",
            "Epoch: 75 | Batch: 20 | Loss:  0.4866\n",
            "Epoch: 76 | Batch: 21 | Loss:  0.4856\n",
            "Epoch: 77 | Batch: 22 | Loss:  0.4222\n",
            "Epoch: 78 | Batch: 23 | Loss:  0.6011\n",
            "Epoch: 79 | Batch: 24 | Loss:  0.4913\n",
            "Epoch: 57 | Batch: 1 | Loss:  0.5429\n",
            "Epoch: 58 | Batch: 2 | Loss:  0.3497\n",
            "Epoch: 59 | Batch: 3 | Loss:  0.5292\n",
            "Epoch: 60 | Batch: 4 | Loss:  0.4603\n",
            "Epoch: 61 | Batch: 5 | Loss:  0.5582\n",
            "Epoch: 62 | Batch: 6 | Loss:  0.4621\n",
            "Epoch: 63 | Batch: 7 | Loss:  0.4852\n",
            "Epoch: 64 | Batch: 8 | Loss:  0.4256\n",
            "Epoch: 65 | Batch: 9 | Loss:  0.4771\n",
            "Epoch: 66 | Batch: 10 | Loss:  0.4241\n",
            "Epoch: 67 | Batch: 11 | Loss:  0.3965\n",
            "Epoch: 68 | Batch: 12 | Loss:  0.5083\n",
            "Epoch: 69 | Batch: 13 | Loss:  0.5307\n",
            "Epoch: 70 | Batch: 14 | Loss:  0.3664\n",
            "Epoch: 71 | Batch: 15 | Loss:  0.3216\n",
            "Epoch: 72 | Batch: 16 | Loss:  0.3610\n",
            "Epoch: 73 | Batch: 17 | Loss:  0.4372\n",
            "Epoch: 74 | Batch: 18 | Loss:  0.4453\n",
            "Epoch: 75 | Batch: 19 | Loss:  0.6559\n",
            "Epoch: 76 | Batch: 20 | Loss:  0.4125\n",
            "Epoch: 77 | Batch: 21 | Loss:  0.3753\n",
            "Epoch: 78 | Batch: 22 | Loss:  0.4146\n",
            "Epoch: 79 | Batch: 23 | Loss:  0.2867\n",
            "Epoch: 80 | Batch: 24 | Loss:  0.6579\n",
            "Epoch: 58 | Batch: 1 | Loss:  0.4943\n",
            "Epoch: 59 | Batch: 2 | Loss:  0.4829\n",
            "Epoch: 60 | Batch: 3 | Loss:  0.4811\n",
            "Epoch: 61 | Batch: 4 | Loss:  0.3314\n",
            "Epoch: 62 | Batch: 5 | Loss:  0.4731\n",
            "Epoch: 63 | Batch: 6 | Loss:  0.4153\n",
            "Epoch: 64 | Batch: 7 | Loss:  0.3940\n",
            "Epoch: 65 | Batch: 8 | Loss:  0.3088\n",
            "Epoch: 66 | Batch: 9 | Loss:  0.3546\n",
            "Epoch: 67 | Batch: 10 | Loss:  0.3392\n",
            "Epoch: 68 | Batch: 11 | Loss:  0.4411\n",
            "Epoch: 69 | Batch: 12 | Loss:  0.4526\n",
            "Epoch: 70 | Batch: 13 | Loss:  0.5557\n",
            "Epoch: 71 | Batch: 14 | Loss:  0.4390\n",
            "Epoch: 72 | Batch: 15 | Loss:  0.3893\n",
            "Epoch: 73 | Batch: 16 | Loss:  0.7210\n",
            "Epoch: 74 | Batch: 17 | Loss:  0.4201\n",
            "Epoch: 75 | Batch: 18 | Loss:  0.3645\n",
            "Epoch: 76 | Batch: 19 | Loss:  0.5028\n",
            "Epoch: 77 | Batch: 20 | Loss:  0.3800\n",
            "Epoch: 78 | Batch: 21 | Loss:  0.6383\n",
            "Epoch: 79 | Batch: 22 | Loss:  0.5243\n",
            "Epoch: 80 | Batch: 23 | Loss:  0.3623\n",
            "Epoch: 81 | Batch: 24 | Loss:  0.5789\n",
            "Epoch: 59 | Batch: 1 | Loss:  0.4430\n",
            "Epoch: 60 | Batch: 2 | Loss:  0.4723\n",
            "Epoch: 61 | Batch: 3 | Loss:  0.4759\n",
            "Epoch: 62 | Batch: 4 | Loss:  0.5231\n",
            "Epoch: 63 | Batch: 5 | Loss:  0.4567\n",
            "Epoch: 64 | Batch: 6 | Loss:  0.3547\n",
            "Epoch: 65 | Batch: 7 | Loss:  0.4851\n",
            "Epoch: 66 | Batch: 8 | Loss:  0.3462\n",
            "Epoch: 67 | Batch: 9 | Loss:  0.4357\n",
            "Epoch: 68 | Batch: 10 | Loss:  0.4470\n",
            "Epoch: 69 | Batch: 11 | Loss:  0.5545\n",
            "Epoch: 70 | Batch: 12 | Loss:  0.2432\n",
            "Epoch: 71 | Batch: 13 | Loss:  0.3388\n",
            "Epoch: 72 | Batch: 14 | Loss:  0.5104\n",
            "Epoch: 73 | Batch: 15 | Loss:  0.6060\n",
            "Epoch: 74 | Batch: 16 | Loss:  0.5285\n",
            "Epoch: 75 | Batch: 17 | Loss:  0.3711\n",
            "Epoch: 76 | Batch: 18 | Loss:  0.5811\n",
            "Epoch: 77 | Batch: 19 | Loss:  0.4297\n",
            "Epoch: 78 | Batch: 20 | Loss:  0.4630\n",
            "Epoch: 79 | Batch: 21 | Loss:  0.4672\n",
            "Epoch: 80 | Batch: 22 | Loss:  0.4431\n",
            "Epoch: 81 | Batch: 23 | Loss:  0.4084\n",
            "Epoch: 82 | Batch: 24 | Loss:  0.5085\n",
            "Epoch: 60 | Batch: 1 | Loss:  0.3695\n",
            "Epoch: 61 | Batch: 2 | Loss:  0.3368\n",
            "Epoch: 62 | Batch: 3 | Loss:  0.5278\n",
            "Epoch: 63 | Batch: 4 | Loss:  0.3538\n",
            "Epoch: 64 | Batch: 5 | Loss:  0.4076\n",
            "Epoch: 65 | Batch: 6 | Loss:  0.4923\n",
            "Epoch: 66 | Batch: 7 | Loss:  0.4300\n",
            "Epoch: 67 | Batch: 8 | Loss:  0.4656\n",
            "Epoch: 68 | Batch: 9 | Loss:  0.5737\n",
            "Epoch: 69 | Batch: 10 | Loss:  0.5388\n",
            "Epoch: 70 | Batch: 11 | Loss:  0.3861\n",
            "Epoch: 71 | Batch: 12 | Loss:  0.4768\n",
            "Epoch: 72 | Batch: 13 | Loss:  0.4123\n",
            "Epoch: 73 | Batch: 14 | Loss:  0.4845\n",
            "Epoch: 74 | Batch: 15 | Loss:  0.4281\n",
            "Epoch: 75 | Batch: 16 | Loss:  0.4691\n",
            "Epoch: 76 | Batch: 17 | Loss:  0.4774\n",
            "Epoch: 77 | Batch: 18 | Loss:  0.4017\n",
            "Epoch: 78 | Batch: 19 | Loss:  0.5584\n",
            "Epoch: 79 | Batch: 20 | Loss:  0.4907\n",
            "Epoch: 80 | Batch: 21 | Loss:  0.4697\n",
            "Epoch: 81 | Batch: 22 | Loss:  0.4834\n",
            "Epoch: 82 | Batch: 23 | Loss:  0.3446\n",
            "Epoch: 83 | Batch: 24 | Loss:  0.3648\n",
            "Epoch: 61 | Batch: 1 | Loss:  0.4278\n",
            "Epoch: 62 | Batch: 2 | Loss:  0.3197\n",
            "Epoch: 63 | Batch: 3 | Loss:  0.4420\n",
            "Epoch: 64 | Batch: 4 | Loss:  0.3967\n",
            "Epoch: 65 | Batch: 5 | Loss:  0.5664\n",
            "Epoch: 66 | Batch: 6 | Loss:  0.3675\n",
            "Epoch: 67 | Batch: 7 | Loss:  0.5613\n",
            "Epoch: 68 | Batch: 8 | Loss:  0.5039\n",
            "Epoch: 69 | Batch: 9 | Loss:  0.4613\n",
            "Epoch: 70 | Batch: 10 | Loss:  0.4215\n",
            "Epoch: 71 | Batch: 11 | Loss:  0.5461\n",
            "Epoch: 72 | Batch: 12 | Loss:  0.4278\n",
            "Epoch: 73 | Batch: 13 | Loss:  0.5331\n",
            "Epoch: 74 | Batch: 14 | Loss:  0.5594\n",
            "Epoch: 75 | Batch: 15 | Loss:  0.3483\n",
            "Epoch: 76 | Batch: 16 | Loss:  0.6835\n",
            "Epoch: 77 | Batch: 17 | Loss:  0.2745\n",
            "Epoch: 78 | Batch: 18 | Loss:  0.4262\n",
            "Epoch: 79 | Batch: 19 | Loss:  0.3454\n",
            "Epoch: 80 | Batch: 20 | Loss:  0.3926\n",
            "Epoch: 81 | Batch: 21 | Loss:  0.5217\n",
            "Epoch: 82 | Batch: 22 | Loss:  0.4288\n",
            "Epoch: 83 | Batch: 23 | Loss:  0.4684\n",
            "Epoch: 84 | Batch: 24 | Loss:  0.4656\n",
            "Epoch: 62 | Batch: 1 | Loss:  0.4489\n",
            "Epoch: 63 | Batch: 2 | Loss:  0.2913\n",
            "Epoch: 64 | Batch: 3 | Loss:  0.5353\n",
            "Epoch: 65 | Batch: 4 | Loss:  0.3653\n",
            "Epoch: 66 | Batch: 5 | Loss:  0.3779\n",
            "Epoch: 67 | Batch: 6 | Loss:  0.4125\n",
            "Epoch: 68 | Batch: 7 | Loss:  0.5601\n",
            "Epoch: 69 | Batch: 8 | Loss:  0.4782\n",
            "Epoch: 70 | Batch: 9 | Loss:  0.4055\n",
            "Epoch: 71 | Batch: 10 | Loss:  0.3640\n",
            "Epoch: 72 | Batch: 11 | Loss:  0.3966\n",
            "Epoch: 73 | Batch: 12 | Loss:  0.5129\n",
            "Epoch: 74 | Batch: 13 | Loss:  0.3380\n",
            "Epoch: 75 | Batch: 14 | Loss:  0.4880\n",
            "Epoch: 76 | Batch: 15 | Loss:  0.5293\n",
            "Epoch: 77 | Batch: 16 | Loss:  0.4094\n",
            "Epoch: 78 | Batch: 17 | Loss:  0.5844\n",
            "Epoch: 79 | Batch: 18 | Loss:  0.2508\n",
            "Epoch: 80 | Batch: 19 | Loss:  0.4234\n",
            "Epoch: 81 | Batch: 20 | Loss:  0.6471\n",
            "Epoch: 82 | Batch: 21 | Loss:  0.4939\n",
            "Epoch: 83 | Batch: 22 | Loss:  0.5276\n",
            "Epoch: 84 | Batch: 23 | Loss:  0.5202\n",
            "Epoch: 85 | Batch: 24 | Loss:  0.4413\n",
            "Epoch: 63 | Batch: 1 | Loss:  0.2935\n",
            "Epoch: 64 | Batch: 2 | Loss:  0.6213\n",
            "Epoch: 65 | Batch: 3 | Loss:  0.6042\n",
            "Epoch: 66 | Batch: 4 | Loss:  0.3654\n",
            "Epoch: 67 | Batch: 5 | Loss:  0.4473\n",
            "Epoch: 68 | Batch: 6 | Loss:  0.3893\n",
            "Epoch: 69 | Batch: 7 | Loss:  0.4682\n",
            "Epoch: 70 | Batch: 8 | Loss:  0.4208\n",
            "Epoch: 71 | Batch: 9 | Loss:  0.3474\n",
            "Epoch: 72 | Batch: 10 | Loss:  0.5902\n",
            "Epoch: 73 | Batch: 11 | Loss:  0.4856\n",
            "Epoch: 74 | Batch: 12 | Loss:  0.4162\n",
            "Epoch: 75 | Batch: 13 | Loss:  0.5572\n",
            "Epoch: 76 | Batch: 14 | Loss:  0.5391\n",
            "Epoch: 77 | Batch: 15 | Loss:  0.3068\n",
            "Epoch: 78 | Batch: 16 | Loss:  0.3372\n",
            "Epoch: 79 | Batch: 17 | Loss:  0.4314\n",
            "Epoch: 80 | Batch: 18 | Loss:  0.4119\n",
            "Epoch: 81 | Batch: 19 | Loss:  0.4039\n",
            "Epoch: 82 | Batch: 20 | Loss:  0.5741\n",
            "Epoch: 83 | Batch: 21 | Loss:  0.4760\n",
            "Epoch: 84 | Batch: 22 | Loss:  0.3264\n",
            "Epoch: 85 | Batch: 23 | Loss:  0.3806\n",
            "Epoch: 86 | Batch: 24 | Loss:  0.6070\n",
            "Epoch: 64 | Batch: 1 | Loss:  0.4627\n",
            "Epoch: 65 | Batch: 2 | Loss:  0.4775\n",
            "Epoch: 66 | Batch: 3 | Loss:  0.4233\n",
            "Epoch: 67 | Batch: 4 | Loss:  0.5878\n",
            "Epoch: 68 | Batch: 5 | Loss:  0.4446\n",
            "Epoch: 69 | Batch: 6 | Loss:  0.3903\n",
            "Epoch: 70 | Batch: 7 | Loss:  0.3231\n",
            "Epoch: 71 | Batch: 8 | Loss:  0.3940\n",
            "Epoch: 72 | Batch: 9 | Loss:  0.4178\n",
            "Epoch: 73 | Batch: 10 | Loss:  0.4338\n",
            "Epoch: 74 | Batch: 11 | Loss:  0.4239\n",
            "Epoch: 75 | Batch: 12 | Loss:  0.4503\n",
            "Epoch: 76 | Batch: 13 | Loss:  0.3089\n",
            "Epoch: 77 | Batch: 14 | Loss:  0.6110\n",
            "Epoch: 78 | Batch: 15 | Loss:  0.3727\n",
            "Epoch: 79 | Batch: 16 | Loss:  0.5015\n",
            "Epoch: 80 | Batch: 17 | Loss:  0.4805\n",
            "Epoch: 81 | Batch: 18 | Loss:  0.3926\n",
            "Epoch: 82 | Batch: 19 | Loss:  0.4042\n",
            "Epoch: 83 | Batch: 20 | Loss:  0.6347\n",
            "Epoch: 84 | Batch: 21 | Loss:  0.5977\n",
            "Epoch: 85 | Batch: 22 | Loss:  0.4924\n",
            "Epoch: 86 | Batch: 23 | Loss:  0.4383\n",
            "Epoch: 87 | Batch: 24 | Loss:  0.4191\n",
            "Epoch: 65 | Batch: 1 | Loss:  0.3948\n",
            "Epoch: 66 | Batch: 2 | Loss:  0.4860\n",
            "Epoch: 67 | Batch: 3 | Loss:  0.4025\n",
            "Epoch: 68 | Batch: 4 | Loss:  0.3413\n",
            "Epoch: 69 | Batch: 5 | Loss:  0.4409\n",
            "Epoch: 70 | Batch: 6 | Loss:  0.3945\n",
            "Epoch: 71 | Batch: 7 | Loss:  0.4410\n",
            "Epoch: 72 | Batch: 8 | Loss:  0.4620\n",
            "Epoch: 73 | Batch: 9 | Loss:  0.4400\n",
            "Epoch: 74 | Batch: 10 | Loss:  0.4733\n",
            "Epoch: 75 | Batch: 11 | Loss:  0.6493\n",
            "Epoch: 76 | Batch: 12 | Loss:  0.3723\n",
            "Epoch: 77 | Batch: 13 | Loss:  0.6346\n",
            "Epoch: 78 | Batch: 14 | Loss:  0.3639\n",
            "Epoch: 79 | Batch: 15 | Loss:  0.4546\n",
            "Epoch: 80 | Batch: 16 | Loss:  0.4423\n",
            "Epoch: 81 | Batch: 17 | Loss:  0.4035\n",
            "Epoch: 82 | Batch: 18 | Loss:  0.3807\n",
            "Epoch: 83 | Batch: 19 | Loss:  0.6025\n",
            "Epoch: 84 | Batch: 20 | Loss:  0.4095\n",
            "Epoch: 85 | Batch: 21 | Loss:  0.4515\n",
            "Epoch: 86 | Batch: 22 | Loss:  0.4658\n",
            "Epoch: 87 | Batch: 23 | Loss:  0.4177\n",
            "Epoch: 88 | Batch: 24 | Loss:  0.4483\n",
            "Epoch: 66 | Batch: 1 | Loss:  0.3507\n",
            "Epoch: 67 | Batch: 2 | Loss:  0.3294\n",
            "Epoch: 68 | Batch: 3 | Loss:  0.3696\n",
            "Epoch: 69 | Batch: 4 | Loss:  0.4502\n",
            "Epoch: 70 | Batch: 5 | Loss:  0.5639\n",
            "Epoch: 71 | Batch: 6 | Loss:  0.3443\n",
            "Epoch: 72 | Batch: 7 | Loss:  0.3267\n",
            "Epoch: 73 | Batch: 8 | Loss:  0.5388\n",
            "Epoch: 74 | Batch: 9 | Loss:  0.6247\n",
            "Epoch: 75 | Batch: 10 | Loss:  0.3693\n",
            "Epoch: 76 | Batch: 11 | Loss:  0.4651\n",
            "Epoch: 77 | Batch: 12 | Loss:  0.3956\n",
            "Epoch: 78 | Batch: 13 | Loss:  0.4165\n",
            "Epoch: 79 | Batch: 14 | Loss:  0.4407\n",
            "Epoch: 80 | Batch: 15 | Loss:  0.4484\n",
            "Epoch: 81 | Batch: 16 | Loss:  0.6027\n",
            "Epoch: 82 | Batch: 17 | Loss:  0.4976\n",
            "Epoch: 83 | Batch: 18 | Loss:  0.4954\n",
            "Epoch: 84 | Batch: 19 | Loss:  0.4675\n",
            "Epoch: 85 | Batch: 20 | Loss:  0.4063\n",
            "Epoch: 86 | Batch: 21 | Loss:  0.4533\n",
            "Epoch: 87 | Batch: 22 | Loss:  0.4521\n",
            "Epoch: 88 | Batch: 23 | Loss:  0.6200\n",
            "Epoch: 89 | Batch: 24 | Loss:  0.2950\n",
            "Epoch: 67 | Batch: 1 | Loss:  0.3556\n",
            "Epoch: 68 | Batch: 2 | Loss:  0.3576\n",
            "Epoch: 69 | Batch: 3 | Loss:  0.4499\n",
            "Epoch: 70 | Batch: 4 | Loss:  0.5238\n",
            "Epoch: 71 | Batch: 5 | Loss:  0.4258\n",
            "Epoch: 72 | Batch: 6 | Loss:  0.4275\n",
            "Epoch: 73 | Batch: 7 | Loss:  0.5573\n",
            "Epoch: 74 | Batch: 8 | Loss:  0.4012\n",
            "Epoch: 75 | Batch: 9 | Loss:  0.3981\n",
            "Epoch: 76 | Batch: 10 | Loss:  0.3716\n",
            "Epoch: 77 | Batch: 11 | Loss:  0.6307\n",
            "Epoch: 78 | Batch: 12 | Loss:  0.5429\n",
            "Epoch: 79 | Batch: 13 | Loss:  0.3288\n",
            "Epoch: 80 | Batch: 14 | Loss:  0.4730\n",
            "Epoch: 81 | Batch: 15 | Loss:  0.4673\n",
            "Epoch: 82 | Batch: 16 | Loss:  0.3703\n",
            "Epoch: 83 | Batch: 17 | Loss:  0.4370\n",
            "Epoch: 84 | Batch: 18 | Loss:  0.4657\n",
            "Epoch: 85 | Batch: 19 | Loss:  0.5155\n",
            "Epoch: 86 | Batch: 20 | Loss:  0.4449\n",
            "Epoch: 87 | Batch: 21 | Loss:  0.5529\n",
            "Epoch: 88 | Batch: 22 | Loss:  0.3195\n",
            "Epoch: 89 | Batch: 23 | Loss:  0.3541\n",
            "Epoch: 90 | Batch: 24 | Loss:  0.5430\n",
            "Epoch: 68 | Batch: 1 | Loss:  0.4680\n",
            "Epoch: 69 | Batch: 2 | Loss:  0.3885\n",
            "Epoch: 70 | Batch: 3 | Loss:  0.5733\n",
            "Epoch: 71 | Batch: 4 | Loss:  0.4817\n",
            "Epoch: 72 | Batch: 5 | Loss:  0.3117\n",
            "Epoch: 73 | Batch: 6 | Loss:  0.3293\n",
            "Epoch: 74 | Batch: 7 | Loss:  0.3666\n",
            "Epoch: 75 | Batch: 8 | Loss:  0.4935\n",
            "Epoch: 76 | Batch: 9 | Loss:  0.5772\n",
            "Epoch: 77 | Batch: 10 | Loss:  0.5000\n",
            "Epoch: 78 | Batch: 11 | Loss:  0.4570\n",
            "Epoch: 79 | Batch: 12 | Loss:  0.4947\n",
            "Epoch: 80 | Batch: 13 | Loss:  0.5788\n",
            "Epoch: 81 | Batch: 14 | Loss:  0.5037\n",
            "Epoch: 82 | Batch: 15 | Loss:  0.4299\n",
            "Epoch: 83 | Batch: 16 | Loss:  0.4177\n",
            "Epoch: 84 | Batch: 17 | Loss:  0.4342\n",
            "Epoch: 85 | Batch: 18 | Loss:  0.4001\n",
            "Epoch: 86 | Batch: 19 | Loss:  0.3557\n",
            "Epoch: 87 | Batch: 20 | Loss:  0.4382\n",
            "Epoch: 88 | Batch: 21 | Loss:  0.4580\n",
            "Epoch: 89 | Batch: 22 | Loss:  0.4123\n",
            "Epoch: 90 | Batch: 23 | Loss:  0.4679\n",
            "Epoch: 91 | Batch: 24 | Loss:  0.3694\n",
            "Epoch: 69 | Batch: 1 | Loss:  0.4321\n",
            "Epoch: 70 | Batch: 2 | Loss:  0.4217\n",
            "Epoch: 71 | Batch: 3 | Loss:  0.4656\n",
            "Epoch: 72 | Batch: 4 | Loss:  0.4988\n",
            "Epoch: 73 | Batch: 5 | Loss:  0.5223\n",
            "Epoch: 74 | Batch: 6 | Loss:  0.5061\n",
            "Epoch: 75 | Batch: 7 | Loss:  0.4660\n",
            "Epoch: 76 | Batch: 8 | Loss:  0.3692\n",
            "Epoch: 77 | Batch: 9 | Loss:  0.5631\n",
            "Epoch: 78 | Batch: 10 | Loss:  0.3662\n",
            "Epoch: 79 | Batch: 11 | Loss:  0.4572\n",
            "Epoch: 80 | Batch: 12 | Loss:  0.3851\n",
            "Epoch: 81 | Batch: 13 | Loss:  0.4125\n",
            "Epoch: 82 | Batch: 14 | Loss:  0.2386\n",
            "Epoch: 83 | Batch: 15 | Loss:  0.3211\n",
            "Epoch: 84 | Batch: 16 | Loss:  0.3703\n",
            "Epoch: 85 | Batch: 17 | Loss:  0.6691\n",
            "Epoch: 86 | Batch: 18 | Loss:  0.5644\n",
            "Epoch: 87 | Batch: 19 | Loss:  0.5338\n",
            "Epoch: 88 | Batch: 20 | Loss:  0.5411\n",
            "Epoch: 89 | Batch: 21 | Loss:  0.4914\n",
            "Epoch: 90 | Batch: 22 | Loss:  0.4006\n",
            "Epoch: 91 | Batch: 23 | Loss:  0.3472\n",
            "Epoch: 92 | Batch: 24 | Loss:  0.3534\n",
            "Epoch: 70 | Batch: 1 | Loss:  0.4801\n",
            "Epoch: 71 | Batch: 2 | Loss:  0.3496\n",
            "Epoch: 72 | Batch: 3 | Loss:  0.4759\n",
            "Epoch: 73 | Batch: 4 | Loss:  0.3337\n",
            "Epoch: 74 | Batch: 5 | Loss:  0.6321\n",
            "Epoch: 75 | Batch: 6 | Loss:  0.3892\n",
            "Epoch: 76 | Batch: 7 | Loss:  0.4173\n",
            "Epoch: 77 | Batch: 8 | Loss:  0.4871\n",
            "Epoch: 78 | Batch: 9 | Loss:  0.3159\n",
            "Epoch: 79 | Batch: 10 | Loss:  0.3704\n",
            "Epoch: 80 | Batch: 11 | Loss:  0.6192\n",
            "Epoch: 81 | Batch: 12 | Loss:  0.2941\n",
            "Epoch: 82 | Batch: 13 | Loss:  0.5823\n",
            "Epoch: 83 | Batch: 14 | Loss:  0.4858\n",
            "Epoch: 84 | Batch: 15 | Loss:  0.3256\n",
            "Epoch: 85 | Batch: 16 | Loss:  0.4713\n",
            "Epoch: 86 | Batch: 17 | Loss:  0.5245\n",
            "Epoch: 87 | Batch: 18 | Loss:  0.4011\n",
            "Epoch: 88 | Batch: 19 | Loss:  0.5397\n",
            "Epoch: 89 | Batch: 20 | Loss:  0.4849\n",
            "Epoch: 90 | Batch: 21 | Loss:  0.3874\n",
            "Epoch: 91 | Batch: 22 | Loss:  0.3730\n",
            "Epoch: 92 | Batch: 23 | Loss:  0.4291\n",
            "Epoch: 93 | Batch: 24 | Loss:  0.6142\n",
            "Epoch: 71 | Batch: 1 | Loss:  0.4838\n",
            "Epoch: 72 | Batch: 2 | Loss:  0.3511\n",
            "Epoch: 73 | Batch: 3 | Loss:  0.3993\n",
            "Epoch: 74 | Batch: 4 | Loss:  0.5502\n",
            "Epoch: 75 | Batch: 5 | Loss:  0.4532\n",
            "Epoch: 76 | Batch: 6 | Loss:  0.3944\n",
            "Epoch: 77 | Batch: 7 | Loss:  0.5276\n",
            "Epoch: 78 | Batch: 8 | Loss:  0.4746\n",
            "Epoch: 79 | Batch: 9 | Loss:  0.4805\n",
            "Epoch: 80 | Batch: 10 | Loss:  0.3791\n",
            "Epoch: 81 | Batch: 11 | Loss:  0.3529\n",
            "Epoch: 82 | Batch: 12 | Loss:  0.5150\n",
            "Epoch: 83 | Batch: 13 | Loss:  0.4880\n",
            "Epoch: 84 | Batch: 14 | Loss:  0.5840\n",
            "Epoch: 85 | Batch: 15 | Loss:  0.5088\n",
            "Epoch: 86 | Batch: 16 | Loss:  0.3758\n",
            "Epoch: 87 | Batch: 17 | Loss:  0.3403\n",
            "Epoch: 88 | Batch: 18 | Loss:  0.4953\n",
            "Epoch: 89 | Batch: 19 | Loss:  0.3169\n",
            "Epoch: 90 | Batch: 20 | Loss:  0.4187\n",
            "Epoch: 91 | Batch: 21 | Loss:  0.3151\n",
            "Epoch: 92 | Batch: 22 | Loss:  0.3918\n",
            "Epoch: 93 | Batch: 23 | Loss:  0.5097\n",
            "Epoch: 94 | Batch: 24 | Loss:  0.6522\n",
            "Epoch: 72 | Batch: 1 | Loss:  0.4246\n",
            "Epoch: 73 | Batch: 2 | Loss:  0.3585\n",
            "Epoch: 74 | Batch: 3 | Loss:  0.3971\n",
            "Epoch: 75 | Batch: 4 | Loss:  0.3130\n",
            "Epoch: 76 | Batch: 5 | Loss:  0.6043\n",
            "Epoch: 77 | Batch: 6 | Loss:  0.4082\n",
            "Epoch: 78 | Batch: 7 | Loss:  0.4003\n",
            "Epoch: 79 | Batch: 8 | Loss:  0.5036\n",
            "Epoch: 80 | Batch: 9 | Loss:  0.6081\n",
            "Epoch: 81 | Batch: 10 | Loss:  0.3453\n",
            "Epoch: 82 | Batch: 11 | Loss:  0.5379\n",
            "Epoch: 83 | Batch: 12 | Loss:  0.3566\n",
            "Epoch: 84 | Batch: 13 | Loss:  0.5242\n",
            "Epoch: 85 | Batch: 14 | Loss:  0.4954\n",
            "Epoch: 86 | Batch: 15 | Loss:  0.5374\n",
            "Epoch: 87 | Batch: 16 | Loss:  0.4167\n",
            "Epoch: 88 | Batch: 17 | Loss:  0.4926\n",
            "Epoch: 89 | Batch: 18 | Loss:  0.4329\n",
            "Epoch: 90 | Batch: 19 | Loss:  0.4818\n",
            "Epoch: 91 | Batch: 20 | Loss:  0.5021\n",
            "Epoch: 92 | Batch: 21 | Loss:  0.3193\n",
            "Epoch: 93 | Batch: 22 | Loss:  0.5842\n",
            "Epoch: 94 | Batch: 23 | Loss:  0.3829\n",
            "Epoch: 95 | Batch: 24 | Loss:  0.3110\n",
            "Epoch: 73 | Batch: 1 | Loss:  0.4777\n",
            "Epoch: 74 | Batch: 2 | Loss:  0.2493\n",
            "Epoch: 75 | Batch: 3 | Loss:  0.4263\n",
            "Epoch: 76 | Batch: 4 | Loss:  0.4814\n",
            "Epoch: 77 | Batch: 5 | Loss:  0.3884\n",
            "Epoch: 78 | Batch: 6 | Loss:  0.2918\n",
            "Epoch: 79 | Batch: 7 | Loss:  0.4316\n",
            "Epoch: 80 | Batch: 8 | Loss:  0.2663\n",
            "Epoch: 81 | Batch: 9 | Loss:  0.3722\n",
            "Epoch: 82 | Batch: 10 | Loss:  0.4399\n",
            "Epoch: 83 | Batch: 11 | Loss:  0.5985\n",
            "Epoch: 84 | Batch: 12 | Loss:  0.5515\n",
            "Epoch: 85 | Batch: 13 | Loss:  0.3378\n",
            "Epoch: 86 | Batch: 14 | Loss:  0.3035\n",
            "Epoch: 87 | Batch: 15 | Loss:  0.6133\n",
            "Epoch: 88 | Batch: 16 | Loss:  0.3853\n",
            "Epoch: 89 | Batch: 17 | Loss:  0.4683\n",
            "Epoch: 90 | Batch: 18 | Loss:  0.4326\n",
            "Epoch: 91 | Batch: 19 | Loss:  0.5217\n",
            "Epoch: 92 | Batch: 20 | Loss:  0.4849\n",
            "Epoch: 93 | Batch: 21 | Loss:  0.4680\n",
            "Epoch: 94 | Batch: 22 | Loss:  0.5150\n",
            "Epoch: 95 | Batch: 23 | Loss:  0.6991\n",
            "Epoch: 96 | Batch: 24 | Loss:  0.5038\n",
            "Epoch: 74 | Batch: 1 | Loss:  0.3941\n",
            "Epoch: 75 | Batch: 2 | Loss:  0.4922\n",
            "Epoch: 76 | Batch: 3 | Loss:  0.3612\n",
            "Epoch: 77 | Batch: 4 | Loss:  0.4350\n",
            "Epoch: 78 | Batch: 5 | Loss:  0.2932\n",
            "Epoch: 79 | Batch: 6 | Loss:  0.5015\n",
            "Epoch: 80 | Batch: 7 | Loss:  0.4139\n",
            "Epoch: 81 | Batch: 8 | Loss:  0.3913\n",
            "Epoch: 82 | Batch: 9 | Loss:  0.4330\n",
            "Epoch: 83 | Batch: 10 | Loss:  0.3661\n",
            "Epoch: 84 | Batch: 11 | Loss:  0.5261\n",
            "Epoch: 85 | Batch: 12 | Loss:  0.3359\n",
            "Epoch: 86 | Batch: 13 | Loss:  0.3252\n",
            "Epoch: 87 | Batch: 14 | Loss:  0.4764\n",
            "Epoch: 88 | Batch: 15 | Loss:  0.5932\n",
            "Epoch: 89 | Batch: 16 | Loss:  0.5691\n",
            "Epoch: 90 | Batch: 17 | Loss:  0.3481\n",
            "Epoch: 91 | Batch: 18 | Loss:  0.4468\n",
            "Epoch: 92 | Batch: 19 | Loss:  0.5270\n",
            "Epoch: 93 | Batch: 20 | Loss:  0.4431\n",
            "Epoch: 94 | Batch: 21 | Loss:  0.5378\n",
            "Epoch: 95 | Batch: 22 | Loss:  0.4756\n",
            "Epoch: 96 | Batch: 23 | Loss:  0.6014\n",
            "Epoch: 97 | Batch: 24 | Loss:  0.3339\n",
            "Epoch: 75 | Batch: 1 | Loss:  0.3885\n",
            "Epoch: 76 | Batch: 2 | Loss:  0.3484\n",
            "Epoch: 77 | Batch: 3 | Loss:  0.5590\n",
            "Epoch: 78 | Batch: 4 | Loss:  0.3617\n",
            "Epoch: 79 | Batch: 5 | Loss:  0.4244\n",
            "Epoch: 80 | Batch: 6 | Loss:  0.6989\n",
            "Epoch: 81 | Batch: 7 | Loss:  0.3051\n",
            "Epoch: 82 | Batch: 8 | Loss:  0.3663\n",
            "Epoch: 83 | Batch: 9 | Loss:  0.4107\n",
            "Epoch: 84 | Batch: 10 | Loss:  0.6049\n",
            "Epoch: 85 | Batch: 11 | Loss:  0.4759\n",
            "Epoch: 86 | Batch: 12 | Loss:  0.3365\n",
            "Epoch: 87 | Batch: 13 | Loss:  0.3845\n",
            "Epoch: 88 | Batch: 14 | Loss:  0.3177\n",
            "Epoch: 89 | Batch: 15 | Loss:  0.5521\n",
            "Epoch: 90 | Batch: 16 | Loss:  0.4572\n",
            "Epoch: 91 | Batch: 17 | Loss:  0.5074\n",
            "Epoch: 92 | Batch: 18 | Loss:  0.5830\n",
            "Epoch: 93 | Batch: 19 | Loss:  0.4631\n",
            "Epoch: 94 | Batch: 20 | Loss:  0.3623\n",
            "Epoch: 95 | Batch: 21 | Loss:  0.3561\n",
            "Epoch: 96 | Batch: 22 | Loss:  0.4602\n",
            "Epoch: 97 | Batch: 23 | Loss:  0.4699\n",
            "Epoch: 98 | Batch: 24 | Loss:  0.5604\n",
            "Epoch: 76 | Batch: 1 | Loss:  0.3890\n",
            "Epoch: 77 | Batch: 2 | Loss:  0.5337\n",
            "Epoch: 78 | Batch: 3 | Loss:  0.3922\n",
            "Epoch: 79 | Batch: 4 | Loss:  0.4826\n",
            "Epoch: 80 | Batch: 5 | Loss:  0.4093\n",
            "Epoch: 81 | Batch: 6 | Loss:  0.5466\n",
            "Epoch: 82 | Batch: 7 | Loss:  0.3798\n",
            "Epoch: 83 | Batch: 8 | Loss:  0.4700\n",
            "Epoch: 84 | Batch: 9 | Loss:  0.5063\n",
            "Epoch: 85 | Batch: 10 | Loss:  0.4911\n",
            "Epoch: 86 | Batch: 11 | Loss:  0.3383\n",
            "Epoch: 87 | Batch: 12 | Loss:  0.3238\n",
            "Epoch: 88 | Batch: 13 | Loss:  0.5220\n",
            "Epoch: 89 | Batch: 14 | Loss:  0.3784\n",
            "Epoch: 90 | Batch: 15 | Loss:  0.2542\n",
            "Epoch: 91 | Batch: 16 | Loss:  0.5009\n",
            "Epoch: 92 | Batch: 17 | Loss:  0.4963\n",
            "Epoch: 93 | Batch: 18 | Loss:  0.4454\n",
            "Epoch: 94 | Batch: 19 | Loss:  0.4500\n",
            "Epoch: 95 | Batch: 20 | Loss:  0.5010\n",
            "Epoch: 96 | Batch: 21 | Loss:  0.4121\n",
            "Epoch: 97 | Batch: 22 | Loss:  0.3422\n",
            "Epoch: 98 | Batch: 23 | Loss:  0.5140\n",
            "Epoch: 99 | Batch: 24 | Loss:  0.7246\n",
            "Epoch: 77 | Batch: 1 | Loss:  0.4800\n",
            "Epoch: 78 | Batch: 2 | Loss:  0.4996\n",
            "Epoch: 79 | Batch: 3 | Loss:  0.4349\n",
            "Epoch: 80 | Batch: 4 | Loss:  0.4739\n",
            "Epoch: 81 | Batch: 5 | Loss:  0.4113\n",
            "Epoch: 82 | Batch: 6 | Loss:  0.3625\n",
            "Epoch: 83 | Batch: 7 | Loss:  0.3537\n",
            "Epoch: 84 | Batch: 8 | Loss:  0.3924\n",
            "Epoch: 85 | Batch: 9 | Loss:  0.5592\n",
            "Epoch: 86 | Batch: 10 | Loss:  0.1911\n",
            "Epoch: 87 | Batch: 11 | Loss:  0.3623\n",
            "Epoch: 88 | Batch: 12 | Loss:  0.3989\n",
            "Epoch: 89 | Batch: 13 | Loss:  0.5061\n",
            "Epoch: 90 | Batch: 14 | Loss:  0.4607\n",
            "Epoch: 91 | Batch: 15 | Loss:  0.3715\n",
            "Epoch: 92 | Batch: 16 | Loss:  0.3697\n",
            "Epoch: 93 | Batch: 17 | Loss:  0.4742\n",
            "Epoch: 94 | Batch: 18 | Loss:  0.4485\n",
            "Epoch: 95 | Batch: 19 | Loss:  0.4313\n",
            "Epoch: 96 | Batch: 20 | Loss:  0.5500\n",
            "Epoch: 97 | Batch: 21 | Loss:  0.5094\n",
            "Epoch: 98 | Batch: 22 | Loss:  0.4067\n",
            "Epoch: 99 | Batch: 23 | Loss:  0.6240\n",
            "Epoch: 100 | Batch: 24 | Loss:  0.7431\n",
            "Epoch: 78 | Batch: 1 | Loss:  0.5597\n",
            "Epoch: 79 | Batch: 2 | Loss:  0.4038\n",
            "Epoch: 80 | Batch: 3 | Loss:  0.3692\n",
            "Epoch: 81 | Batch: 4 | Loss:  0.5297\n",
            "Epoch: 82 | Batch: 5 | Loss:  0.5158\n",
            "Epoch: 83 | Batch: 6 | Loss:  0.3783\n",
            "Epoch: 84 | Batch: 7 | Loss:  0.4781\n",
            "Epoch: 85 | Batch: 8 | Loss:  0.4251\n",
            "Epoch: 86 | Batch: 9 | Loss:  0.3937\n",
            "Epoch: 87 | Batch: 10 | Loss:  0.4704\n",
            "Epoch: 88 | Batch: 11 | Loss:  0.3038\n",
            "Epoch: 89 | Batch: 12 | Loss:  0.4040\n",
            "Epoch: 90 | Batch: 13 | Loss:  0.5244\n",
            "Epoch: 91 | Batch: 14 | Loss:  0.4829\n",
            "Epoch: 92 | Batch: 15 | Loss:  0.3359\n",
            "Epoch: 93 | Batch: 16 | Loss:  0.4112\n",
            "Epoch: 94 | Batch: 17 | Loss:  0.4530\n",
            "Epoch: 95 | Batch: 18 | Loss:  0.3728\n",
            "Epoch: 96 | Batch: 19 | Loss:  0.4636\n",
            "Epoch: 97 | Batch: 20 | Loss:  0.3814\n",
            "Epoch: 98 | Batch: 21 | Loss:  0.5704\n",
            "Epoch: 99 | Batch: 22 | Loss:  0.4013\n",
            "Epoch: 100 | Batch: 23 | Loss:  0.6333\n",
            "Epoch: 101 | Batch: 24 | Loss:  0.5695\n",
            "Epoch: 79 | Batch: 1 | Loss:  0.4474\n",
            "Epoch: 80 | Batch: 2 | Loss:  0.4449\n",
            "Epoch: 81 | Batch: 3 | Loss:  0.4126\n",
            "Epoch: 82 | Batch: 4 | Loss:  0.5315\n",
            "Epoch: 83 | Batch: 5 | Loss:  0.3916\n",
            "Epoch: 84 | Batch: 6 | Loss:  0.4080\n",
            "Epoch: 85 | Batch: 7 | Loss:  0.4222\n",
            "Epoch: 86 | Batch: 8 | Loss:  0.4044\n",
            "Epoch: 87 | Batch: 9 | Loss:  0.4871\n",
            "Epoch: 88 | Batch: 10 | Loss:  0.4654\n",
            "Epoch: 89 | Batch: 11 | Loss:  0.2727\n",
            "Epoch: 90 | Batch: 12 | Loss:  0.5306\n",
            "Epoch: 91 | Batch: 13 | Loss:  0.5320\n",
            "Epoch: 92 | Batch: 14 | Loss:  0.3908\n",
            "Epoch: 93 | Batch: 15 | Loss:  0.5047\n",
            "Epoch: 94 | Batch: 16 | Loss:  0.5667\n",
            "Epoch: 95 | Batch: 17 | Loss:  0.4288\n",
            "Epoch: 96 | Batch: 18 | Loss:  0.3116\n",
            "Epoch: 97 | Batch: 19 | Loss:  0.2917\n",
            "Epoch: 98 | Batch: 20 | Loss:  0.4046\n",
            "Epoch: 99 | Batch: 21 | Loss:  0.5130\n",
            "Epoch: 100 | Batch: 22 | Loss:  0.4229\n",
            "Epoch: 101 | Batch: 23 | Loss:  0.4702\n",
            "Epoch: 102 | Batch: 24 | Loss:  0.5725\n",
            "Epoch: 80 | Batch: 1 | Loss:  0.3670\n",
            "Epoch: 81 | Batch: 2 | Loss:  0.3809\n",
            "Epoch: 82 | Batch: 3 | Loss:  0.5856\n",
            "Epoch: 83 | Batch: 4 | Loss:  0.4160\n",
            "Epoch: 84 | Batch: 5 | Loss:  0.3783\n",
            "Epoch: 85 | Batch: 6 | Loss:  0.5005\n",
            "Epoch: 86 | Batch: 7 | Loss:  0.4675\n",
            "Epoch: 87 | Batch: 8 | Loss:  0.3487\n",
            "Epoch: 88 | Batch: 9 | Loss:  0.4303\n",
            "Epoch: 89 | Batch: 10 | Loss:  0.4815\n",
            "Epoch: 90 | Batch: 11 | Loss:  0.4657\n",
            "Epoch: 91 | Batch: 12 | Loss:  0.3591\n",
            "Epoch: 92 | Batch: 13 | Loss:  0.3891\n",
            "Epoch: 93 | Batch: 14 | Loss:  0.3812\n",
            "Epoch: 94 | Batch: 15 | Loss:  0.5254\n",
            "Epoch: 95 | Batch: 16 | Loss:  0.3999\n",
            "Epoch: 96 | Batch: 17 | Loss:  0.4214\n",
            "Epoch: 97 | Batch: 18 | Loss:  0.6280\n",
            "Epoch: 98 | Batch: 19 | Loss:  0.5211\n",
            "Epoch: 99 | Batch: 20 | Loss:  0.3614\n",
            "Epoch: 100 | Batch: 21 | Loss:  0.5057\n",
            "Epoch: 101 | Batch: 22 | Loss:  0.5124\n",
            "Epoch: 102 | Batch: 23 | Loss:  0.4351\n",
            "Epoch: 103 | Batch: 24 | Loss:  0.4292\n",
            "Epoch: 81 | Batch: 1 | Loss:  0.3269\n",
            "Epoch: 82 | Batch: 2 | Loss:  0.4970\n",
            "Epoch: 83 | Batch: 3 | Loss:  0.2971\n",
            "Epoch: 84 | Batch: 4 | Loss:  0.4009\n",
            "Epoch: 85 | Batch: 5 | Loss:  0.5895\n",
            "Epoch: 86 | Batch: 6 | Loss:  0.5362\n",
            "Epoch: 87 | Batch: 7 | Loss:  0.2801\n",
            "Epoch: 88 | Batch: 8 | Loss:  0.3012\n",
            "Epoch: 89 | Batch: 9 | Loss:  0.5410\n",
            "Epoch: 90 | Batch: 10 | Loss:  0.4410\n",
            "Epoch: 91 | Batch: 11 | Loss:  0.4360\n",
            "Epoch: 92 | Batch: 12 | Loss:  0.4162\n",
            "Epoch: 93 | Batch: 13 | Loss:  0.4366\n",
            "Epoch: 94 | Batch: 14 | Loss:  0.3227\n",
            "Epoch: 95 | Batch: 15 | Loss:  0.4728\n",
            "Epoch: 96 | Batch: 16 | Loss:  0.4238\n",
            "Epoch: 97 | Batch: 17 | Loss:  0.5415\n",
            "Epoch: 98 | Batch: 18 | Loss:  0.3373\n",
            "Epoch: 99 | Batch: 19 | Loss:  0.5435\n",
            "Epoch: 100 | Batch: 20 | Loss:  0.5101\n",
            "Epoch: 101 | Batch: 21 | Loss:  0.5494\n",
            "Epoch: 102 | Batch: 22 | Loss:  0.4890\n",
            "Epoch: 103 | Batch: 23 | Loss:  0.5722\n",
            "Epoch: 104 | Batch: 24 | Loss:  0.3925\n",
            "Epoch: 82 | Batch: 1 | Loss:  0.5004\n",
            "Epoch: 83 | Batch: 2 | Loss:  0.5112\n",
            "Epoch: 84 | Batch: 3 | Loss:  0.3428\n",
            "Epoch: 85 | Batch: 4 | Loss:  0.3862\n",
            "Epoch: 86 | Batch: 5 | Loss:  0.5930\n",
            "Epoch: 87 | Batch: 6 | Loss:  0.4848\n",
            "Epoch: 88 | Batch: 7 | Loss:  0.3687\n",
            "Epoch: 89 | Batch: 8 | Loss:  0.3283\n",
            "Epoch: 90 | Batch: 9 | Loss:  0.3118\n",
            "Epoch: 91 | Batch: 10 | Loss:  0.4707\n",
            "Epoch: 92 | Batch: 11 | Loss:  0.4823\n",
            "Epoch: 93 | Batch: 12 | Loss:  0.3511\n",
            "Epoch: 94 | Batch: 13 | Loss:  0.3303\n",
            "Epoch: 95 | Batch: 14 | Loss:  0.3548\n",
            "Epoch: 96 | Batch: 15 | Loss:  0.6424\n",
            "Epoch: 97 | Batch: 16 | Loss:  0.8590\n",
            "Epoch: 98 | Batch: 17 | Loss:  0.4472\n",
            "Epoch: 99 | Batch: 18 | Loss:  0.3672\n",
            "Epoch: 100 | Batch: 19 | Loss:  0.4251\n",
            "Epoch: 101 | Batch: 20 | Loss:  0.4053\n",
            "Epoch: 102 | Batch: 21 | Loss:  0.3213\n",
            "Epoch: 103 | Batch: 22 | Loss:  0.3822\n",
            "Epoch: 104 | Batch: 23 | Loss:  0.4530\n",
            "Epoch: 105 | Batch: 24 | Loss:  0.4841\n",
            "Epoch: 83 | Batch: 1 | Loss:  0.4191\n",
            "Epoch: 84 | Batch: 2 | Loss:  0.2321\n",
            "Epoch: 85 | Batch: 3 | Loss:  0.4869\n",
            "Epoch: 86 | Batch: 4 | Loss:  0.2648\n",
            "Epoch: 87 | Batch: 5 | Loss:  0.7260\n",
            "Epoch: 88 | Batch: 6 | Loss:  0.4123\n",
            "Epoch: 89 | Batch: 7 | Loss:  0.6173\n",
            "Epoch: 90 | Batch: 8 | Loss:  0.4302\n",
            "Epoch: 91 | Batch: 9 | Loss:  0.4901\n",
            "Epoch: 92 | Batch: 10 | Loss:  0.4129\n",
            "Epoch: 93 | Batch: 11 | Loss:  0.4117\n",
            "Epoch: 94 | Batch: 12 | Loss:  0.3819\n",
            "Epoch: 95 | Batch: 13 | Loss:  0.3325\n",
            "Epoch: 96 | Batch: 14 | Loss:  0.4787\n",
            "Epoch: 97 | Batch: 15 | Loss:  0.3692\n",
            "Epoch: 98 | Batch: 16 | Loss:  0.4833\n",
            "Epoch: 99 | Batch: 17 | Loss:  0.3080\n",
            "Epoch: 100 | Batch: 18 | Loss:  0.6697\n",
            "Epoch: 101 | Batch: 19 | Loss:  0.4151\n",
            "Epoch: 102 | Batch: 20 | Loss:  0.6004\n",
            "Epoch: 103 | Batch: 21 | Loss:  0.3936\n",
            "Epoch: 104 | Batch: 22 | Loss:  0.5278\n",
            "Epoch: 105 | Batch: 23 | Loss:  0.4302\n",
            "Epoch: 106 | Batch: 24 | Loss:  0.3252\n",
            "Epoch: 84 | Batch: 1 | Loss:  0.4464\n",
            "Epoch: 85 | Batch: 2 | Loss:  0.4609\n",
            "Epoch: 86 | Batch: 3 | Loss:  0.4081\n",
            "Epoch: 87 | Batch: 4 | Loss:  0.4086\n",
            "Epoch: 88 | Batch: 5 | Loss:  0.5619\n",
            "Epoch: 89 | Batch: 6 | Loss:  0.4581\n",
            "Epoch: 90 | Batch: 7 | Loss:  0.3866\n",
            "Epoch: 91 | Batch: 8 | Loss:  0.4770\n",
            "Epoch: 92 | Batch: 9 | Loss:  0.4892\n",
            "Epoch: 93 | Batch: 10 | Loss:  0.3477\n",
            "Epoch: 94 | Batch: 11 | Loss:  0.3838\n",
            "Epoch: 95 | Batch: 12 | Loss:  0.3848\n",
            "Epoch: 96 | Batch: 13 | Loss:  0.4922\n",
            "Epoch: 97 | Batch: 14 | Loss:  0.4336\n",
            "Epoch: 98 | Batch: 15 | Loss:  0.4845\n",
            "Epoch: 99 | Batch: 16 | Loss:  0.3573\n",
            "Epoch: 100 | Batch: 17 | Loss:  0.4571\n",
            "Epoch: 101 | Batch: 18 | Loss:  0.4832\n",
            "Epoch: 102 | Batch: 19 | Loss:  0.5012\n",
            "Epoch: 103 | Batch: 20 | Loss:  0.4703\n",
            "Epoch: 104 | Batch: 21 | Loss:  0.4530\n",
            "Epoch: 105 | Batch: 22 | Loss:  0.3355\n",
            "Epoch: 106 | Batch: 23 | Loss:  0.4305\n",
            "Epoch: 107 | Batch: 24 | Loss:  0.5237\n",
            "Epoch: 85 | Batch: 1 | Loss:  0.4847\n",
            "Epoch: 86 | Batch: 2 | Loss:  0.4717\n",
            "Epoch: 87 | Batch: 3 | Loss:  0.3004\n",
            "Epoch: 88 | Batch: 4 | Loss:  0.4526\n",
            "Epoch: 89 | Batch: 5 | Loss:  0.4943\n",
            "Epoch: 90 | Batch: 6 | Loss:  0.5493\n",
            "Epoch: 91 | Batch: 7 | Loss:  0.2529\n",
            "Epoch: 92 | Batch: 8 | Loss:  0.5127\n",
            "Epoch: 93 | Batch: 9 | Loss:  0.3701\n",
            "Epoch: 94 | Batch: 10 | Loss:  0.4866\n",
            "Epoch: 95 | Batch: 11 | Loss:  0.5920\n",
            "Epoch: 96 | Batch: 12 | Loss:  0.4173\n",
            "Epoch: 97 | Batch: 13 | Loss:  0.5183\n",
            "Epoch: 98 | Batch: 14 | Loss:  0.5492\n",
            "Epoch: 99 | Batch: 15 | Loss:  0.3501\n",
            "Epoch: 100 | Batch: 16 | Loss:  0.3715\n",
            "Epoch: 101 | Batch: 17 | Loss:  0.5844\n",
            "Epoch: 102 | Batch: 18 | Loss:  0.3698\n",
            "Epoch: 103 | Batch: 19 | Loss:  0.4267\n",
            "Epoch: 104 | Batch: 20 | Loss:  0.4251\n",
            "Epoch: 105 | Batch: 21 | Loss:  0.5506\n",
            "Epoch: 106 | Batch: 22 | Loss:  0.3776\n",
            "Epoch: 107 | Batch: 23 | Loss:  0.4027\n",
            "Epoch: 108 | Batch: 24 | Loss:  0.2070\n",
            "Epoch: 86 | Batch: 1 | Loss:  0.3356\n",
            "Epoch: 87 | Batch: 2 | Loss:  0.3820\n",
            "Epoch: 88 | Batch: 3 | Loss:  0.3955\n",
            "Epoch: 89 | Batch: 4 | Loss:  0.4993\n",
            "Epoch: 90 | Batch: 5 | Loss:  0.3425\n",
            "Epoch: 91 | Batch: 6 | Loss:  0.4685\n",
            "Epoch: 92 | Batch: 7 | Loss:  0.4222\n",
            "Epoch: 93 | Batch: 8 | Loss:  0.2833\n",
            "Epoch: 94 | Batch: 9 | Loss:  0.4029\n",
            "Epoch: 95 | Batch: 10 | Loss:  0.3351\n",
            "Epoch: 96 | Batch: 11 | Loss:  0.5608\n",
            "Epoch: 97 | Batch: 12 | Loss:  0.4845\n",
            "Epoch: 98 | Batch: 13 | Loss:  0.4085\n",
            "Epoch: 99 | Batch: 14 | Loss:  0.5616\n",
            "Epoch: 100 | Batch: 15 | Loss:  0.5438\n",
            "Epoch: 101 | Batch: 16 | Loss:  0.5152\n",
            "Epoch: 102 | Batch: 17 | Loss:  0.4747\n",
            "Epoch: 103 | Batch: 18 | Loss:  0.5282\n",
            "Epoch: 104 | Batch: 19 | Loss:  0.4765\n",
            "Epoch: 105 | Batch: 20 | Loss:  0.4472\n",
            "Epoch: 106 | Batch: 21 | Loss:  0.3744\n",
            "Epoch: 107 | Batch: 22 | Loss:  0.4438\n",
            "Epoch: 108 | Batch: 23 | Loss:  0.4355\n",
            "Epoch: 109 | Batch: 24 | Loss:  0.4415\n",
            "Epoch: 87 | Batch: 1 | Loss:  0.3569\n",
            "Epoch: 88 | Batch: 2 | Loss:  0.5327\n",
            "Epoch: 89 | Batch: 3 | Loss:  0.3343\n",
            "Epoch: 90 | Batch: 4 | Loss:  0.5082\n",
            "Epoch: 91 | Batch: 5 | Loss:  0.3011\n",
            "Epoch: 92 | Batch: 6 | Loss:  0.5043\n",
            "Epoch: 93 | Batch: 7 | Loss:  0.4679\n",
            "Epoch: 94 | Batch: 8 | Loss:  0.4128\n",
            "Epoch: 95 | Batch: 9 | Loss:  0.3405\n",
            "Epoch: 96 | Batch: 10 | Loss:  0.4708\n",
            "Epoch: 97 | Batch: 11 | Loss:  0.3855\n",
            "Epoch: 98 | Batch: 12 | Loss:  0.4722\n",
            "Epoch: 99 | Batch: 13 | Loss:  0.5977\n",
            "Epoch: 100 | Batch: 14 | Loss:  0.5163\n",
            "Epoch: 101 | Batch: 15 | Loss:  0.3852\n",
            "Epoch: 102 | Batch: 16 | Loss:  0.3451\n",
            "Epoch: 103 | Batch: 17 | Loss:  0.5998\n",
            "Epoch: 104 | Batch: 18 | Loss:  0.3628\n",
            "Epoch: 105 | Batch: 19 | Loss:  0.3794\n",
            "Epoch: 106 | Batch: 20 | Loss:  0.4538\n",
            "Epoch: 107 | Batch: 21 | Loss:  0.4495\n",
            "Epoch: 108 | Batch: 22 | Loss:  0.4237\n",
            "Epoch: 109 | Batch: 23 | Loss:  0.4706\n",
            "Epoch: 110 | Batch: 24 | Loss:  0.4782\n",
            "Epoch: 88 | Batch: 1 | Loss:  0.4184\n",
            "Epoch: 89 | Batch: 2 | Loss:  0.4672\n",
            "Epoch: 90 | Batch: 3 | Loss:  0.7094\n",
            "Epoch: 91 | Batch: 4 | Loss:  0.5190\n",
            "Epoch: 92 | Batch: 5 | Loss:  0.3804\n",
            "Epoch: 93 | Batch: 6 | Loss:  0.5118\n",
            "Epoch: 94 | Batch: 7 | Loss:  0.3115\n",
            "Epoch: 95 | Batch: 8 | Loss:  0.3148\n",
            "Epoch: 96 | Batch: 9 | Loss:  0.2816\n",
            "Epoch: 97 | Batch: 10 | Loss:  0.4331\n",
            "Epoch: 98 | Batch: 11 | Loss:  0.4359\n",
            "Epoch: 99 | Batch: 12 | Loss:  0.5256\n",
            "Epoch: 100 | Batch: 13 | Loss:  0.4810\n",
            "Epoch: 101 | Batch: 14 | Loss:  0.3685\n",
            "Epoch: 102 | Batch: 15 | Loss:  0.6395\n",
            "Epoch: 103 | Batch: 16 | Loss:  0.4019\n",
            "Epoch: 104 | Batch: 17 | Loss:  0.4062\n",
            "Epoch: 105 | Batch: 18 | Loss:  0.5522\n",
            "Epoch: 106 | Batch: 19 | Loss:  0.3101\n",
            "Epoch: 107 | Batch: 20 | Loss:  0.4565\n",
            "Epoch: 108 | Batch: 21 | Loss:  0.2735\n",
            "Epoch: 109 | Batch: 22 | Loss:  0.4513\n",
            "Epoch: 110 | Batch: 23 | Loss:  0.4033\n",
            "Epoch: 111 | Batch: 24 | Loss:  0.5872\n",
            "Epoch: 89 | Batch: 1 | Loss:  0.4132\n",
            "Epoch: 90 | Batch: 2 | Loss:  0.3898\n",
            "Epoch: 91 | Batch: 3 | Loss:  0.4159\n",
            "Epoch: 92 | Batch: 4 | Loss:  0.4873\n",
            "Epoch: 93 | Batch: 5 | Loss:  0.4519\n",
            "Epoch: 94 | Batch: 6 | Loss:  0.3957\n",
            "Epoch: 95 | Batch: 7 | Loss:  0.4335\n",
            "Epoch: 96 | Batch: 8 | Loss:  0.3725\n",
            "Epoch: 97 | Batch: 9 | Loss:  0.4795\n",
            "Epoch: 98 | Batch: 10 | Loss:  0.4793\n",
            "Epoch: 99 | Batch: 11 | Loss:  0.6710\n",
            "Epoch: 100 | Batch: 12 | Loss:  0.5920\n",
            "Epoch: 101 | Batch: 13 | Loss:  0.5646\n",
            "Epoch: 102 | Batch: 14 | Loss:  0.3986\n",
            "Epoch: 103 | Batch: 15 | Loss:  0.4001\n",
            "Epoch: 104 | Batch: 16 | Loss:  0.3671\n",
            "Epoch: 105 | Batch: 17 | Loss:  0.3331\n",
            "Epoch: 106 | Batch: 18 | Loss:  0.4587\n",
            "Epoch: 107 | Batch: 19 | Loss:  0.4523\n",
            "Epoch: 108 | Batch: 20 | Loss:  0.4995\n",
            "Epoch: 109 | Batch: 21 | Loss:  0.5412\n",
            "Epoch: 110 | Batch: 22 | Loss:  0.3500\n",
            "Epoch: 111 | Batch: 23 | Loss:  0.3088\n",
            "Epoch: 112 | Batch: 24 | Loss:  0.3209\n",
            "Epoch: 90 | Batch: 1 | Loss:  0.3821\n",
            "Epoch: 91 | Batch: 2 | Loss:  0.4055\n",
            "Epoch: 92 | Batch: 3 | Loss:  0.5213\n",
            "Epoch: 93 | Batch: 4 | Loss:  0.5559\n",
            "Epoch: 94 | Batch: 5 | Loss:  0.5921\n",
            "Epoch: 95 | Batch: 6 | Loss:  0.4015\n",
            "Epoch: 96 | Batch: 7 | Loss:  0.4178\n",
            "Epoch: 97 | Batch: 8 | Loss:  0.4546\n",
            "Epoch: 98 | Batch: 9 | Loss:  0.4143\n",
            "Epoch: 99 | Batch: 10 | Loss:  0.4412\n",
            "Epoch: 100 | Batch: 11 | Loss:  0.2802\n",
            "Epoch: 101 | Batch: 12 | Loss:  0.4322\n",
            "Epoch: 102 | Batch: 13 | Loss:  0.4022\n",
            "Epoch: 103 | Batch: 14 | Loss:  0.4922\n",
            "Epoch: 104 | Batch: 15 | Loss:  0.3712\n",
            "Epoch: 105 | Batch: 16 | Loss:  0.3734\n",
            "Epoch: 106 | Batch: 17 | Loss:  0.4833\n",
            "Epoch: 107 | Batch: 18 | Loss:  0.3214\n",
            "Epoch: 108 | Batch: 19 | Loss:  0.5632\n",
            "Epoch: 109 | Batch: 20 | Loss:  0.3524\n",
            "Epoch: 110 | Batch: 21 | Loss:  0.6253\n",
            "Epoch: 111 | Batch: 22 | Loss:  0.3316\n",
            "Epoch: 112 | Batch: 23 | Loss:  0.3505\n",
            "Epoch: 113 | Batch: 24 | Loss:  0.7266\n",
            "Epoch: 91 | Batch: 1 | Loss:  0.2591\n",
            "Epoch: 92 | Batch: 2 | Loss:  0.4451\n",
            "Epoch: 93 | Batch: 3 | Loss:  0.3742\n",
            "Epoch: 94 | Batch: 4 | Loss:  0.4239\n",
            "Epoch: 95 | Batch: 5 | Loss:  0.5925\n",
            "Epoch: 96 | Batch: 6 | Loss:  0.4470\n",
            "Epoch: 97 | Batch: 7 | Loss:  0.4544\n",
            "Epoch: 98 | Batch: 8 | Loss:  0.4166\n",
            "Epoch: 99 | Batch: 9 | Loss:  0.3555\n",
            "Epoch: 100 | Batch: 10 | Loss:  0.3405\n",
            "Epoch: 101 | Batch: 11 | Loss:  0.6514\n",
            "Epoch: 102 | Batch: 12 | Loss:  0.4212\n",
            "Epoch: 103 | Batch: 13 | Loss:  0.4427\n",
            "Epoch: 104 | Batch: 14 | Loss:  0.5342\n",
            "Epoch: 105 | Batch: 15 | Loss:  0.4297\n",
            "Epoch: 106 | Batch: 16 | Loss:  0.3610\n",
            "Epoch: 107 | Batch: 17 | Loss:  0.5281\n",
            "Epoch: 108 | Batch: 18 | Loss:  0.4460\n",
            "Epoch: 109 | Batch: 19 | Loss:  0.3958\n",
            "Epoch: 110 | Batch: 20 | Loss:  0.4172\n",
            "Epoch: 111 | Batch: 21 | Loss:  0.5077\n",
            "Epoch: 112 | Batch: 22 | Loss:  0.4437\n",
            "Epoch: 113 | Batch: 23 | Loss:  0.4677\n",
            "Epoch: 114 | Batch: 24 | Loss:  0.4183\n",
            "Epoch: 92 | Batch: 1 | Loss:  0.3615\n",
            "Epoch: 93 | Batch: 2 | Loss:  0.5501\n",
            "Epoch: 94 | Batch: 3 | Loss:  0.5366\n",
            "Epoch: 95 | Batch: 4 | Loss:  0.3077\n",
            "Epoch: 96 | Batch: 5 | Loss:  0.3897\n",
            "Epoch: 97 | Batch: 6 | Loss:  0.4348\n",
            "Epoch: 98 | Batch: 7 | Loss:  0.3739\n",
            "Epoch: 99 | Batch: 8 | Loss:  0.2967\n",
            "Epoch: 100 | Batch: 9 | Loss:  0.5004\n",
            "Epoch: 101 | Batch: 10 | Loss:  0.5278\n",
            "Epoch: 102 | Batch: 11 | Loss:  0.4086\n",
            "Epoch: 103 | Batch: 12 | Loss:  0.3467\n",
            "Epoch: 104 | Batch: 13 | Loss:  0.5772\n",
            "Epoch: 105 | Batch: 14 | Loss:  0.4929\n",
            "Epoch: 106 | Batch: 15 | Loss:  0.5395\n",
            "Epoch: 107 | Batch: 16 | Loss:  0.3816\n",
            "Epoch: 108 | Batch: 17 | Loss:  0.4049\n",
            "Epoch: 109 | Batch: 18 | Loss:  0.3677\n",
            "Epoch: 110 | Batch: 19 | Loss:  0.3857\n",
            "Epoch: 111 | Batch: 20 | Loss:  0.3807\n",
            "Epoch: 112 | Batch: 21 | Loss:  0.6354\n",
            "Epoch: 113 | Batch: 22 | Loss:  0.5242\n",
            "Epoch: 114 | Batch: 23 | Loss:  0.2900\n",
            "Epoch: 115 | Batch: 24 | Loss:  0.6418\n",
            "Epoch: 93 | Batch: 1 | Loss:  0.3869\n",
            "Epoch: 94 | Batch: 2 | Loss:  0.3775\n",
            "Epoch: 95 | Batch: 3 | Loss:  0.6341\n",
            "Epoch: 96 | Batch: 4 | Loss:  0.4048\n",
            "Epoch: 97 | Batch: 5 | Loss:  0.5328\n",
            "Epoch: 98 | Batch: 6 | Loss:  0.3572\n",
            "Epoch: 99 | Batch: 7 | Loss:  0.5145\n",
            "Epoch: 100 | Batch: 8 | Loss:  0.4035\n",
            "Epoch: 101 | Batch: 9 | Loss:  0.3949\n",
            "Epoch: 102 | Batch: 10 | Loss:  0.4967\n",
            "Epoch: 103 | Batch: 11 | Loss:  0.2534\n",
            "Epoch: 104 | Batch: 12 | Loss:  0.4022\n",
            "Epoch: 105 | Batch: 13 | Loss:  0.4219\n",
            "Epoch: 106 | Batch: 14 | Loss:  0.3564\n",
            "Epoch: 107 | Batch: 15 | Loss:  0.3268\n",
            "Epoch: 108 | Batch: 16 | Loss:  0.4972\n",
            "Epoch: 109 | Batch: 17 | Loss:  0.7020\n",
            "Epoch: 110 | Batch: 18 | Loss:  0.4899\n",
            "Epoch: 111 | Batch: 19 | Loss:  0.3962\n",
            "Epoch: 112 | Batch: 20 | Loss:  0.3972\n",
            "Epoch: 113 | Batch: 21 | Loss:  0.3248\n",
            "Epoch: 114 | Batch: 22 | Loss:  0.4592\n",
            "Epoch: 115 | Batch: 23 | Loss:  0.5730\n",
            "Epoch: 116 | Batch: 24 | Loss:  0.4803\n",
            "Epoch: 94 | Batch: 1 | Loss:  0.3888\n",
            "Epoch: 95 | Batch: 2 | Loss:  0.3848\n",
            "Epoch: 96 | Batch: 3 | Loss:  0.2810\n",
            "Epoch: 97 | Batch: 4 | Loss:  0.6140\n",
            "Epoch: 98 | Batch: 5 | Loss:  0.3742\n",
            "Epoch: 99 | Batch: 6 | Loss:  0.6296\n",
            "Epoch: 100 | Batch: 7 | Loss:  0.5840\n",
            "Epoch: 101 | Batch: 8 | Loss:  0.3855\n",
            "Epoch: 102 | Batch: 9 | Loss:  0.5789\n",
            "Epoch: 103 | Batch: 10 | Loss:  0.3375\n",
            "Epoch: 104 | Batch: 11 | Loss:  0.2946\n",
            "Epoch: 105 | Batch: 12 | Loss:  0.4424\n",
            "Epoch: 106 | Batch: 13 | Loss:  0.4774\n",
            "Epoch: 107 | Batch: 14 | Loss:  0.4574\n",
            "Epoch: 108 | Batch: 15 | Loss:  0.4496\n",
            "Epoch: 109 | Batch: 16 | Loss:  0.4910\n",
            "Epoch: 110 | Batch: 17 | Loss:  0.4762\n",
            "Epoch: 111 | Batch: 18 | Loss:  0.3299\n",
            "Epoch: 112 | Batch: 19 | Loss:  0.4699\n",
            "Epoch: 113 | Batch: 20 | Loss:  0.3560\n",
            "Epoch: 114 | Batch: 21 | Loss:  0.4434\n",
            "Epoch: 115 | Batch: 22 | Loss:  0.5187\n",
            "Epoch: 116 | Batch: 23 | Loss:  0.5113\n",
            "Epoch: 117 | Batch: 24 | Loss:  0.4017\n",
            "Epoch: 95 | Batch: 1 | Loss:  0.4723\n",
            "Epoch: 96 | Batch: 2 | Loss:  0.3380\n",
            "Epoch: 97 | Batch: 3 | Loss:  0.5594\n",
            "Epoch: 98 | Batch: 4 | Loss:  0.3601\n",
            "Epoch: 99 | Batch: 5 | Loss:  0.5257\n",
            "Epoch: 100 | Batch: 6 | Loss:  0.6206\n",
            "Epoch: 101 | Batch: 7 | Loss:  0.4110\n",
            "Epoch: 102 | Batch: 8 | Loss:  0.4352\n",
            "Epoch: 103 | Batch: 9 | Loss:  0.3282\n",
            "Epoch: 104 | Batch: 10 | Loss:  0.4229\n",
            "Epoch: 105 | Batch: 11 | Loss:  0.4632\n",
            "Epoch: 106 | Batch: 12 | Loss:  0.4532\n",
            "Epoch: 107 | Batch: 13 | Loss:  0.6096\n",
            "Epoch: 108 | Batch: 14 | Loss:  0.3646\n",
            "Epoch: 109 | Batch: 15 | Loss:  0.4982\n",
            "Epoch: 110 | Batch: 16 | Loss:  0.3557\n",
            "Epoch: 111 | Batch: 17 | Loss:  0.4037\n",
            "Epoch: 112 | Batch: 18 | Loss:  0.4141\n",
            "Epoch: 113 | Batch: 19 | Loss:  0.5884\n",
            "Epoch: 114 | Batch: 20 | Loss:  0.4645\n",
            "Epoch: 115 | Batch: 21 | Loss:  0.3402\n",
            "Epoch: 116 | Batch: 22 | Loss:  0.3957\n",
            "Epoch: 117 | Batch: 23 | Loss:  0.4110\n",
            "Epoch: 118 | Batch: 24 | Loss:  0.4446\n",
            "Epoch: 96 | Batch: 1 | Loss:  0.3353\n",
            "Epoch: 97 | Batch: 2 | Loss:  0.5529\n",
            "Epoch: 98 | Batch: 3 | Loss:  0.4176\n",
            "Epoch: 99 | Batch: 4 | Loss:  0.3907\n",
            "Epoch: 100 | Batch: 5 | Loss:  0.4764\n",
            "Epoch: 101 | Batch: 6 | Loss:  0.4399\n",
            "Epoch: 102 | Batch: 7 | Loss:  0.4966\n",
            "Epoch: 103 | Batch: 8 | Loss:  0.2448\n",
            "Epoch: 104 | Batch: 9 | Loss:  0.3595\n",
            "Epoch: 105 | Batch: 10 | Loss:  0.5076\n",
            "Epoch: 106 | Batch: 11 | Loss:  0.2485\n",
            "Epoch: 107 | Batch: 12 | Loss:  0.4113\n",
            "Epoch: 108 | Batch: 13 | Loss:  0.4070\n",
            "Epoch: 109 | Batch: 14 | Loss:  0.4338\n",
            "Epoch: 110 | Batch: 15 | Loss:  0.6574\n",
            "Epoch: 111 | Batch: 16 | Loss:  0.6320\n",
            "Epoch: 112 | Batch: 17 | Loss:  0.3402\n",
            "Epoch: 113 | Batch: 18 | Loss:  0.4867\n",
            "Epoch: 114 | Batch: 19 | Loss:  0.5381\n",
            "Epoch: 115 | Batch: 20 | Loss:  0.4361\n",
            "Epoch: 116 | Batch: 21 | Loss:  0.3434\n",
            "Epoch: 117 | Batch: 22 | Loss:  0.3459\n",
            "Epoch: 118 | Batch: 23 | Loss:  0.3929\n",
            "Epoch: 119 | Batch: 24 | Loss:  0.8581\n",
            "Epoch: 97 | Batch: 1 | Loss:  0.4226\n",
            "Epoch: 98 | Batch: 2 | Loss:  0.3071\n",
            "Epoch: 99 | Batch: 3 | Loss:  0.4986\n",
            "Epoch: 100 | Batch: 4 | Loss:  0.3835\n",
            "Epoch: 101 | Batch: 5 | Loss:  0.5470\n",
            "Epoch: 102 | Batch: 6 | Loss:  0.4191\n",
            "Epoch: 103 | Batch: 7 | Loss:  0.4943\n",
            "Epoch: 104 | Batch: 8 | Loss:  0.4284\n",
            "Epoch: 105 | Batch: 9 | Loss:  0.3987\n",
            "Epoch: 106 | Batch: 10 | Loss:  0.3433\n",
            "Epoch: 107 | Batch: 11 | Loss:  0.3960\n",
            "Epoch: 108 | Batch: 12 | Loss:  0.4777\n",
            "Epoch: 109 | Batch: 13 | Loss:  0.3165\n",
            "Epoch: 110 | Batch: 14 | Loss:  0.3714\n",
            "Epoch: 111 | Batch: 15 | Loss:  0.5058\n",
            "Epoch: 112 | Batch: 16 | Loss:  0.4386\n",
            "Epoch: 113 | Batch: 17 | Loss:  0.5696\n",
            "Epoch: 114 | Batch: 18 | Loss:  0.4798\n",
            "Epoch: 115 | Batch: 19 | Loss:  0.3785\n",
            "Epoch: 116 | Batch: 20 | Loss:  0.6688\n",
            "Epoch: 117 | Batch: 21 | Loss:  0.3869\n",
            "Epoch: 118 | Batch: 22 | Loss:  0.4495\n",
            "Epoch: 119 | Batch: 23 | Loss:  0.2962\n",
            "Epoch: 120 | Batch: 24 | Loss:  0.5685\n",
            "Epoch: 98 | Batch: 1 | Loss:  0.4230\n",
            "Epoch: 99 | Batch: 2 | Loss:  0.5207\n",
            "Epoch: 100 | Batch: 3 | Loss:  0.3230\n",
            "Epoch: 101 | Batch: 4 | Loss:  0.5868\n",
            "Epoch: 102 | Batch: 5 | Loss:  0.4506\n",
            "Epoch: 103 | Batch: 6 | Loss:  0.3970\n",
            "Epoch: 104 | Batch: 7 | Loss:  0.4967\n",
            "Epoch: 105 | Batch: 8 | Loss:  0.5733\n",
            "Epoch: 106 | Batch: 9 | Loss:  0.4931\n",
            "Epoch: 107 | Batch: 10 | Loss:  0.5375\n",
            "Epoch: 108 | Batch: 11 | Loss:  0.6232\n",
            "Epoch: 109 | Batch: 12 | Loss:  0.3911\n",
            "Epoch: 110 | Batch: 13 | Loss:  0.4050\n",
            "Epoch: 111 | Batch: 14 | Loss:  0.3525\n",
            "Epoch: 112 | Batch: 15 | Loss:  0.4326\n",
            "Epoch: 113 | Batch: 16 | Loss:  0.2845\n",
            "Epoch: 114 | Batch: 17 | Loss:  0.4627\n",
            "Epoch: 115 | Batch: 18 | Loss:  0.4175\n",
            "Epoch: 116 | Batch: 19 | Loss:  0.3009\n",
            "Epoch: 117 | Batch: 20 | Loss:  0.5076\n",
            "Epoch: 118 | Batch: 21 | Loss:  0.4283\n",
            "Epoch: 119 | Batch: 22 | Loss:  0.2524\n",
            "Epoch: 120 | Batch: 23 | Loss:  0.5399\n",
            "Epoch: 121 | Batch: 24 | Loss:  0.3858\n",
            "Epoch: 99 | Batch: 1 | Loss:  0.4588\n",
            "Epoch: 100 | Batch: 2 | Loss:  0.2964\n",
            "Epoch: 101 | Batch: 3 | Loss:  0.6292\n",
            "Epoch: 102 | Batch: 4 | Loss:  0.4958\n",
            "Epoch: 103 | Batch: 5 | Loss:  0.4407\n",
            "Epoch: 104 | Batch: 6 | Loss:  0.4159\n",
            "Epoch: 105 | Batch: 7 | Loss:  0.3903\n",
            "Epoch: 106 | Batch: 8 | Loss:  0.4799\n",
            "Epoch: 107 | Batch: 9 | Loss:  0.4314\n",
            "Epoch: 108 | Batch: 10 | Loss:  0.3702\n",
            "Epoch: 109 | Batch: 11 | Loss:  0.3031\n",
            "Epoch: 110 | Batch: 12 | Loss:  0.4808\n",
            "Epoch: 111 | Batch: 13 | Loss:  0.5135\n",
            "Epoch: 112 | Batch: 14 | Loss:  0.3774\n",
            "Epoch: 113 | Batch: 15 | Loss:  0.5238\n",
            "Epoch: 114 | Batch: 16 | Loss:  0.4345\n",
            "Epoch: 115 | Batch: 17 | Loss:  0.3256\n",
            "Epoch: 116 | Batch: 18 | Loss:  0.4949\n",
            "Epoch: 117 | Batch: 19 | Loss:  0.4336\n",
            "Epoch: 118 | Batch: 20 | Loss:  0.3956\n",
            "Epoch: 119 | Batch: 21 | Loss:  0.4554\n",
            "Epoch: 120 | Batch: 22 | Loss:  0.5763\n",
            "Epoch: 121 | Batch: 23 | Loss:  0.5399\n",
            "Epoch: 122 | Batch: 24 | Loss:  0.3361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRXuXyU9sxLv"
      },
      "source": [
        "- loss가 일정하게 감소하지 않는 이유\n",
        "  - batch statistic이 모집단의 statistic과 다르기 때문에 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy0vTqxztAH3"
      },
      "source": [
        "## Softmax Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv4ndXrxiSg_",
        "outputId": "6b554cc5-345f-47aa-dec9-5afa916630ee"
      },
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train = True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                               batch_size=batch_size,\n",
        "                               shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MNIST Model on cuda\n",
            "============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxXSCr5siSe5"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(784, 520)\n",
        "        self.l2 = nn.Linear(520, 320)\n",
        "        self.l3 = nn.Linear(320, 240)\n",
        "        self.l4 = nn.Linear(240, 120)\n",
        "        self.l5 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHdOMGqKiSZo"
      },
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gVhxaebiSWU"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {0} | Batch Status: {1}/{2} ({3:.0f}%) | Loss: {4:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('{0}\nTest set: Average loss: {1:.4f}, Accuracy: {2} / {3} ({4:.0f}%)'.format( '\n",
        "    '=' * 50, test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMk3NYRmFbef",
        "outputId": "fa3bc5b6-35f2-425b-d8fb-011cf3fe5681"
      },
      "source": [
        "start = time.time()\n",
        "for epoch in range(1, 10):\n",
        "    epoch_start = time.time()\n",
        "    train(epoch)\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print('Training time: {0:.0f}m {1:.0f}s').format(m, s)\n",
        "    test()\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print('Testing time: {0:.0f}m {1:.0f}s').format(m, x)\n",
        "m, s = divmod(time.time() - start, 60)\n",
        "print('Totatl Time: {0:.0f}m {1:.0f}s\\nModel was trained on {2}'.format(m, s, device))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 1.704619\n",
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 1.754647\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 1.612978\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 1.617566\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 1.518240\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 1.530224\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 1.433820\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 1.448874\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 1.439362\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 1.313036\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 1.232088\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 1.244667\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 1.436423\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 1.249957\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 1.275509\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 1.037451\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 1.240641\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 0.986538\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 1.133991\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 1.108026\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 1.012209\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 1.125203\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 0.911472\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 0.928932\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 1.128002\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 0.960376\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 0.966085\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 0.722940\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 0.833247\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 0.785602\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 0.800555\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 0.765614\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 0.913971\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 0.761627\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 0.776147\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 0.664483\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 0.661503\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 0.714063\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 0.832174\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 0.564941\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 0.591425\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 0.695863\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 0.480942\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 0.693935\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 0.708556\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 0.698928\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 0.494237\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 0.622241\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 0.580482\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 0.791605\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.511453\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 0.689735\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 0.933248\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 0.462108\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 0.450019\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 0.843020\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 0.522647\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 0.413949\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 0.447293\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 0.427322\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 0.534072\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 0.294233\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 0.795019\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 0.436502\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 0.713367\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 0.356070\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 0.297177\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 0.563389\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 0.456796\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 0.492760\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 0.715976\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 0.627621\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 0.527459\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 0.407814\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 0.465859\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 0.716859\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 0.322236\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 0.535040\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 0.445083\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 0.346917\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 0.544884\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 0.591970\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 0.485538\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 0.422341\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 0.500051\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 0.524450\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 0.360152\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 0.328914\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 0.397500\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 0.374379\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 0.275731\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 0.449599\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 0.328868\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 0.335288\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0069, Accuracy: 8713 / 10000 (87%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.400377\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 0.480252\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 0.437054\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 0.301380\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 0.419447\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 0.475853\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 0.399079\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 0.393585\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 0.317561\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.613060\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.592026\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 0.386302\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.332324\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.374414\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.492189\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.526008\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.411533\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.231712\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.373239\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.271760\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.395717\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.428102\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.479855\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.353993\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.411412\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.364776\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.362739\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.388398\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.369057\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.361113\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.448747\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.616093\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.325772\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.323622\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.478320\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.284011\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.571718\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.372217\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.283497\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.361931\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.392945\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.296036\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.382655\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.415405\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.252608\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.309993\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.438516\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.294678\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.305880\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.393735\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.210797\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.338001\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.406004\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.252131\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.317395\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.450858\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.389947\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.429062\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.388290\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.270487\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.170747\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.398351\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.386963\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.308869\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.301748\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.299301\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.563136\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.191134\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.253810\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.235962\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.191154\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.247654\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.206111\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.188802\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.151544\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.499169\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.388678\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.247581\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.264069\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.308675\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.290357\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.395977\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.288245\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.366461\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.453616\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.285122\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.223533\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.280626\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.328435\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.246730\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.361469\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.260792\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.250610\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.335932\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0047, Accuracy: 9121 / 10000 (91%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.202925\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.346684\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.297718\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.484894\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.289239\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.208924\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.170458\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.294526\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.293893\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.381283\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.380662\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.306486\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.162847\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.389471\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.409700\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.189987\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.229399\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.181718\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.275477\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.279480\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.221039\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.119184\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.278875\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.240334\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.272714\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.164407\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.174341\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.309367\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.487908\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.174702\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.467235\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.231576\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.261068\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.296023\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.304789\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.206899\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.511561\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.236289\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.365254\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.190307\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.404912\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.215022\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.176911\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.168961\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.147191\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.249807\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.305743\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.242487\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.260620\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.208737\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.204029\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.260486\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.116763\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.278463\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.240410\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.186932\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.241025\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.395688\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.188888\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.228516\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.270734\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.331719\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.386214\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.209195\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.168024\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.206350\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.210688\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.287743\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.143386\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.151009\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.212156\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.183110\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.252100\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.151140\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.195813\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.272053\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.528335\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.175305\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.247275\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.370528\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.177590\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.174919\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.257338\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.325674\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.119859\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.210075\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.250631\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.270136\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.281362\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.201122\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.341822\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.142657\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.228023\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.198026\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0036, Accuracy: 9309 / 10000 (93%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.217774\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.321693\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.256719\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.101060\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.252198\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.090209\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.287326\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.133381\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.283524\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.209865\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.216053\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.308741\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.150197\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.307964\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.128379\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.098314\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.176128\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.354217\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.143405\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.314536\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.208378\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.107536\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.173792\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.173965\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.249009\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.145210\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.285830\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.267464\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.292007\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.240569\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.114203\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.295878\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.199024\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.185914\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.386192\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.186156\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.261504\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.394641\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.231057\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.110141\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.165781\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.186141\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.136259\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.326566\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.169476\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.232001\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.252183\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.248467\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.236723\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.180773\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.075422\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.149542\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.136538\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.137055\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.116287\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.137977\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.312314\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.170163\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.131768\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.165840\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.139280\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.209794\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.172035\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.132986\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.230596\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.153642\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.420963\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.213921\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.256469\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.133749\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.216912\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.099918\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.227538\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.080565\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.138090\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.156615\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.159905\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.106990\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.167383\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.137308\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.203854\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.146955\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.277011\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.105921\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.125947\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.300603\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.228398\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.128066\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.117116\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.081410\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.245239\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.358527\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.145401\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.144446\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0029, Accuracy: 9440 / 10000 (94%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.113910\n",
            "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.097908\n",
            "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.214551\n",
            "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.153444\n",
            "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.083114\n",
            "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.254200\n",
            "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.029358\n",
            "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.214780\n",
            "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.447889\n",
            "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.064311\n",
            "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.127721\n",
            "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.108357\n",
            "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.164905\n",
            "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.201215\n",
            "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.212158\n",
            "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.119147\n",
            "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.248936\n",
            "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.270875\n",
            "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.123953\n",
            "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.075482\n",
            "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.108906\n",
            "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.199701\n",
            "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.300136\n",
            "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.100775\n",
            "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.211604\n",
            "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.191742\n",
            "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.138536\n",
            "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.172029\n",
            "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.284847\n",
            "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.166758\n",
            "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.083260\n",
            "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.110306\n",
            "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.169107\n",
            "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.191523\n",
            "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.107542\n",
            "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.199775\n",
            "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.135203\n",
            "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.177061\n",
            "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.195627\n",
            "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.240571\n",
            "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.134271\n",
            "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.096881\n",
            "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.153367\n",
            "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.154939\n",
            "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.313783\n",
            "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.204388\n",
            "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.067683\n",
            "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.143414\n",
            "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.177450\n",
            "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.114747\n",
            "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.177612\n",
            "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.133170\n",
            "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.198265\n",
            "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.107703\n",
            "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.101672\n",
            "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.195524\n",
            "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.218369\n",
            "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.267587\n",
            "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.174689\n",
            "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.055378\n",
            "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.104936\n",
            "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.126981\n",
            "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.149573\n",
            "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.237921\n",
            "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.219069\n",
            "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.125111\n",
            "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.109856\n",
            "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.228394\n",
            "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.137755\n",
            "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.075403\n",
            "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.081949\n",
            "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.180559\n",
            "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.256548\n",
            "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.078815\n",
            "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.064744\n",
            "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.210677\n",
            "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.180526\n",
            "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.074573\n",
            "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.151880\n",
            "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.194466\n",
            "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.236378\n",
            "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.088745\n",
            "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.047438\n",
            "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.052256\n",
            "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.199971\n",
            "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.175068\n",
            "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.089513\n",
            "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.156153\n",
            "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.077268\n",
            "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.195486\n",
            "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.113307\n",
            "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.199515\n",
            "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.108645\n",
            "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.027204\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0023, Accuracy: 9568 / 10000 (96%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.183579\n",
            "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.095422\n",
            "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.072259\n",
            "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.123866\n",
            "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.085476\n",
            "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.097389\n",
            "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.112529\n",
            "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.080092\n",
            "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.152352\n",
            "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.095097\n",
            "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.039485\n",
            "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.104006\n",
            "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.224099\n",
            "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.141799\n",
            "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.207957\n",
            "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.261992\n",
            "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.247598\n",
            "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.128959\n",
            "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.161026\n",
            "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.039179\n",
            "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.190290\n",
            "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.160960\n",
            "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.088987\n",
            "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.061684\n",
            "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.099666\n",
            "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.207708\n",
            "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.155722\n",
            "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.041282\n",
            "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.052111\n",
            "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.082003\n",
            "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.073223\n",
            "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.063925\n",
            "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.084235\n",
            "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.145662\n",
            "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.088981\n",
            "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.156116\n",
            "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.092257\n",
            "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.127307\n",
            "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.122890\n",
            "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.150869\n",
            "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.126210\n",
            "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.115754\n",
            "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.231075\n",
            "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.053080\n",
            "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.150142\n",
            "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.101666\n",
            "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.086218\n",
            "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.043163\n",
            "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.091829\n",
            "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.091395\n",
            "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.076314\n",
            "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.146376\n",
            "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.039744\n",
            "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.222537\n",
            "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.077098\n",
            "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.146092\n",
            "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.222248\n",
            "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.320567\n",
            "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.146983\n",
            "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.194478\n",
            "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.150601\n",
            "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.187592\n",
            "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.264152\n",
            "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.044862\n",
            "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.281666\n",
            "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.061764\n",
            "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.124026\n",
            "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.102777\n",
            "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.302378\n",
            "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.159291\n",
            "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.061149\n",
            "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.223455\n",
            "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.035540\n",
            "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.169883\n",
            "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.264477\n",
            "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.092207\n",
            "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.126738\n",
            "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.052905\n",
            "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.054754\n",
            "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.347134\n",
            "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.052680\n",
            "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.171584\n",
            "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.187685\n",
            "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.146656\n",
            "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.086627\n",
            "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.141780\n",
            "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.037915\n",
            "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.202261\n",
            "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.079347\n",
            "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.061967\n",
            "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.090792\n",
            "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.094560\n",
            "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.102035\n",
            "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.053097\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0021, Accuracy: 9610 / 10000 (96%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.094998\n",
            "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.081910\n",
            "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.064721\n",
            "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.046152\n",
            "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.108819\n",
            "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.061813\n",
            "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.134337\n",
            "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.083211\n",
            "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.084047\n",
            "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.077095\n",
            "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.027695\n",
            "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.155936\n",
            "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.061941\n",
            "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.117734\n",
            "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.078025\n",
            "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.126197\n",
            "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.110480\n",
            "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.094423\n",
            "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.042011\n",
            "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.287869\n",
            "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.175567\n",
            "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.026216\n",
            "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.039166\n",
            "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.133853\n",
            "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.052730\n",
            "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.106057\n",
            "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.151570\n",
            "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.147868\n",
            "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.012697\n",
            "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.152528\n",
            "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.030000\n",
            "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.088474\n",
            "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.176225\n",
            "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.114936\n",
            "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.044267\n",
            "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.092509\n",
            "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.132566\n",
            "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.227361\n",
            "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.043475\n",
            "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.167768\n",
            "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.030792\n",
            "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.069756\n",
            "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.125749\n",
            "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.041417\n",
            "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.062443\n",
            "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.028625\n",
            "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.255901\n",
            "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.017846\n",
            "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.170803\n",
            "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.039536\n",
            "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.081014\n",
            "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.047473\n",
            "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.112177\n",
            "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.029517\n",
            "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.201036\n",
            "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.123317\n",
            "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.134563\n",
            "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.114837\n",
            "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.131739\n",
            "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.144856\n",
            "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.166910\n",
            "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.060019\n",
            "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.058493\n",
            "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.109449\n",
            "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.085212\n",
            "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.206021\n",
            "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.102334\n",
            "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.049537\n",
            "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.247231\n",
            "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.110070\n",
            "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.088723\n",
            "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.032713\n",
            "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.045726\n",
            "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.105234\n",
            "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.072686\n",
            "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.147357\n",
            "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.090404\n",
            "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.189219\n",
            "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.074175\n",
            "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.133846\n",
            "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.130608\n",
            "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.135220\n",
            "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.038669\n",
            "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.120705\n",
            "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.281957\n",
            "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.120926\n",
            "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.138054\n",
            "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.078634\n",
            "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.030723\n",
            "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.106852\n",
            "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.073946\n",
            "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.177602\n",
            "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.044964\n",
            "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.093357\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0019, Accuracy: 9662 / 10000 (97%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.041092\n",
            "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.043838\n",
            "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.076707\n",
            "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.145590\n",
            "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.027864\n",
            "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.030856\n",
            "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.029106\n",
            "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.109882\n",
            "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.070669\n",
            "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.084390\n",
            "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.056721\n",
            "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.056649\n",
            "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.064108\n",
            "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.079677\n",
            "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.129556\n",
            "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.087635\n",
            "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.108851\n",
            "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.093376\n",
            "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.190488\n",
            "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.027810\n",
            "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.147692\n",
            "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.061536\n",
            "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.029760\n",
            "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.170739\n",
            "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.040069\n",
            "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.119552\n",
            "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.067591\n",
            "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.131180\n",
            "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.033455\n",
            "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.092328\n",
            "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.107126\n",
            "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.083080\n",
            "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.191558\n",
            "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.051561\n",
            "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.097783\n",
            "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.053131\n",
            "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.151836\n",
            "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.117090\n",
            "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.119527\n",
            "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.051264\n",
            "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.272910\n",
            "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.032949\n",
            "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.103414\n",
            "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.190863\n",
            "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.159532\n",
            "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.096474\n",
            "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.047384\n",
            "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.101939\n",
            "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.154138\n",
            "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.205804\n",
            "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.029254\n",
            "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.043234\n",
            "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.083890\n",
            "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.155714\n",
            "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.105043\n",
            "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.032193\n",
            "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.075858\n",
            "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.073540\n",
            "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.148397\n",
            "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.021433\n",
            "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.111022\n",
            "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.043189\n",
            "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.054405\n",
            "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.046819\n",
            "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.057210\n",
            "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.129400\n",
            "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.123059\n",
            "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.136905\n",
            "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.145924\n",
            "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.031258\n",
            "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.026473\n",
            "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.046938\n",
            "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.065307\n",
            "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.048703\n",
            "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.040951\n",
            "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.203908\n",
            "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.139176\n",
            "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.128315\n",
            "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.039830\n",
            "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.109819\n",
            "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.066385\n",
            "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.039258\n",
            "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.053830\n",
            "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.126172\n",
            "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.033352\n",
            "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.174666\n",
            "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.040082\n",
            "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.090309\n",
            "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.011152\n",
            "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.027572\n",
            "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.084166\n",
            "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.058701\n",
            "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.093438\n",
            "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.046056\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0016, Accuracy: 9688 / 10000 (97%)\n",
            "Testing time: 0m 6s\n",
            "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.105597\n",
            "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.028753\n",
            "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.119956\n",
            "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.176814\n",
            "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.105960\n",
            "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.052027\n",
            "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.045730\n",
            "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.031493\n",
            "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.168748\n",
            "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.158375\n",
            "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.235366\n",
            "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.016003\n",
            "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.033654\n",
            "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.051839\n",
            "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.042212\n",
            "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.054501\n",
            "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.063113\n",
            "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.020814\n",
            "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.209160\n",
            "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.047860\n",
            "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.065376\n",
            "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.071599\n",
            "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.096227\n",
            "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.134850\n",
            "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.119556\n",
            "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.102148\n",
            "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.032062\n",
            "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.023361\n",
            "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.101410\n",
            "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.093263\n",
            "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.032890\n",
            "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.118000\n",
            "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.155449\n",
            "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.070816\n",
            "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.085031\n",
            "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.065864\n",
            "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.051078\n",
            "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.145871\n",
            "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.045758\n",
            "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.073918\n",
            "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.019497\n",
            "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.118361\n",
            "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.039308\n",
            "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.022682\n",
            "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.165304\n",
            "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.065631\n",
            "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.087628\n",
            "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.146234\n",
            "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.082763\n",
            "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.036740\n",
            "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.100284\n",
            "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.013273\n",
            "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.142766\n",
            "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.039158\n",
            "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.062424\n",
            "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.176453\n",
            "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.140939\n",
            "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.047582\n",
            "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.044316\n",
            "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.017384\n",
            "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.059316\n",
            "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.040962\n",
            "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.035371\n",
            "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.064660\n",
            "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.026023\n",
            "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.039693\n",
            "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.056057\n",
            "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.135345\n",
            "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.071180\n",
            "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.173600\n",
            "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.036136\n",
            "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.142058\n",
            "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.082309\n",
            "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.079019\n",
            "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.041639\n",
            "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.036903\n",
            "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.041821\n",
            "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.068834\n",
            "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.240430\n",
            "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.054815\n",
            "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.224736\n",
            "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.049518\n",
            "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.053898\n",
            "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.089281\n",
            "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.218924\n",
            "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.042172\n",
            "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.097783\n",
            "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.026085\n",
            "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.069461\n",
            "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.111321\n",
            "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.185613\n",
            "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.051379\n",
            "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.072901\n",
            "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.137929\n",
            "Training time: 0m 0s\n",
            "=============================\n",
            "Test set: Average loss: 0.0015, Accuracy: 9699 / 10000 (97%)\n",
            "Testing time: 0m 6s\n",
            "Totatl Time: 0m 57s\n",
            "Model was trained on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8w_DGP7ieUB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
