{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5Xr1MtQ8r7kEI6KodM3QZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuEwdScvO_sr"
      },
      "source": [
        "# 신경망 기초"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lhnsgiEgg8X"
      },
      "source": [
        "## 4-1 OR 데이터에 퍼셉트론 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laS9N9ObwLK1",
        "outputId": "3f75078b-05ff-4d76-96ed-d0b60560c326"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# 훈련 집합 구축\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y = [-1, 1, 1, 1]\n",
        "\n",
        "# fit 함수로 Perceptron 학습\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "print('학습된 퍼셉트론의 매개변수:', perceptron.coef_, perceptron.intercept_)\n",
        "print('훈련집합에 대한 예측:', perceptron.predict(X))\n",
        "print('정황률 측정: {}%'.format(perceptron.score(X, y) * 100))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습된 퍼셉트론의 매개변수: [[2. 2.]] [-1.]\n",
            "훈련집합에 대한 예측: [-1  1  1  1]\n",
            "정황률 측정: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxJqsGyfhL62"
      },
      "source": [
        "## 4-2 필기 숫자 데이터에 퍼셉트론 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q2edXjGhQOV",
        "outputId": "06320e6d-acb9-405c-8417-d600abd593b2"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "digit = datasets.load_digits()\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size=0.6)\n",
        "\n",
        "# verbose- 학습과정에서 발생하는 메세지 출력\n",
        "perceptron = Perceptron(max_iter=100, eta0=0.001, verbose=1)\n",
        "perceptron.fit(x_train, y_train)\n",
        "\n",
        "result = perceptron.predict(x_test)\n",
        "#print(result)\n",
        "\n",
        "conf_mat = np.zeros((10, 10))\n",
        "for i in range(len(result)):\n",
        "    conf_mat[result[i]][y_test[i]] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 53, Bias: -0.003000, T: 1078, Avg. loss: 0.020678\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.29, NNZs: 53, Bias: -0.003000, T: 2156, Avg. loss: 0.002637\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.32, NNZs: 53, Bias: -0.005000, T: 3234, Avg. loss: 0.005288\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.35, NNZs: 53, Bias: -0.005000, T: 4312, Avg. loss: 0.004568\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.39, NNZs: 53, Bias: -0.005000, T: 5390, Avg. loss: 0.004780\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.40, NNZs: 53, Bias: -0.006000, T: 6468, Avg. loss: 0.001766\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.41, NNZs: 53, Bias: -0.007000, T: 7546, Avg. loss: 0.001998\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.38, NNZs: 54, Bias: -0.006000, T: 1078, Avg. loss: 0.109201\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.50, NNZs: 56, Bias: -0.010000, T: 2156, Avg. loss: 0.061920\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.55, NNZs: 57, Bias: -0.013000, T: 3234, Avg. loss: 0.054964\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.61, NNZs: 57, Bias: -0.015000, T: 4312, Avg. loss: 0.051780\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.66, NNZs: 57, Bias: -0.018000, T: 5390, Avg. loss: 0.041738\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.69, NNZs: 57, Bias: -0.020000, T: 6468, Avg. loss: 0.050162\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.72, NNZs: 57, Bias: -0.022000, T: 7546, Avg. loss: 0.040517\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.75, NNZs: 57, Bias: -0.025000, T: 8624, Avg. loss: 0.044208\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.79, NNZs: 57, Bias: -0.027000, T: 9702, Avg. loss: 0.038575\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.83, NNZs: 58, Bias: -0.029000, T: 10780, Avg. loss: 0.034429\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.85, NNZs: 58, Bias: -0.032000, T: 11858, Avg. loss: 0.032688\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.87, NNZs: 58, Bias: -0.034000, T: 12936, Avg. loss: 0.028948\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.88, NNZs: 58, Bias: -0.036000, T: 14014, Avg. loss: 0.036838\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.91, NNZs: 58, Bias: -0.037000, T: 15092, Avg. loss: 0.024327\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.93, NNZs: 58, Bias: -0.040000, T: 16170, Avg. loss: 0.035209\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.95, NNZs: 58, Bias: -0.042000, T: 17248, Avg. loss: 0.035089\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 0.98, NNZs: 58, Bias: -0.044000, T: 18326, Avg. loss: 0.023450\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1.01, NNZs: 58, Bias: -0.044000, T: 19404, Avg. loss: 0.035579\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1.03, NNZs: 58, Bias: -0.048000, T: 20482, Avg. loss: 0.036520\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.30, NNZs: 54, Bias: -0.003000, T: 1078, Avg. loss: 0.042501\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.38, NNZs: 54, Bias: -0.003000, T: 2156, Avg. loss: 0.017842\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.44, NNZs: 54, Bias: -0.005000, T: 3234, Avg. loss: 0.011878\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.48, NNZs: 54, Bias: -0.006000, T: 4312, Avg. loss: 0.005275\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.48, NNZs: 54, Bias: -0.006000, T: 5390, Avg. loss: 0.000910\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.50, NNZs: 54, Bias: -0.006000, T: 6468, Avg. loss: 0.003920\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.51, NNZs: 54, Bias: -0.006000, T: 7546, Avg. loss: 0.001313\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.51, NNZs: 54, Bias: -0.006000, T: 8624, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.51, NNZs: 54, Bias: -0.006000, T: 9702, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.51, NNZs: 54, Bias: -0.006000, T: 10780, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 10 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.34, NNZs: 53, Bias: -0.002000, T: 1078, Avg. loss: 0.064058\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.43, NNZs: 52, Bias: -0.004000, T: 2156, Avg. loss: 0.027904\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.48, NNZs: 53, Bias: -0.004000, T: 3234, Avg. loss: 0.025877\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.51, NNZs: 53, Bias: -0.005000, T: 4312, Avg. loss: 0.020985\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.55, NNZs: 52, Bias: -0.005000, T: 5390, Avg. loss: 0.027141\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.58, NNZs: 53, Bias: -0.005000, T: 6468, Avg. loss: 0.015263\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.61, NNZs: 54, Bias: -0.006000, T: 7546, Avg. loss: 0.017508\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.64, NNZs: 55, Bias: -0.006000, T: 8624, Avg. loss: 0.017040\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.65, NNZs: 55, Bias: -0.006000, T: 9702, Avg. loss: 0.022505\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.68, NNZs: 55, Bias: -0.006000, T: 10780, Avg. loss: 0.027170\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.71, NNZs: 55, Bias: -0.006000, T: 11858, Avg. loss: 0.019699\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 11 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 49, Bias: -0.001000, T: 1078, Avg. loss: 0.033782\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.35, NNZs: 54, Bias: -0.001000, T: 2156, Avg. loss: 0.015831\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.36, NNZs: 54, Bias: -0.001000, T: 3234, Avg. loss: 0.005182\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.40, NNZs: 55, Bias: -0.001000, T: 4312, Avg. loss: 0.008312\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.44, NNZs: 55, Bias: -0.001000, T: 5390, Avg. loss: 0.009491\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.45, NNZs: 55, Bias: 0.000000, T: 6468, Avg. loss: 0.003812\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.48, NNZs: 55, Bias: 0.000000, T: 7546, Avg. loss: 0.006615\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.51, NNZs: 55, Bias: 0.000000, T: 8624, Avg. loss: 0.008163\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.54, NNZs: 55, Bias: 0.000000, T: 9702, Avg. loss: 0.007378\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.56, NNZs: 55, Bias: 0.000000, T: 10780, Avg. loss: 0.005966\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.58, NNZs: 55, Bias: 0.000000, T: 11858, Avg. loss: 0.003301\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 11 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.34, NNZs: 57, Bias: -0.002000, T: 1078, Avg. loss: 0.041525\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.40, NNZs: 56, Bias: -0.003000, T: 2156, Avg. loss: 0.011437\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.43, NNZs: 57, Bias: -0.003000, T: 3234, Avg. loss: 0.012723\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 57, Bias: -0.003000, T: 4312, Avg. loss: 0.013980\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.51, NNZs: 57, Bias: -0.003000, T: 5390, Avg. loss: 0.012045\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.54, NNZs: 57, Bias: -0.004000, T: 6468, Avg. loss: 0.009909\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.56, NNZs: 57, Bias: -0.004000, T: 7546, Avg. loss: 0.004921\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.58, NNZs: 57, Bias: -0.004000, T: 8624, Avg. loss: 0.003225\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.59, NNZs: 57, Bias: -0.003000, T: 9702, Avg. loss: 0.004006\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.60, NNZs: 57, Bias: -0.004000, T: 10780, Avg. loss: 0.001942\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.63, NNZs: 57, Bias: -0.004000, T: 11858, Avg. loss: 0.003101\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.64, NNZs: 57, Bias: -0.004000, T: 12936, Avg. loss: 0.006782\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.65, NNZs: 57, Bias: -0.004000, T: 14014, Avg. loss: 0.004249\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.66, NNZs: 57, Bias: -0.005000, T: 15092, Avg. loss: 0.002103\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.68, NNZs: 57, Bias: -0.005000, T: 16170, Avg. loss: 0.007965\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 15 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.32, NNZs: 52, Bias: -0.004000, T: 1078, Avg. loss: 0.037767\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.40, NNZs: 52, Bias: -0.005000, T: 2156, Avg. loss: 0.018409\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.42, NNZs: 52, Bias: -0.006000, T: 3234, Avg. loss: 0.016012\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.45, NNZs: 54, Bias: -0.007000, T: 4312, Avg. loss: 0.019388\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.46, NNZs: 54, Bias: -0.008000, T: 5390, Avg. loss: 0.008696\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.49, NNZs: 54, Bias: -0.008000, T: 6468, Avg. loss: 0.014006\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.52, NNZs: 54, Bias: -0.009000, T: 7546, Avg. loss: 0.017881\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.54, NNZs: 54, Bias: -0.010000, T: 8624, Avg. loss: 0.012340\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.56, NNZs: 54, Bias: -0.009000, T: 9702, Avg. loss: 0.008803\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.58, NNZs: 54, Bias: -0.011000, T: 10780, Avg. loss: 0.010274\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 10 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 53, Bias: 0.000000, T: 1078, Avg. loss: 0.042908\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.35, NNZs: 53, Bias: 0.000000, T: 2156, Avg. loss: 0.013641\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.40, NNZs: 53, Bias: 0.000000, T: 3234, Avg. loss: 0.021194\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.43, NNZs: 53, Bias: -0.001000, T: 4312, Avg. loss: 0.010531\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.45, NNZs: 53, Bias: 0.000000, T: 5390, Avg. loss: 0.011351\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.46, NNZs: 54, Bias: -0.001000, T: 6468, Avg. loss: 0.011391\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.49, NNZs: 54, Bias: -0.001000, T: 7546, Avg. loss: 0.018295\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.51, NNZs: 54, Bias: -0.002000, T: 8624, Avg. loss: 0.011499\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.54, NNZs: 53, Bias: -0.001000, T: 9702, Avg. loss: 0.009724\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 9 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.45, NNZs: 53, Bias: -0.006000, T: 1078, Avg. loss: 0.121783\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.54, NNZs: 54, Bias: -0.009000, T: 2156, Avg. loss: 0.082534\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.57, NNZs: 54, Bias: -0.011000, T: 3234, Avg. loss: 0.072431\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.63, NNZs: 54, Bias: -0.012000, T: 4312, Avg. loss: 0.078062\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.67, NNZs: 54, Bias: -0.015000, T: 5390, Avg. loss: 0.061870\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.72, NNZs: 54, Bias: -0.016000, T: 6468, Avg. loss: 0.066237\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.77, NNZs: 54, Bias: -0.017000, T: 7546, Avg. loss: 0.067172\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.81, NNZs: 56, Bias: -0.019000, T: 8624, Avg. loss: 0.066666\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.84, NNZs: 55, Bias: -0.020000, T: 9702, Avg. loss: 0.070982\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.87, NNZs: 55, Bias: -0.022000, T: 10780, Avg. loss: 0.056939\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.90, NNZs: 55, Bias: -0.025000, T: 11858, Avg. loss: 0.078201\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.91, NNZs: 55, Bias: -0.026000, T: 12936, Avg. loss: 0.065958\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.95, NNZs: 56, Bias: -0.028000, T: 14014, Avg. loss: 0.052956\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.95, NNZs: 56, Bias: -0.030000, T: 15092, Avg. loss: 0.076601\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.98, NNZs: 56, Bias: -0.031000, T: 16170, Avg. loss: 0.060780\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.99, NNZs: 56, Bias: -0.030000, T: 17248, Avg. loss: 0.056194\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1.01, NNZs: 56, Bias: -0.033000, T: 18326, Avg. loss: 0.055309\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1.03, NNZs: 56, Bias: -0.035000, T: 19404, Avg. loss: 0.062459\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 18 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.37, NNZs: 54, Bias: -0.005000, T: 1078, Avg. loss: 0.090069\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.47, NNZs: 54, Bias: -0.009000, T: 2156, Avg. loss: 0.055067\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.53, NNZs: 54, Bias: -0.010000, T: 3234, Avg. loss: 0.048178\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.58, NNZs: 54, Bias: -0.012000, T: 4312, Avg. loss: 0.032728\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.63, NNZs: 54, Bias: -0.014000, T: 5390, Avg. loss: 0.043149\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.68, NNZs: 54, Bias: -0.015000, T: 6468, Avg. loss: 0.041463\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.70, NNZs: 54, Bias: -0.017000, T: 7546, Avg. loss: 0.039677\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.72, NNZs: 53, Bias: -0.018000, T: 8624, Avg. loss: 0.038177\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.74, NNZs: 54, Bias: -0.020000, T: 9702, Avg. loss: 0.037797\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 9 epochs took 0.00 seconds\n",
            "[[72.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 67.  0.  3.  1.  1.  3.  0.  9.  2.]\n",
            " [ 0.  0. 72.  0.  0.  0.  1.  0.  0.  0.]\n",
            " [ 0.  1.  0. 75.  0.  0.  0.  0.  1.  0.]\n",
            " [ 0.  1.  0.  0. 60.  0.  0.  0.  1.  0.]\n",
            " [ 2.  0.  0.  2.  0. 67.  0.  0.  2.  1.]\n",
            " [ 0.  0.  0.  0.  0.  0. 74.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1.  0.  0.  0. 57.  1.  1.]\n",
            " [ 0.  1.  0.  6.  1.  0.  0.  1. 52.  3.]\n",
            " [ 0.  1.  0.  1.  0.  2.  0.  2.  2. 68.]]\n",
            "테스트 집합에 대한 정확률은 92.35048678720446%입니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPXXJGTqe81"
      },
      "source": [
        "## 4-3 sklearn 필기 숫자 데이터에 다층 퍼셉트론 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM436_q5qmPr",
        "outputId": "2490a6b9-14de-42e4-8d04-422f399a00d5"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "digit = datasets.load_digits()\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size=0.6)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10))\n",
        "for i in range(len(result)):\n",
        "    conf_mat[result[i]][y_test[i]] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.98921901\n",
            "Iteration 2, loss = 0.26061820\n",
            "Iteration 3, loss = 0.15814594\n",
            "Iteration 4, loss = 0.11745525\n",
            "Iteration 5, loss = 0.09351198\n",
            "Iteration 6, loss = 0.07954411\n",
            "Iteration 7, loss = 0.06554411\n",
            "Iteration 8, loss = 0.06127121\n",
            "Iteration 9, loss = 0.05312082\n",
            "Iteration 10, loss = 0.04593159\n",
            "Iteration 11, loss = 0.04382565\n",
            "Iteration 12, loss = 0.03963568\n",
            "Iteration 13, loss = 0.03698633\n",
            "Iteration 14, loss = 0.03327996\n",
            "Iteration 15, loss = 0.03296182\n",
            "Iteration 16, loss = 0.02922096\n",
            "Iteration 17, loss = 0.02698835\n",
            "Iteration 18, loss = 0.02652831\n",
            "Iteration 19, loss = 0.02444300\n",
            "Iteration 20, loss = 0.02326050\n",
            "Iteration 21, loss = 0.02171045\n",
            "Iteration 22, loss = 0.02047784\n",
            "Iteration 23, loss = 0.02044960\n",
            "Iteration 24, loss = 0.01933997\n",
            "Iteration 25, loss = 0.01828673\n",
            "Iteration 26, loss = 0.01725158\n",
            "Iteration 27, loss = 0.01654484\n",
            "Iteration 28, loss = 0.01644016\n",
            "Iteration 29, loss = 0.01544867\n",
            "Iteration 30, loss = 0.01514796\n",
            "Iteration 31, loss = 0.01448596\n",
            "Iteration 32, loss = 0.01384456\n",
            "Iteration 33, loss = 0.01418009\n",
            "Iteration 34, loss = 0.01303650\n",
            "Iteration 35, loss = 0.01281479\n",
            "Iteration 36, loss = 0.01291020\n",
            "Iteration 37, loss = 0.01192739\n",
            "Iteration 38, loss = 0.01181996\n",
            "Iteration 39, loss = 0.01143425\n",
            "Iteration 40, loss = 0.01105612\n",
            "Iteration 41, loss = 0.01092844\n",
            "Iteration 42, loss = 0.01073983\n",
            "Iteration 43, loss = 0.01040619\n",
            "Iteration 44, loss = 0.01026558\n",
            "Iteration 45, loss = 0.01028823\n",
            "Iteration 46, loss = 0.01006670\n",
            "Iteration 47, loss = 0.00973224\n",
            "Iteration 48, loss = 0.00918280\n",
            "Iteration 49, loss = 0.00891265\n",
            "Iteration 50, loss = 0.00910755\n",
            "Iteration 51, loss = 0.00876629\n",
            "Iteration 52, loss = 0.00851374\n",
            "Iteration 53, loss = 0.00833249\n",
            "Iteration 54, loss = 0.00816889\n",
            "Iteration 55, loss = 0.00806846\n",
            "Iteration 56, loss = 0.00780863\n",
            "Iteration 57, loss = 0.00780913\n",
            "Iteration 58, loss = 0.00760356\n",
            "Iteration 59, loss = 0.00733412\n",
            "Iteration 60, loss = 0.00735447\n",
            "Iteration 61, loss = 0.00720172\n",
            "Iteration 62, loss = 0.00708791\n",
            "Iteration 63, loss = 0.00696629\n",
            "Iteration 64, loss = 0.00679262\n",
            "Iteration 65, loss = 0.00666855\n",
            "Iteration 66, loss = 0.00675191\n",
            "Iteration 67, loss = 0.00650586\n",
            "Iteration 68, loss = 0.00639949\n",
            "Iteration 69, loss = 0.00630906\n",
            "Iteration 70, loss = 0.00621362\n",
            "Iteration 71, loss = 0.00617762\n",
            "Iteration 72, loss = 0.00605847\n",
            "Iteration 73, loss = 0.00598913\n",
            "Iteration 74, loss = 0.00587512\n",
            "Iteration 75, loss = 0.00577370\n",
            "Iteration 76, loss = 0.00577364\n",
            "Iteration 77, loss = 0.00579077\n",
            "Iteration 78, loss = 0.00557215\n",
            "Iteration 79, loss = 0.00546877\n",
            "Iteration 80, loss = 0.00543563\n",
            "Iteration 81, loss = 0.00537869\n",
            "Iteration 82, loss = 0.00535595\n",
            "Iteration 83, loss = 0.00521430\n",
            "Iteration 84, loss = 0.00521711\n",
            "Iteration 85, loss = 0.00512458\n",
            "Iteration 86, loss = 0.00505303\n",
            "Iteration 87, loss = 0.00497062\n",
            "Iteration 88, loss = 0.00496299\n",
            "Iteration 89, loss = 0.00488433\n",
            "Iteration 90, loss = 0.00490764\n",
            "Iteration 91, loss = 0.00473962\n",
            "Iteration 92, loss = 0.00473400\n",
            "Iteration 93, loss = 0.00468149\n",
            "Iteration 94, loss = 0.00463164\n",
            "Iteration 95, loss = 0.00457108\n",
            "Iteration 96, loss = 0.00456021\n",
            "Iteration 97, loss = 0.00449247\n",
            "Iteration 98, loss = 0.00442403\n",
            "Iteration 99, loss = 0.00437448\n",
            "Iteration 100, loss = 0.00433280\n",
            "Iteration 101, loss = 0.00427423\n",
            "Iteration 102, loss = 0.00427666\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[64.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 70.  0.  0.  0.  0.  1.  0.  3.  1.]\n",
            " [ 0.  3. 58.  1.  0.  0.  0.  0.  2.  0.]\n",
            " [ 0.  0.  0. 79.  0.  0.  0.  1.  1.  2.]\n",
            " [ 0.  0.  0.  0. 75.  1.  1.  0.  0.  0.]\n",
            " [ 1.  0.  0.  2.  0. 70.  1.  0.  1.  2.]\n",
            " [ 0.  1.  0.  0.  0.  0. 75.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0. 74.  0.  0.]\n",
            " [ 0.  0.  0.  3.  0.  0.  0.  0. 64.  1.]\n",
            " [ 0.  0.  0.  0.  1.  1.  0.  2.  0. 57.]]\n",
            "테스트 집합에 대한 정확률은 95.41029207232266%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25VtG8YMrpB8"
      },
      "source": [
        "## 4-4 MNIST 데이터셋을 다층 퍼셉트론으로 인식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIaW8ohlrWNS",
        "outputId": "8490084a-633d-4a24-c846-06ec3b37e0a8"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "mnist.data = mnist.data / 255.0 #[0, 255] 범위를 [0, 1] 범위로 변환\n",
        "x_train, x_test = mnist.data[:60000], mnist.data[60000:]\n",
        "y_train, y_test = np.int16(mnist.target[:60000]), np.int16(mnist.target[60000:])\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[result[i]][y_test[i]] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.60374694\n",
            "Iteration 2, loss = 0.26129722\n",
            "Iteration 3, loss = 0.20418650\n",
            "Iteration 4, loss = 0.17070456\n",
            "Iteration 5, loss = 0.14600334\n",
            "Iteration 6, loss = 0.12865914\n",
            "Iteration 7, loss = 0.11451364\n",
            "Iteration 8, loss = 0.10256260\n",
            "Iteration 9, loss = 0.09178685\n",
            "Iteration 10, loss = 0.08406255\n",
            "Iteration 11, loss = 0.07672505\n",
            "Iteration 12, loss = 0.07042061\n",
            "Iteration 13, loss = 0.06544557\n",
            "Iteration 14, loss = 0.05983267\n",
            "Iteration 15, loss = 0.05557075\n",
            "Iteration 16, loss = 0.05144207\n",
            "Iteration 17, loss = 0.04821736\n",
            "Iteration 18, loss = 0.04422411\n",
            "Iteration 19, loss = 0.04113170\n",
            "Iteration 20, loss = 0.03842616\n",
            "Iteration 21, loss = 0.03621384\n",
            "Iteration 22, loss = 0.03383912\n",
            "Iteration 23, loss = 0.03131442\n",
            "Iteration 24, loss = 0.02902793\n",
            "Iteration 25, loss = 0.02695969\n",
            "Iteration 26, loss = 0.02510012\n",
            "Iteration 27, loss = 0.02411855\n",
            "Iteration 28, loss = 0.02214552\n",
            "Iteration 29, loss = 0.02037746\n",
            "Iteration 30, loss = 0.01940980\n",
            "Iteration 31, loss = 0.01754763\n",
            "Iteration 32, loss = 0.01685912\n",
            "Iteration 33, loss = 0.01598683\n",
            "Iteration 34, loss = 0.01420243\n",
            "Iteration 35, loss = 0.01322958\n",
            "Iteration 36, loss = 0.01243371\n",
            "Iteration 37, loss = 0.01175538\n",
            "Iteration 38, loss = 0.01068753\n",
            "Iteration 39, loss = 0.01032615\n",
            "Iteration 40, loss = 0.00952071\n",
            "Iteration 41, loss = 0.00861431\n",
            "Iteration 42, loss = 0.00840033\n",
            "Iteration 43, loss = 0.00767951\n",
            "Iteration 44, loss = 0.00719104\n",
            "Iteration 45, loss = 0.00675433\n",
            "Iteration 46, loss = 0.00622053\n",
            "Iteration 47, loss = 0.00620742\n",
            "Iteration 48, loss = 0.00555275\n",
            "Iteration 49, loss = 0.00496755\n",
            "Iteration 50, loss = 0.00456973\n",
            "Iteration 51, loss = 0.00434137\n",
            "Iteration 52, loss = 0.00448206\n",
            "Iteration 53, loss = 0.00398885\n",
            "Iteration 54, loss = 0.00370788\n",
            "Iteration 55, loss = 0.00382843\n",
            "Iteration 56, loss = 0.00322775\n",
            "Iteration 57, loss = 0.00295924\n",
            "Iteration 58, loss = 0.00275955\n",
            "Iteration 59, loss = 0.00260350\n",
            "Iteration 60, loss = 0.00236157\n",
            "Iteration 61, loss = 0.00229311\n",
            "Iteration 62, loss = 0.00221169\n",
            "Iteration 63, loss = 0.00204018\n",
            "Iteration 64, loss = 0.00189459\n",
            "Iteration 65, loss = 0.00182827\n",
            "Iteration 66, loss = 0.00172890\n",
            "Iteration 67, loss = 0.00169516\n",
            "Iteration 68, loss = 0.00257685\n",
            "Iteration 69, loss = 0.00469750\n",
            "Iteration 70, loss = 0.00517873\n",
            "Iteration 71, loss = 0.00226625\n",
            "Iteration 72, loss = 0.00135477\n",
            "Iteration 73, loss = 0.00120088\n",
            "Iteration 74, loss = 0.00119919\n",
            "Iteration 75, loss = 0.00108304\n",
            "Iteration 76, loss = 0.00102164\n",
            "Iteration 77, loss = 0.00098179\n",
            "Iteration 78, loss = 0.00098884\n",
            "Iteration 79, loss = 0.00093147\n",
            "Iteration 80, loss = 0.00088609\n",
            "Iteration 81, loss = 0.00085330\n",
            "Iteration 82, loss = 0.00085909\n",
            "Iteration 83, loss = 0.00083303\n",
            "Iteration 84, loss = 0.00078366\n",
            "Iteration 85, loss = 0.00075740\n",
            "Iteration 86, loss = 0.00074593\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 967    0    4    0    0    2    5    1    6    2]\n",
            " [   0 1125    1    0    1    0    2    2    1    2]\n",
            " [   1    3 1000    4    2    0    4    7    4    0]\n",
            " [   1    1    5  990    1   10    1    4    7    4]\n",
            " [   1    0    3    1  962    1    5    0    5    6]\n",
            " [   1    1    1    3    2  870    5    0    3    5]\n",
            " [   4    2    4    0    5    2  933    0    1    0]\n",
            " [   1    1    7    1    2    1    0 1002    4    1]\n",
            " [   3    2    7    3    1    4    3    4  941    1]\n",
            " [   1    0    0    8    6    2    0    8    2  988]]\n",
            "테스트 집합에 대한 정확률은 97.78%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv2CCb7St5qn"
      },
      "source": [
        "## 4-5 validation_curve 함수로 최적의 은닉 노드 개수 찾기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "iQVgbCRnr4F4",
        "outputId": "3da359df-3fa4-4196-eb7f-5cf69737fa79"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split, validation_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "digit = datasets.load_digits()\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size=0.6)\n",
        "\n",
        "start = time.time()\n",
        "mlp = MLPClassifier(learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd')\n",
        "param_range = range(50, 1001, 50)\n",
        "train_score, test_score = validation_curve(mlp, x_train, y_train, param_name='hidden_layer_sizes', param_range=param_range, cv=10, scoring='accuracy', n_jobs=4)\n",
        "end = time.time()\n",
        "print('하이퍼 파라미터 최적화에 걸린 시간은 {}초입니다.'.format(end-start))\n",
        "\n",
        "# 교차 검증 결과의 평균과 분산 구하기\n",
        "train_mean = np.mean(train_score, axis=1)\n",
        "train_std = np.std(train_score, axis=1)\n",
        "test_mean = np.mean(test_score, axis=1)\n",
        "test_std = np.std(test_score, axis=1)\n",
        "\n",
        "# 성능 그래프\n",
        "plt.plot(param_range, train_mean, label='Train score', color='r')\n",
        "plt.plot(param_range, test_mean, label='Test score', color='b')\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='r')\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color='b')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Validation Curve with MLP')\n",
        "plt.xlabel('Number of hidden nodes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.9, 1.01)\n",
        "plt.grid(axis='both')\n",
        "plt.show()\n",
        "\n",
        "best_number_nodes = param_range[np.argmax(test_mean)]\n",
        "print('\\n최적의 은닉층의 노드 개수는 {}개 입니다.'.format(best_number_nodes))\n",
        "\n",
        "mlp_test = MLPClassifier(hidden_layer_sizes=(best_number_nodes), learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd')\n",
        "mlp_test.fit(x_train, y_train)\n",
        "\n",
        "result = mlp_test.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10))\n",
        "for i in range(len(result)):\n",
        "    conf_mat[result[i]][y_test[i]] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하이퍼 파라미터 최적화에 걸린 시간은 263.7117328643799초입니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "최적의 은닉층의 노드 개수는 800개 입니다.\n",
            "[[63.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 66.  0.  0.  0.  0.  2.  0.  4.  0.]\n",
            " [ 0.  0. 78.  0.  0.  1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. 63.  0.  0.  0.  0.  0.  2.]\n",
            " [ 0.  0.  0.  0. 61.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. 77.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.  0. 74.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  1.  0. 75.  0.  0.]\n",
            " [ 0.  0.  0.  5.  0.  0.  0.  0. 69.  1.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. 76.]]\n",
            "테스트 집합에 대한 정확률은 97.63560500695411입니다.\n"
          ]
        }
      ]
    }
  ]
}