{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMTzrTXVIU4Q8gR40jQXhgM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hG4OcebGtyr"
      },
      "source": [
        "## 1번\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqKt7EEnIVB-"
      },
      "source": [
        "### 은닉층의 노드 개수 100개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIxiybqGmoH",
        "outputId": "8f442e2b-b649-48b5-af75-30bf9f0d9155"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "mnist.data = mnist.data / 255.0 #[0, 255] 범위를 [0, 1] 범위로 변환\n",
        "x_train, x_test = mnist.data[:60000], mnist.data[60000:]\n",
        "y_train, y_test = np.int16(mnist.target[:60000]), np.int16(mnist.target[60000:])\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[result[i]][y_test[i]] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.60421530\n",
            "Iteration 2, loss = 0.27367034\n",
            "Iteration 3, loss = 0.21894166\n",
            "Iteration 4, loss = 0.18402242\n",
            "Iteration 5, loss = 0.15824814\n",
            "Iteration 6, loss = 0.13955520\n",
            "Iteration 7, loss = 0.12416375\n",
            "Iteration 8, loss = 0.11163046\n",
            "Iteration 9, loss = 0.10171971\n",
            "Iteration 10, loss = 0.09197490\n",
            "Iteration 11, loss = 0.08423444\n",
            "Iteration 12, loss = 0.07758165\n",
            "Iteration 13, loss = 0.07103680\n",
            "Iteration 14, loss = 0.06560256\n",
            "Iteration 15, loss = 0.06002018\n",
            "Iteration 16, loss = 0.05552313\n",
            "Iteration 17, loss = 0.05184376\n",
            "Iteration 18, loss = 0.04893511\n",
            "Iteration 19, loss = 0.04511114\n",
            "Iteration 20, loss = 0.04145788\n",
            "Iteration 21, loss = 0.03882973\n",
            "Iteration 22, loss = 0.03733959\n",
            "Iteration 23, loss = 0.03383513\n",
            "Iteration 24, loss = 0.03148437\n",
            "Iteration 25, loss = 0.02940237\n",
            "Iteration 26, loss = 0.02708787\n",
            "Iteration 27, loss = 0.02527714\n",
            "Iteration 28, loss = 0.02373051\n",
            "Iteration 29, loss = 0.02204970\n",
            "Iteration 30, loss = 0.02024081\n",
            "Iteration 31, loss = 0.01915557\n",
            "Iteration 32, loss = 0.01784568\n",
            "Iteration 33, loss = 0.01613192\n",
            "Iteration 34, loss = 0.01557331\n",
            "Iteration 35, loss = 0.01424438\n",
            "Iteration 36, loss = 0.01358366\n",
            "Iteration 37, loss = 0.01281064\n",
            "Iteration 38, loss = 0.01171052\n",
            "Iteration 39, loss = 0.01082333\n",
            "Iteration 40, loss = 0.01033148\n",
            "Iteration 41, loss = 0.00938019\n",
            "Iteration 42, loss = 0.00892700\n",
            "Iteration 43, loss = 0.00813124\n",
            "Iteration 44, loss = 0.00793233\n",
            "Iteration 45, loss = 0.00717212\n",
            "Iteration 46, loss = 0.00659767\n",
            "Iteration 47, loss = 0.00624694\n",
            "Iteration 48, loss = 0.00569020\n",
            "Iteration 49, loss = 0.00529177\n",
            "Iteration 50, loss = 0.00483068\n",
            "Iteration 51, loss = 0.00458184\n",
            "Iteration 52, loss = 0.00446906\n",
            "Iteration 53, loss = 0.00417866\n",
            "Iteration 54, loss = 0.00411072\n",
            "Iteration 55, loss = 0.00345356\n",
            "Iteration 56, loss = 0.00359476\n",
            "Iteration 57, loss = 0.00300221\n",
            "Iteration 58, loss = 0.00280455\n",
            "Iteration 59, loss = 0.00268644\n",
            "Iteration 60, loss = 0.00251982\n",
            "Iteration 61, loss = 0.00245936\n",
            "Iteration 62, loss = 0.00227546\n",
            "Iteration 63, loss = 0.00208940\n",
            "Iteration 64, loss = 0.00198511\n",
            "Iteration 65, loss = 0.00183311\n",
            "Iteration 66, loss = 0.00179470\n",
            "Iteration 67, loss = 0.00167010\n",
            "Iteration 68, loss = 0.00152506\n",
            "Iteration 69, loss = 0.00193955\n",
            "Iteration 70, loss = 0.00185602\n",
            "Iteration 71, loss = 0.00141447\n",
            "Iteration 72, loss = 0.00152333\n",
            "Iteration 73, loss = 0.00130792\n",
            "Iteration 74, loss = 0.00117432\n",
            "Iteration 75, loss = 0.00108492\n",
            "Iteration 76, loss = 0.00105222\n",
            "Iteration 77, loss = 0.00099295\n",
            "Iteration 78, loss = 0.00092451\n",
            "Iteration 79, loss = 0.00088123\n",
            "Iteration 80, loss = 0.00088247\n",
            "Iteration 81, loss = 0.00082358\n",
            "Iteration 82, loss = 0.00079743\n",
            "Iteration 83, loss = 0.00078253\n",
            "Iteration 84, loss = 0.00073778\n",
            "Iteration 85, loss = 0.00072480\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 967    0    5    0    3    2    4    2    5    2]\n",
            " [   0 1123    2    1    0    0    1    3    0    5]\n",
            " [   1    3 1008    4    2    0    4    6    1    0]\n",
            " [   1    1    3  983    0   10    1    4    4    5]\n",
            " [   1    0    3    0  960    0    4    3    4    7]\n",
            " [   2    1    0    7    0  871    5    0    2    3]\n",
            " [   4    3    2    0    3    2  937    0    2    0]\n",
            " [   1    1    6    4    4    2    0 1001    3    6]\n",
            " [   2    3    2    8    1    3    2    5  949    7]\n",
            " [   1    0    1    3    9    2    0    4    4  974]]\n",
            "테스트 집합에 대한 정확률은 97.72999999999999%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-EhVoKNIXjP"
      },
      "source": [
        "### 은닉층의 노드 개수 200개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd3BmCLAHDWE",
        "outputId": "5afc17a3-890b-4b38-e409-f338ba6cfe52"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(200), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[result[i]][y_test[i]] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.52116091\n",
            "Iteration 2, loss = 0.22109073\n",
            "Iteration 3, loss = 0.16675561\n",
            "Iteration 4, loss = 0.13234213\n",
            "Iteration 5, loss = 0.10933705\n",
            "Iteration 6, loss = 0.09281842\n",
            "Iteration 7, loss = 0.08083899\n",
            "Iteration 8, loss = 0.07067541\n",
            "Iteration 9, loss = 0.06213149\n",
            "Iteration 10, loss = 0.05488509\n",
            "Iteration 11, loss = 0.04895860\n",
            "Iteration 12, loss = 0.04340864\n",
            "Iteration 13, loss = 0.03844830\n",
            "Iteration 14, loss = 0.03417420\n",
            "Iteration 15, loss = 0.03144059\n",
            "Iteration 16, loss = 0.02809630\n",
            "Iteration 17, loss = 0.02552330\n",
            "Iteration 18, loss = 0.02271122\n",
            "Iteration 19, loss = 0.02015577\n",
            "Iteration 20, loss = 0.01829449\n",
            "Iteration 21, loss = 0.01596853\n",
            "Iteration 22, loss = 0.01497614\n",
            "Iteration 23, loss = 0.01334436\n",
            "Iteration 24, loss = 0.01182337\n",
            "Iteration 25, loss = 0.01050355\n",
            "Iteration 26, loss = 0.00968008\n",
            "Iteration 27, loss = 0.00883762\n",
            "Iteration 28, loss = 0.00798617\n",
            "Iteration 29, loss = 0.00723905\n",
            "Iteration 30, loss = 0.00614784\n",
            "Iteration 31, loss = 0.00584361\n",
            "Iteration 32, loss = 0.00520588\n",
            "Iteration 33, loss = 0.00482041\n",
            "Iteration 34, loss = 0.00427493\n",
            "Iteration 35, loss = 0.00413224\n",
            "Iteration 36, loss = 0.00355463\n",
            "Iteration 37, loss = 0.00327980\n",
            "Iteration 38, loss = 0.00317958\n",
            "Iteration 39, loss = 0.00278323\n",
            "Iteration 40, loss = 0.00257660\n",
            "Iteration 41, loss = 0.00230367\n",
            "Iteration 42, loss = 0.00225887\n",
            "Iteration 43, loss = 0.00201064\n",
            "Iteration 44, loss = 0.00181876\n",
            "Iteration 45, loss = 0.00179028\n",
            "Iteration 46, loss = 0.00161955\n",
            "Iteration 47, loss = 0.00156274\n",
            "Iteration 48, loss = 0.00143901\n",
            "Iteration 49, loss = 0.00138655\n",
            "Iteration 50, loss = 0.00123286\n",
            "Iteration 51, loss = 0.00116677\n",
            "Iteration 52, loss = 0.00107999\n",
            "Iteration 53, loss = 0.00102610\n",
            "Iteration 54, loss = 0.00097950\n",
            "Iteration 55, loss = 0.00090647\n",
            "Iteration 56, loss = 0.00085998\n",
            "Iteration 57, loss = 0.00080900\n",
            "Iteration 58, loss = 0.00076258\n",
            "Iteration 59, loss = 0.00076840\n",
            "Iteration 60, loss = 0.00070315\n",
            "Iteration 61, loss = 0.00067583\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 971    0    1    1    0    1    5    0    4    2]\n",
            " [   1 1125    2    0    1    1    2    4    0    2]\n",
            " [   1    2 1007    5    2    0    4    9    1    0]\n",
            " [   0    1    1  993    1    9    1    1    7    1]\n",
            " [   0    0    2    0  963    1    4    1    5    6]\n",
            " [   0    2    1    2    0  867    5    0    4    2]\n",
            " [   3    2    3    1    5    4  935    0    1    1]\n",
            " [   1    1    7    3    2    0    0 1005    4    5]\n",
            " [   2    2    7    3    0    5    2    2  945    3]\n",
            " [   1    0    1    2    8    4    0    6    3  987]]\n",
            "테스트 집합에 대한 정확률은 97.98%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAl_9RrKG5ta"
      },
      "source": [
        "## 2번"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "madZs73fGzfo",
        "outputId": "ecf181be-3295-4ba1-ee2b-f42cc1b0b8be"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split, validation_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "mnist.data = mnist.data / 255.0 #[0, 255] 범위를 [0, 1] 범위로 변환\n",
        "x_train, x_test, y_train, y_test = train_test_split(mnist.data, mnist.target, train_size=0.6)\n",
        "\n",
        "start = time.time()\n",
        "mlp = MLPClassifier(learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd')\n",
        "param_range = range(50, 101, 50)\n",
        "train_score, test_score = validation_curve(mlp, x_train, y_train, param_name='hidden_layer_sizes', param_range=param_range, cv=5, scoring='accuracy', n_jobs=4, verbose=True)\n",
        "end = time.time()\n",
        "print('하이퍼 파라미터 최적화에 걸린 시간은 {}초입니다.'.format(end-start))\n",
        "\n",
        "# 교차 검증 결과의 평균과 분산 구하기\n",
        "train_mean = np.mean(train_score, axis=1)\n",
        "train_std = np.std(train_score, axis=1)\n",
        "test_mean = np.mean(test_score, axis=1)\n",
        "test_std = np.std(test_score, axis=1)\n",
        "\n",
        "# 성능 그래프\n",
        "plt.plot(param_range, train_mean, label='Train score', color='r')\n",
        "plt.plot(param_range, test_mean, label='Test score', color='b')\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='r')\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color='b')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Validation Curve with MLP')\n",
        "plt.xlabel('Number of hidden nodes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.9, 1.01)\n",
        "plt.grid(axis='both')\n",
        "plt.show()\n",
        "\n",
        "best_number_nodes = param_range[np.argmax(test_mean)]\n",
        "print('\\n최적의 은닉층의 노드 개수는 {}개 입니다.'.format(best_number_nodes))\n",
        "\n",
        "mlp_test = MLPClassifier(hidden_layer_sizes=(best_number_nodes), learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd')\n",
        "mlp_test.fit(x_train, y_train)\n",
        "\n",
        "result = mlp_test.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10))\n",
        "for i in range(len(result)):\n",
        "    conf_mat[int(result[i])][int(y_test[i])] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 20.7min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하이퍼 파라미터 최적화에 걸린 시간은 1243.7220222949982초입니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "최적의 은닉층의 노드 개수는 100개 입니다.\n",
            "[[2.721e+03 1.000e+00 1.000e+01 3.000e+00 1.000e+00 9.000e+00 8.000e+00\n",
            "  6.000e+00 8.000e+00 1.000e+01]\n",
            " [0.000e+00 3.111e+03 1.600e+01 2.000e+00 3.000e+00 1.000e+00 5.000e+00\n",
            "  8.000e+00 1.500e+01 3.000e+00]\n",
            " [6.000e+00 3.000e+00 2.770e+03 2.900e+01 9.000e+00 1.000e+00 2.000e+00\n",
            "  1.500e+01 8.000e+00 2.000e+00]\n",
            " [1.000e+00 2.000e+00 1.400e+01 2.763e+03 0.000e+00 2.500e+01 1.000e+00\n",
            "  6.000e+00 1.600e+01 1.200e+01]\n",
            " [5.000e+00 0.000e+00 7.000e+00 0.000e+00 2.720e+03 1.000e+00 6.000e+00\n",
            "  6.000e+00 5.000e+00 3.200e+01]\n",
            " [4.000e+00 1.000e+00 2.000e+00 2.500e+01 1.000e+00 2.394e+03 8.000e+00\n",
            "  3.000e+00 8.000e+00 1.000e+01]\n",
            " [8.000e+00 2.000e+00 4.000e+00 2.000e+00 6.000e+00 1.100e+01 2.706e+03\n",
            "  0.000e+00 7.000e+00 3.000e+00]\n",
            " [2.000e+00 4.000e+00 1.600e+01 8.000e+00 6.000e+00 3.000e+00 0.000e+00\n",
            "  2.867e+03 4.000e+00 1.500e+01]\n",
            " [1.200e+01 9.000e+00 1.500e+01 2.600e+01 5.000e+00 1.500e+01 8.000e+00\n",
            "  4.000e+00 2.634e+03 1.000e+01]\n",
            " [7.000e+00 1.000e+00 3.000e+00 1.400e+01 4.000e+01 9.000e+00 0.000e+00\n",
            "  2.000e+01 1.200e+01 2.618e+03]]\n",
            "테스트 집합에 대한 정확률은 97.5142857142857입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ6MqxY9Hn_e"
      },
      "source": [
        "## 3번"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY5LCs22IbF2"
      },
      "source": [
        "### 은닉층 2개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7BdkK6GHpH8",
        "outputId": "e990bc24-205e-4fe8-beaf-7266aecfc33b"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "mnist.data = mnist.data / 255.0 #[0, 255] 범위를 [0, 1] 범위로 변환\n",
        "x_train, x_test, y_train, y_test = train_test_split(mnist.data, mnist.target, train_size=0.6)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[int(result[i])][int(y_test[i])] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.68977625\n",
            "Iteration 2, loss = 0.25219056\n",
            "Iteration 3, loss = 0.18974491\n",
            "Iteration 4, loss = 0.15266031\n",
            "Iteration 5, loss = 0.13058231\n",
            "Iteration 6, loss = 0.11150246\n",
            "Iteration 7, loss = 0.09718902\n",
            "Iteration 8, loss = 0.08928608\n",
            "Iteration 9, loss = 0.07828672\n",
            "Iteration 10, loss = 0.06688415\n",
            "Iteration 11, loss = 0.06486473\n",
            "Iteration 12, loss = 0.05437522\n",
            "Iteration 13, loss = 0.04956037\n",
            "Iteration 14, loss = 0.05618834\n",
            "Iteration 15, loss = 0.03880291\n",
            "Iteration 16, loss = 0.03611484\n",
            "Iteration 17, loss = 0.03064203\n",
            "Iteration 18, loss = 0.03131737\n",
            "Iteration 19, loss = 0.02955295\n",
            "Iteration 20, loss = 0.02419897\n",
            "Iteration 21, loss = 0.02237004\n",
            "Iteration 22, loss = 0.01899997\n",
            "Iteration 23, loss = 0.01677128\n",
            "Iteration 24, loss = 0.01516832\n",
            "Iteration 25, loss = 0.01399239\n",
            "Iteration 26, loss = 0.01263010\n",
            "Iteration 27, loss = 0.01051410\n",
            "Iteration 28, loss = 0.01060301\n",
            "Iteration 29, loss = 0.00872318\n",
            "Iteration 30, loss = 0.00750307\n",
            "Iteration 31, loss = 0.00686082\n",
            "Iteration 32, loss = 0.00605150\n",
            "Iteration 33, loss = 0.00715992\n",
            "Iteration 34, loss = 0.00589843\n",
            "Iteration 35, loss = 0.03050844\n",
            "Iteration 36, loss = 0.00901279\n",
            "Iteration 37, loss = 0.00539135\n",
            "Iteration 38, loss = 0.00401495\n",
            "Iteration 39, loss = 0.00361220\n",
            "Iteration 40, loss = 0.00269452\n",
            "Iteration 41, loss = 0.00861595\n",
            "Iteration 42, loss = 0.00275794\n",
            "Iteration 43, loss = 0.00222709\n",
            "Iteration 44, loss = 0.00190277\n",
            "Iteration 45, loss = 0.00165044\n",
            "Iteration 46, loss = 0.00152353\n",
            "Iteration 47, loss = 0.00158811\n",
            "Iteration 48, loss = 0.00136471\n",
            "Iteration 49, loss = 0.00122231\n",
            "Iteration 50, loss = 0.00114360\n",
            "Iteration 51, loss = 0.00108277\n",
            "Iteration 52, loss = 0.00103969\n",
            "Iteration 53, loss = 0.00095513\n",
            "Iteration 54, loss = 0.00818159\n",
            "Iteration 55, loss = 0.00960968\n",
            "Iteration 56, loss = 0.02412147\n",
            "Iteration 57, loss = 0.00614908\n",
            "Iteration 58, loss = 0.00162317\n",
            "Iteration 59, loss = 0.00099091\n",
            "Iteration 60, loss = 0.00082720\n",
            "Iteration 61, loss = 0.00077591\n",
            "Iteration 62, loss = 0.00077682\n",
            "Iteration 63, loss = 0.00064045\n",
            "Iteration 64, loss = 0.00064753\n",
            "Iteration 65, loss = 0.00057050\n",
            "Iteration 66, loss = 0.00053384\n",
            "Iteration 67, loss = 0.00051039\n",
            "Iteration 68, loss = 0.00049093\n",
            "Iteration 69, loss = 0.00047127\n",
            "Iteration 70, loss = 0.00045372\n",
            "Iteration 71, loss = 0.00043765\n",
            "Iteration 72, loss = 0.00042082\n",
            "Iteration 73, loss = 0.00040779\n",
            "Iteration 74, loss = 0.00039107\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[2756    0    9    5    0    7    2    1   10    8]\n",
            " [   0 3030    6    4    4    3    4    6   11    4]\n",
            " [   3   13 2767   24    5    3    3   13   11    9]\n",
            " [   1    5   14 2750    1   29    0    6   25   11]\n",
            " [   2    7    7    3 2636    2    7   10    8   27]\n",
            " [   5    0    1    9    0 2464   19    4   15    7]\n",
            " [   9    6    5    1   10   10 2648    0    7    1]\n",
            " [   2    8   14   13    7    8    2 2876    5   30]\n",
            " [   7   13   12   15    9   10    7    3 2652   14]\n",
            " [   4    0    6   13   37   16    0   16   14 2694]]\n",
            "테스트 집합에 대한 정확률은 97.40357142857144%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT6rUbOzIibh"
      },
      "source": [
        "### 은닉층 3개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjAr3Gw3IhdR",
        "outputId": "cd2275db-4ccc-4833-e88c-9700c2922564"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[int(result[i])][int(y_test[i])] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.68489161\n",
            "Iteration 2, loss = 0.22285378\n",
            "Iteration 3, loss = 0.16517272\n",
            "Iteration 4, loss = 0.13293558\n",
            "Iteration 5, loss = 0.10767057\n",
            "Iteration 6, loss = 0.08896766\n",
            "Iteration 7, loss = 0.08319695\n",
            "Iteration 8, loss = 0.06525094\n",
            "Iteration 9, loss = 0.05433188\n",
            "Iteration 10, loss = 0.04718563\n",
            "Iteration 11, loss = 0.04215895\n",
            "Iteration 12, loss = 0.03638494\n",
            "Iteration 13, loss = 0.03213814\n",
            "Iteration 14, loss = 0.02777631\n",
            "Iteration 15, loss = 0.02246911\n",
            "Iteration 16, loss = 0.02494910\n",
            "Iteration 17, loss = 0.01875505\n",
            "Iteration 18, loss = 0.04364960\n",
            "Iteration 19, loss = 0.01481164\n",
            "Iteration 20, loss = 0.01140767\n",
            "Iteration 21, loss = 0.00876069\n",
            "Iteration 22, loss = 0.00975883\n",
            "Iteration 23, loss = 0.00650464\n",
            "Iteration 24, loss = 0.00500788\n",
            "Iteration 25, loss = 0.00424166\n",
            "Iteration 26, loss = 0.00352863\n",
            "Iteration 27, loss = 0.00306335\n",
            "Iteration 28, loss = 0.00232680\n",
            "Iteration 29, loss = 0.00218832\n",
            "Iteration 30, loss = 0.00186461\n",
            "Iteration 31, loss = 0.00156158\n",
            "Iteration 32, loss = 0.00134567\n",
            "Iteration 33, loss = 0.00123834\n",
            "Iteration 34, loss = 0.00113372\n",
            "Iteration 35, loss = 0.00091808\n",
            "Iteration 36, loss = 0.00080248\n",
            "Iteration 37, loss = 0.00071022\n",
            "Iteration 38, loss = 0.00066092\n",
            "Iteration 39, loss = 0.00061244\n",
            "Iteration 40, loss = 0.00056554\n",
            "Iteration 41, loss = 0.00051138\n",
            "Iteration 42, loss = 0.00046329\n",
            "Iteration 43, loss = 0.00044724\n",
            "Iteration 44, loss = 0.00041255\n",
            "Iteration 45, loss = 0.00038894\n",
            "Iteration 46, loss = 0.00036844\n",
            "Iteration 47, loss = 0.00040069\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[2750    0   12    4    1    5   11    4   13    9]\n",
            " [   0 3036    3    5    3    4    3    6   12    3]\n",
            " [   4   12 2765   23    7    3    5   16   15    5]\n",
            " [   1    6   15 2751    0   36    0    8   26   13]\n",
            " [   2    5    9    1 2642    5    9   15    1   28]\n",
            " [   7    1    3   13    0 2449   14    4   14    4]\n",
            " [  12    8    4    0   11   15 2644    1    6    2]\n",
            " [   3    7   16   12    5    5    0 2866    5   27]\n",
            " [   8    5   12   18    4   10    6    2 2648   13]\n",
            " [   2    2    2   10   36   20    0   13   18 2701]]\n",
            "테스트 집합에 대한 정확률은 97.32857142857144%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTbLm5MAIgVp"
      },
      "source": [
        "### 은닉층 4개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxXdeoFbI7OK",
        "outputId": "a3b0b018-afc7-4048-a6b1-054cc60a1bd6"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[int(result[i])][int(y_test[i])] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.73431641\n",
            "Iteration 2, loss = 0.22581289\n",
            "Iteration 3, loss = 0.16343664\n",
            "Iteration 4, loss = 0.12585086\n",
            "Iteration 5, loss = 0.12097503\n",
            "Iteration 6, loss = 0.08764070\n",
            "Iteration 7, loss = 0.07868410\n",
            "Iteration 8, loss = 0.06972919\n",
            "Iteration 9, loss = 0.05225680\n",
            "Iteration 10, loss = 0.04542185\n",
            "Iteration 11, loss = 0.04423209\n",
            "Iteration 12, loss = 0.04036323\n",
            "Iteration 13, loss = 0.02888854\n",
            "Iteration 14, loss = 0.02484424\n",
            "Iteration 15, loss = 0.03123607\n",
            "Iteration 16, loss = 0.02347579\n",
            "Iteration 17, loss = 0.01447457\n",
            "Iteration 18, loss = 0.01438332\n",
            "Iteration 19, loss = 0.01318404\n",
            "Iteration 20, loss = 0.00928925\n",
            "Iteration 21, loss = 0.02883050\n",
            "Iteration 22, loss = 0.00941008\n",
            "Iteration 23, loss = 0.00736014\n",
            "Iteration 24, loss = 0.00514808\n",
            "Iteration 25, loss = 0.00359639\n",
            "Iteration 26, loss = 0.00331465\n",
            "Iteration 27, loss = 0.01269035\n",
            "Iteration 28, loss = 0.00933239\n",
            "Iteration 29, loss = 0.00516340\n",
            "Iteration 30, loss = 0.00340176\n",
            "Iteration 31, loss = 0.00138864\n",
            "Iteration 32, loss = 0.00080495\n",
            "Iteration 33, loss = 0.00055334\n",
            "Iteration 34, loss = 0.00047515\n",
            "Iteration 35, loss = 0.00042350\n",
            "Iteration 36, loss = 0.00040033\n",
            "Iteration 37, loss = 0.00035771\n",
            "Iteration 38, loss = 0.00044210\n",
            "Iteration 39, loss = 0.00032127\n",
            "Iteration 40, loss = 0.00029295\n",
            "Iteration 41, loss = 0.00027449\n",
            "Iteration 42, loss = 0.00025892\n",
            "Iteration 43, loss = 0.00024477\n",
            "Iteration 44, loss = 0.00023571\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[2755    0    6    2    1    7    6    2   10    6]\n",
            " [   0 3041    6    4    2    6    3    6    9    6]\n",
            " [   4   12 2777   26    6    3    3   12   14    4]\n",
            " [   0    5   11 2748    0   28    0    3   26   11]\n",
            " [   5    4    6    2 2649    6    6    9    1   30]\n",
            " [   4    2    2   19    0 2451   12    2   10    6]\n",
            " [  12    6    2    1   10   19 2654    0   10    2]\n",
            " [   0   10   14   11    6    4    2 2886    7   28]\n",
            " [   7    2   11   17    7   12    6    4 2657   15]\n",
            " [   2    0    6    7   28   16    0   11   14 2697]]\n",
            "테스트 집합에 대한 정확률은 97.55357142857143%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Ddgf9hI64e"
      },
      "source": [
        "### 은닉층 5개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISlQJ5wmJBVI",
        "outputId": "8aaa35c9-9ee2-4760-c340-495f8e0037c3"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "result = mlp.predict(x_test)\n",
        "\n",
        "conf_mat = np.zeros((10, 10), dtype=np.int16)\n",
        "for i in range(len(result)):\n",
        "    conf_mat[int(result[i])][int(y_test[i])] += 1\n",
        "print(conf_mat)\n",
        "\n",
        "num_correct = 0\n",
        "for i in range(10):\n",
        "    num_correct += conf_mat[i][i]\n",
        "accuracy = num_correct / len(result)\n",
        "print('테스트 집합에 대한 정확률은 {}%입니다.'.format(accuracy * 100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.76516677\n",
            "Iteration 2, loss = 0.21465562\n",
            "Iteration 3, loss = 0.14999890\n",
            "Iteration 4, loss = 0.11372629\n",
            "Iteration 5, loss = 0.09616105\n",
            "Iteration 6, loss = 0.08200891\n",
            "Iteration 7, loss = 0.07099499\n",
            "Iteration 8, loss = 0.05481933\n",
            "Iteration 9, loss = 0.04558493\n",
            "Iteration 10, loss = 0.05907601\n",
            "Iteration 11, loss = 0.03416715\n",
            "Iteration 12, loss = 0.03121248\n",
            "Iteration 13, loss = 0.02274166\n",
            "Iteration 14, loss = 0.02609881\n",
            "Iteration 15, loss = 0.01703338\n",
            "Iteration 16, loss = 0.03467898\n",
            "Iteration 17, loss = 0.01438574\n",
            "Iteration 18, loss = 0.01261304\n",
            "Iteration 19, loss = 0.02488887\n",
            "Iteration 20, loss = 0.00879081\n",
            "Iteration 21, loss = 0.00559081\n",
            "Iteration 22, loss = 0.00613221\n",
            "Iteration 23, loss = 0.00590958\n",
            "Iteration 24, loss = 0.01125894\n",
            "Iteration 25, loss = 0.01783293\n",
            "Iteration 26, loss = 0.01007050\n",
            "Iteration 27, loss = 0.00785342\n",
            "Iteration 28, loss = 0.00237625\n",
            "Iteration 29, loss = 0.00177521\n",
            "Iteration 30, loss = 0.00413637\n",
            "Iteration 31, loss = 0.00793974\n",
            "Iteration 32, loss = 0.00792290\n",
            "Iteration 33, loss = 0.01290862\n",
            "Iteration 34, loss = 0.00941113\n",
            "Iteration 35, loss = 0.00461038\n",
            "Iteration 36, loss = 0.01255861\n",
            "Iteration 37, loss = 0.00928160\n",
            "Iteration 38, loss = 0.00312283\n",
            "Iteration 39, loss = 0.00085053\n",
            "Iteration 40, loss = 0.00036992\n",
            "Iteration 41, loss = 0.00024251\n",
            "Iteration 42, loss = 0.00020990\n",
            "Iteration 43, loss = 0.00019504\n",
            "Iteration 44, loss = 0.00017822\n",
            "Iteration 45, loss = 0.00016882\n",
            "Iteration 46, loss = 0.00016108\n",
            "Iteration 47, loss = 0.00015456\n",
            "Iteration 48, loss = 0.00014835\n",
            "Iteration 49, loss = 0.00014328\n",
            "Iteration 50, loss = 0.00013892\n",
            "Iteration 51, loss = 0.00013553\n",
            "Iteration 52, loss = 0.00013259\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[2766    0    6    6    2    5   13    2   12    9]\n",
            " [   0 3036    3    6    4    2    2    5   12    3]\n",
            " [   5   15 2782   28    6    4    1   14   16    8]\n",
            " [   0    8   10 2745    1   26    0    6   13    9]\n",
            " [   2    4    4    2 2652    2    4    6    5   29]\n",
            " [   3    1    1   21    0 2461   11    1   13    8]\n",
            " [   7    8    7    0    6   17 2655    0    7    3]\n",
            " [   1    5   17    7    5    4    0 2880    7   28]\n",
            " [   4    4    8   12    3   12    6    9 2661    6]\n",
            " [   1    1    3   10   30   19    0   12   12 2702]]\n",
            "테스트 집합에 대한 정확률은 97.64285714285714%입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9FnsEtEJD2y"
      },
      "source": [
        "## 4번"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLqpdlwOJE2x",
        "outputId": "2732b4bb-f164-4ff6-8916-42f772fa516a"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# 훈련 집합 구축\n",
        "X = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
        "     [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]\n",
        "y = [-1, 1, -1, 1, -1, -1, -1, -1]\n",
        "\n",
        "# fit 함수로 Perceptron 학습\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "print('학습된 퍼셉트론의 매개변수:', perceptron.coef_, perceptron.intercept_)\n",
        "print('훈련집합에 대한 예측:', perceptron.predict(X))\n",
        "print('정확률 측정: {}%'.format(perceptron.score(X, y) * 100))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습된 퍼셉트론의 매개변수: [[-3.  0.  3.]] [-1.]\n",
            "훈련집합에 대한 예측: [-1  1 -1  1 -1 -1 -1 -1]\n",
            "정확률 측정: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_PWP6leOBgX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}