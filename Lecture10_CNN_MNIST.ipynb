{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture10_CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s0afLGkF8lB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGhsIFDkGSP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf9a3d3-7fdf-4017-db09-442681105dac"
      },
      "source": [
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.032089\n",
            "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.034909\n",
            "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.009963\n",
            "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.114862\n",
            "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.009309\n",
            "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.074461\n",
            "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.131663\n",
            "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.030535\n",
            "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.039867\n",
            "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.056245\n",
            "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.127311\n",
            "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.086779\n",
            "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.035014\n",
            "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.055682\n",
            "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.073267\n",
            "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.068587\n",
            "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.083520\n",
            "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.052983\n",
            "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.009309\n",
            "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.173930\n",
            "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.051045\n",
            "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.049232\n",
            "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.020813\n",
            "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.013994\n",
            "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.120081\n",
            "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.011927\n",
            "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.022983\n",
            "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.033390\n",
            "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.024586\n",
            "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.106782\n",
            "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.056101\n",
            "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.096201\n",
            "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.069782\n",
            "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.025176\n",
            "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.049865\n",
            "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.045665\n",
            "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.074568\n",
            "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.020619\n",
            "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.162914\n",
            "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.143763\n",
            "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.010994\n",
            "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.107749\n",
            "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.013552\n",
            "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.082115\n",
            "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.062765\n",
            "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.043583\n",
            "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.119764\n",
            "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.034726\n",
            "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.044307\n",
            "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.067869\n",
            "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.050867\n",
            "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.040410\n",
            "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.062908\n",
            "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.019699\n",
            "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.019869\n",
            "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.035919\n",
            "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.029022\n",
            "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.070872\n",
            "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.053534\n",
            "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.175938\n",
            "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.044358\n",
            "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.125777\n",
            "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.027171\n",
            "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.265726\n",
            "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.055653\n",
            "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.049217\n",
            "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.075989\n",
            "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.003246\n",
            "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.032690\n",
            "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.042437\n",
            "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.080605\n",
            "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.087404\n",
            "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.017585\n",
            "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.010288\n",
            "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.021499\n",
            "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.111356\n",
            "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.018266\n",
            "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.100375\n",
            "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.048052\n",
            "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.084067\n",
            "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.041537\n",
            "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.103586\n",
            "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.049621\n",
            "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.094186\n",
            "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.033719\n",
            "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.045999\n",
            "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.019445\n",
            "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.010946\n",
            "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.045467\n",
            "Training time: 0m 10s\n",
            "===========================\n",
            "Test set: Average loss: 0.0009, Accuracy: 9817/10000 (98%)\n",
            "Testing time: 0m 11s\n",
            "Total Time: 1m 42s\n",
            "Model was trained on cuda!\n"
          ]
        }
      ]
    }
  ]
}