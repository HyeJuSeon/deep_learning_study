{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_AutoEncoder.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOWuwHs/9WIq3Io7d+ri32c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq1gv14vJs8z"
      },
      "source": [
        "# LSTM AutoEncoder 실습\n",
        "\n",
        "https://machinelearningmastery.com/lstm-autoencoders/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb8H8nvAJ3Ch"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, models, layers, optimizers, utils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3IVMvGXJyLI"
      },
      "source": [
        "## Reconstruction LSTM AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP8rIWaaJmAf",
        "outputId": "919baa1d-704a-4e09-ac23-7bd2d98f7880"
      },
      "source": [
        "# define input sequence\n",
        "seq = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
        "\n",
        "# reshape input into [samples, timesteps, features]\n",
        "n_in = len(seq)\n",
        "seq = seq.reshape((1, n_in, 1))\n",
        "\n",
        "# define model\n",
        "model = models.Sequential()\n",
        "model.add(layers.LSTM(100, activation='relu', input_shape=(n_in, 1)))\n",
        "model.add(layers.RepeatVector(n_in))\n",
        "model.add(layers.LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(layers.TimeDistributed(layers.Dense(1)))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(seq, seq, epochs=300, verbose=0)\n",
        "\n",
        "# predict\n",
        "yhat = model.predict(seq)\n",
        "yhat"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.10480094],\n",
              "        [0.20075469],\n",
              "        [0.2992439 ],\n",
              "        [0.39900646],\n",
              "        [0.49926344],\n",
              "        [0.59960264],\n",
              "        [0.6998814 ],\n",
              "        [0.8001507 ],\n",
              "        [0.9006021 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJGSxga5LQJj"
      },
      "source": [
        "## Prediction LSTM AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tu0XPSBLGhO",
        "outputId": "da0d82bd-63af-4266-948b-f9d533aa8f9e"
      },
      "source": [
        "# define input sequence\n",
        "seq_in = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
        "\n",
        "# reshape input into [samples, timesteps, features]\n",
        "n_in = len(seq_in)\n",
        "seq_in = seq_in.reshape((1, n_in, 1))\n",
        "\n",
        "# prepare output sequence\n",
        "seq_out = seq_in[:, 1:, :]\n",
        "n_out = n_in - 1\n",
        "\n",
        "# define model\n",
        "model = models.Sequential()\n",
        "model.add(layers.LSTM(100, activation='relu', input_shape=(n_in, 1)))\n",
        "model.add(layers.RepeatVector(n_out))\n",
        "model.add(layers.LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(layers.TimeDistributed(layers.Dense(1)))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(seq_in, seq_out, epochs=300, verbose=0)\n",
        "\n",
        "# predict\n",
        "yhat = model.predict(seq_in)\n",
        "yhat"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.16838059],\n",
              "        [0.29353157],\n",
              "        [0.40570223],\n",
              "        [0.50761384],\n",
              "        [0.60400295],\n",
              "        [0.69897115],\n",
              "        [0.79663056],\n",
              "        [0.9015523 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuhqFAoHMcY3",
        "outputId": "4b390aa0-1a41-40c2-b05f-fd367c013a4c"
      },
      "source": [
        "# define input sequence\n",
        "seq_in = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
        "\n",
        "# reshape input into [samples, timesteps, features]\n",
        "n_in = len(seq_in)\n",
        "seq_in = seq_in.reshape((1, n_in, 1))\n",
        "\n",
        "# prepare output sequence\n",
        "seq_out = seq_in[:, 1:, :]\n",
        "n_out = n_in - 1\n",
        "\n",
        "# define encoder\n",
        "visible = layers.Input(shape=(n_in, 1))\n",
        "encoder = layers.LSTM(100, activation='relu')(visible)\n",
        "# define reconstruct decoder\n",
        "decoder1 = layers.RepeatVector(n_in)(encoder)\n",
        "decoder1 = layers.LSTM(100, activation='relu', return_sequences=True)(decoder1)\n",
        "decoder1 = layers.TimeDistributed(layers.Dense(1))(decoder1)\n",
        "# define predict decoder\n",
        "decoder2 = layers.RepeatVector(n_out)(encoder)\n",
        "decoder2 = layers.LSTM(100, activation='relu', return_sequences=True)(decoder2)\n",
        "decoder2 = layers.TimeDistributed(layers.Dense(1))(decoder2)\n",
        "# concat model\n",
        "model = Model(inputs=visible, outputs=[decoder1, decoder2])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# utils.plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')\n",
        "\n",
        "# fit model\n",
        "model.fit(seq_in, [seq_in, seq_out], epochs=300, verbose=0)\n",
        "\n",
        "# predict\n",
        "y_hat = model.predict(seq_in)\n",
        "y_hat"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[0.1115096 ],\n",
              "         [0.20743433],\n",
              "         [0.30335915],\n",
              "         [0.3996057 ],\n",
              "         [0.4966244 ],\n",
              "         [0.59498614],\n",
              "         [0.69538325],\n",
              "         [0.7986429 ],\n",
              "         [0.9057488 ]]], dtype=float32), array([[[0.16606757],\n",
              "         [0.28853485],\n",
              "         [0.40248707],\n",
              "         [0.5094125 ],\n",
              "         [0.6108996 ],\n",
              "         [0.7076266 ],\n",
              "         [0.8002331 ],\n",
              "         [0.88931644]]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}
